{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 02\n",
    "\n",
    "In this notebook you'll explore, train, and evaluate models on the FashionMNIST dataset.  FashionMNIST was set up as a more difficult drop-in replacement for MNIST.\n",
    "\n",
    "For this assigment you'll want to use a CoCalc compute server with GPU.  Make sure you've watched the video at the beginning of the lesson about compute servers."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Add your imports here\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nfrom introdl.utils import config_paths_keys\n\n# Configure paths\npaths = config_paths_keys()\nDATA_PATH = paths['DATA_PATH']\nMODELS_PATH = paths['MODELS_PATH']",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup (5 points)\n",
    "\n",
    "Train LeNet5Rev on FashionMNIST and evaluate the performance on the test set.  Include convergence plots of loss and accuracy on the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the model (24 pts)\n",
    "\n",
    "Try increasing the number of convolutional layers up to six with ReLU layers.  You many need to increase\n",
    "the number of channels (but not in every layer).  Use two max pooling layers.  Kernel size can be 3 or 5\n",
    "but adjust the padding so that the convolutional layers preserve the size of the feature maps.\n",
    "\n",
    "You can also simplify the classifier.  Try a single linear layer instead of multiple linear layers\n",
    "separated by ReLU functions.\n",
    "\n",
    "You should be able to achieve about 92% accuracy on the test set.  Show convergence plots for each model you try.   \n",
    "\n",
    "You should try at least three different models.  Describe your experiments.  For each experiment include the model and plot convergence results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the things you tried (3 pts)\n",
    "\n",
    "Summarize the network architectures you tried.  What worked best?  What didn't help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze your best model (8 pts)\n",
    "\n",
    "Make a confusion matrix for the predictions of your best model on the test set.  You can set `use_class_labels = True` when using `evaluate_classifier` to see the names of the classes.  You can also access the names of the classes as an attribute of the dataset, e.g. `dataset.classes`.\n",
    "\n",
    "Describe which classes get most confused by your model.  Plot examples of the images that your model is getting wrong.  Do these misclassifications make sense?  Are the images from the misclassified classes hard to distinguish by eye?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Understanding Model Training",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs Validation Data [3 pts]\n",
    "\n",
    "1. In your FashionMNIST experiments above, you used training and test sets. Explain why we need separate datasets for training and evaluation. What problem are we trying to avoid?\n",
    "\n",
    "YOUR ANSWER:\n",
    "\n",
    "2. During training, when should you evaluate on the test/validation set? Every batch? Every epoch? Only at the end? Explain your reasoning.\n",
    "\n",
    "YOUR ANSWER:\n",
    "\n",
    "3. If your training accuracy is 99% but your test accuracy is only 60%, what does this indicate about your model? What might you do to fix this?\n",
    "\n",
    "YOUR ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Training Loop [5 pts]\n",
    "\n",
    "The `train_simple_network` function you used above contains a training loop. Below is simplified code showing the key parts of what happens inside that function. For each marked line, explain in your own words what that line does and why it's necessary for training the network.\n",
    "\n",
    "```python\n",
    "def training_loop(model, train_loader, loss_func, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            # LINE A: What does optimizer.zero_grad() do and why is it necessary?\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # LINE B: What is happening when we call model(data)?\n",
    "            output = model(data)\n",
    "            \n",
    "            # LINE C: What does the loss function compute and what type of value does it return?\n",
    "            loss = loss_func(output, target)\n",
    "            \n",
    "            # LINE D: What does loss.backward() calculate and where does it store the results?\n",
    "            loss.backward()\n",
    "            \n",
    "            # LINE E: What does optimizer.step() do with the information from loss.backward()?\n",
    "            optimizer.step()\n",
    "```\n",
    "\n",
    "YOUR EXPLANATIONS:\n",
    "- LINE A: \n",
    "- LINE B: \n",
    "- LINE C: \n",
    "- LINE D: \n",
    "- LINE E: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection [2 pts]\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for this lesson? Why?\n",
    "\n",
    "YOUR ANSWER:\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "YOUR ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Reading Comprehension: Chapter 3 - Convolutional Neural Networks [10 pts]\n\nThese questions test your understanding of Chapter 3 from \"Inside Deep Learning\" by Edward Raff. \n\n### Spatial Structure and Prior Beliefs [2 pts]\n\n1. The textbook states that \"convolutions are powerful yet simple tools that help us encode information about the problem into the design of our network architecture.\" Explain what **spatial structural prior belief** means in the context of CNNs and why shuffling pixels in an image destroys this structure. Give a concrete example from your FashionMNIST experiments.\n\nYOUR ANSWER:\n\n2. Why does the textbook emphasize that CNNs work well for images but not for tabular/columnar data? What assumption do CNNs make that may not hold for spreadsheet-like data?\n\nYOUR ANSWER:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Weight Sharing and Translation [3 pts]\n\n3. Section 3.2.4 introduces the concept of **weight sharing**. Explain how weight sharing in convolutions differs from fully connected layers. Why does this make CNNs more parameter-efficient for image data?\n\nYOUR ANSWER:\n\n4. The textbook discusses **translation invariance** as a desired property for image classification (Section 3.5). In your experiments above, did you observe any issues with small shifts in the input? How does max pooling help achieve partial translation invariance, and why is it only \"partial\"?\n\nYOUR ANSWER:\n\n5. According to the textbook, what is the relationship between stride and the output size? If you use a stride of 3 in a convolution, how does this affect the spatial dimensions of your output?\n\nYOUR ANSWER:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Architectural Design Choices [3 pts]\n\n6. The textbook mentions that \"you can have too much of a good thing and make a network too deep to learn\" (Section 3.5.1). Based on your reading and experiments, what are the tradeoffs between network depth and performance? Why might a 200-layer network not always be better than a 20-layer network?\n\nYOUR ANSWER:\n\n7. In Section 3.4.3, the textbook introduces the `nn.Flatten()` operation. Explain why this operation is necessary when transitioning from convolutional layers to fully connected layers. What would happen if you tried to connect a Conv2d output directly to a Linear layer without flattening?\n\nYOUR ANSWER:\n\n8. The textbook suggests increasing the number of filters by a factor of K after each pooling layer of size K (Section 3.5.1). What is the computational reasoning behind this guideline? Did you follow this pattern in your experiments above?\n\nYOUR ANSWER:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Data Augmentation Philosophy [2 pts]\n\n9. Section 3.6 describes data augmentation as \"the feature engineering counterpart to deep learning.\" Based on the textbook's examples, what makes a good vs. bad augmentation for a specific dataset? Why does the author warn against using vertical flips for MNIST?\n\nYOUR ANSWER:\n\n10. The textbook states that neural networks are \"data-hungry\" and learn best with diverse data. How does data augmentation address this need differently than simply collecting more real data? What are the limitations of augmentation that the textbook mentions?\n\nYOUR ANSWER:",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}