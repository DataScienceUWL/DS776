import torch
import matplotlib.pyplot as plt
import numpy as np
from bertviz import head_view

def visualize_positional_encodings(pos_encoder, max_len=50, d_model=16, figsize=(8,5)):
    """
    Visualizes the positional encodings generated by the PositionalEncoding module.

    Args:
        pos_encoder (PositionalEncoding): Instance of the PositionalEncoding class.
        max_len (int): Number of positions to visualize.
        d_model (int): Dimensionality of the embedding space to visualize.
    """
    # Generate a dummy input to get positional encodings
    dummy_input = torch.zeros(1, max_len, d_model)
    pos_encodings = pos_encoder(dummy_input)[0].detach().numpy()  # Remove batch dimension
    
    # Create the plot
    plt.figure(figsize=figsize)
    plt.title(f"Positional Encodings (Max Length: {max_len}, Embedding Dim: {d_model})")
    plt.imshow(pos_encodings, aspect='auto', cmap='viridis')
    plt.colorbar(label="Encoding Value")
    plt.xlabel("Embedding Dimension")
    plt.ylabel("Position Index")
    plt.show()

def plot_attention_weights(model, loader, vocab, layer_idx, idx, figsize=(6, 6)):
    import matplotlib.pyplot as plt

    # Get a batch from the loader
    for batch_idx, (inputs, labels) in enumerate(loader):
        break  # Take the first batch

    # Move inputs and model to the correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = inputs.to(device)
    model = model.to(device)

    # Forward pass to generate predictions
    outputs = model(inputs)

    # Extract attention weights
    attention_weights = model.get_attention_weights()  # List of attention weights from all layers

    # Select attention weights for the specified sequence and layer
    attention_for_idx = attention_weights[layer_idx][idx].detach().cpu().numpy()  # Shape: (seq_len, seq_len)
    inputs_for_idx = inputs[idx].cpu()
    mask = (inputs_for_idx != 0)
    attention_for_idx = attention_for_idx[:, mask][mask, :]
    inputs_for_idx = inputs_for_idx[mask].numpy()

    # Map token indices to tokens
    idx_to_token = {v: k for k, v in vocab.items()}
    tokens = [idx_to_token.get(i, "[UNK]") for i in inputs_for_idx]

    # Plot the attention weights without gridlines
    plt.figure(figsize=figsize)
    plt.imshow(attention_for_idx, cmap='viridis')
    plt.title(f"Attention Weights, Layer {layer_idx}")
    plt.xlabel("Keys")
    plt.ylabel("Querys")
    plt.colorbar()
    plt.grid(False)  # Remove gridlines

    # Move xticks to the top
    plt.gca().xaxis.set_ticks_position('top')
    plt.gca().xaxis.set_label_position('top')

    plt.xticks(ticks=np.arange(attention_for_idx.shape[1]), labels=tokens, rotation=45)
    plt.yticks(ticks=np.arange(attention_for_idx.shape[0]), labels=tokens)

    plt.show()


def display_attention(idx, model, test_loader, vocab):
    """
    Displays the BertViz head_view for a specific sequence in the batch.

    Args:
        idx (int): Index of the sequence in the batch to visualize.
        model (torch.nn.Module): The trained model.
        test_loader (DataLoader): DataLoader containing the test dataset.
        vocab (dict): Mapping of token indices to tokens.
    """
    # Get a batch from the test loader
    for batch_idx, (inputs, labels) in enumerate(test_loader):
        break  # Take the first batch

    # Move inputs and model to the correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = inputs.to(device)
    model = model.to(device)

    # Forward pass to generate predictions
    outputs = model(inputs)

    # Extract attention weights
    attention_weights = model.get_attention_weights()  # List of attention weights from all layers

    # Select the sequence and reshape attention weights
    inputs_for_idx = inputs[idx].cpu()
    attention_for_idx0 = [
        layer_attention[idx].unsqueeze(0).unsqueeze(0).detach().cpu()
        for layer_attention in attention_weights
    ]

    # Remove padding
    mask = (inputs_for_idx != 0)
    attention_for_idx = [
        layer_attention[:, :, mask][:, :, :, mask]
        for layer_attention in attention_for_idx0
    ]
    inputs_for_idx = inputs_for_idx[mask].numpy()

    # Map token indices to tokens
    idx_to_token = {v: k for k, v in vocab.items()}
    tokens = [idx_to_token.get(i, "[UNK]") for i in inputs_for_idx]

    # Display BertViz head_view
    head_view(
        attention=attention_for_idx,  # Attention weights as a list of tensors
        tokens=tokens,
    )

