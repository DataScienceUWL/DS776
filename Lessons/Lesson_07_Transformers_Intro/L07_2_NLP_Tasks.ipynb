{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ introdl v1.6.16 already up to date\n"
     ]
    }
   ],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:18.532720Z",
     "iopub.status.busy": "2025-08-26T00:43:18.532532Z",
     "iopub.status.idle": "2025-08-26T00:43:33.211450Z",
     "shell.execute_reply": "2025-08-26T00:43:33.210391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment: Unknown Environment | Course root: /mnt/e/GDrive_baggett.jeff/Teaching/Classes_current/2025-2026_Fall_DS776/DS776\n",
      "   Using workspace: <DS776_ROOT_DIR>/home_workspace\n",
      "\n",
      "üìÇ Storage Configuration:\n",
      "   DATA_PATH: <DS776_ROOT_DIR>/home_workspace/data\n",
      "   MODELS_PATH: <DS776_ROOT_DIR>/Lessons/Lesson_07_Transformers_Intro/Lesson_07_Models (local to this notebook)\n",
      "   CACHE_PATH: <DS776_ROOT_DIR>/home_workspace/downloads\n",
      "üîë API keys: 9 loaded from home_workspace/api_keys.env\n",
      "üîê Available: ANTHROPIC_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY... (9 total)\n",
      "‚úÖ HuggingFace Hub: Logged in\n",
      "üí∞ OpenRouter credit: $9.93\n",
      "üì¶ introdl v1.6.16 ready\n",
      "\n",
      "‚úÖ Loaded pricing for 327 OpenRouter models\n",
      "‚úÖ Cost tracking initialized ($9.93 credit remaining)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from introdl.utils import get_device, wrap_print_text, config_paths_keys\n",
    "from introdl.nlp import llm_generate, init_cost_tracking, clear_pipeline, print_pipeline_info, display_markdown, llm_list_models\n",
    "\n",
    "# Wrap print to format text nicely at 120 characters\n",
    "print = wrap_print_text(print, width=120)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "paths = config_paths_keys()\n",
    "\n",
    "# Initialize LLM cost tracking\n",
    "init_cost_tracking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L07_2_NLP_Tasks Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_2_nlp_tasks\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_2_nlp_tasks\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/omj2ldze713\" target=\"_blank\">Open Descript version of video in new tab</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Tasks with Transformer Models\n",
    "\n",
    "In this notebook we'll demonstrate solutions to some common Natural Language Processing (NLP) tasks that use transformer models.  We expand on the material in our NLP textbook Chapter 1 - Hello Transformers.  We'll add a little background about the underlying models.  We'll also demonstrate how these same tasks come be done using a large language model with either \"zero-shot prompting\" or \"few-shot prompting\".\n",
    "\n",
    "Over the next five lessons we'll go into some of these NLP tasks in detail and a learn a bit about the transformer neural network architecture.  For each of the NLP tasks that follows we'll demonstrate how to do the task two ways.  The first is by using a pre-trained transformer-based model downloaded from HuggingFace.  In the second approach we'll use a a large language model and prompting.\n",
    "\n",
    "Using LLMs for various NLP tasks is common when there isn't much labeled data available.  Zero-shot prompting means that no examples are provided to the LLM.  Few-shot prompting means that a small number of examples are provided to the LLM.  In this notebook we'll demonstrate zero-shot prompting, but in the lessons to come we'll include few-shot prompting examples.\n",
    "\n",
    "Throughout this notebook we'll use the following customer feedback message that as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:33.214197Z",
     "iopub.status.busy": "2025-08-26T00:43:33.213694Z",
     "iopub.status.idle": "2025-08-26T00:43:33.218096Z",
     "shell.execute_reply": "2025-08-26T00:43:33.217310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three days, I hadn‚Äôt even\n",
      "received a shipping update. After waiting 45 minutes on hold, customer service told me there was a stock issue‚Äîyet no\n",
      "one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The support rep was apologetic\n",
      "but said an exchange would take another two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. To add\n",
      "insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake. The entire experience has been incredibly\n",
      "disappointing and has left me questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and efficiency. I hope this feedback reaches\n",
      "someone who can make a difference, as no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three days, I hadn‚Äôt even received a shipping update. After waiting 45 minutes on hold, customer service told me there was a stock issue‚Äîyet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. To add insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. I had to explain my situation multiple times before they even acknowledged the mistake. The entire experience has been incredibly disappointing and has left me questioning whether I should ever shop with Tech Haven again. \n",
    "\n",
    "It's baffling how a company can operate with such a lack of transparency and efficiency. I hope this feedback reaches someone who can make a difference, as no customer should have to go through what I did. Tech Haven, you need to do better! Sincerely, Jamie.\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Classification\n",
    "\n",
    "Text classification is the process of assigning predefined categories to text. It involves analyzing the content of the text and categorizing it based on its subject, sentiment, or other criteria. One common application of text classification is sentiment analysis, which determines the sentiment expressed in a piece of text, such as positive, negative, or neutral. Sentiment analysis is widely used in customer feedback analysis, social media monitoring, and market research to gauge public opinion and customer satisfaction.\n",
    "\n",
    "### Sentiment Analysis with a Specialized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for sentiment analysis and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:33.220072Z",
     "iopub.status.busy": "2025-08-26T00:43:33.219895Z",
     "iopub.status.idle": "2025-08-26T00:43:34.243427Z",
     "shell.execute_reply": "2025-08-26T00:43:34.242537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english, Size: 66,955,010 parameters\n",
      "[{'label': 'NEGATIVE', 'score': 0.9989209175109863}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "print_pipeline_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a \"BERT\" model correctly classified the customer feedback as negative. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model developed by Google. It is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows BERT to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. The particular model used here is a distilled BERT model that has been fine-tuned on a sentiment dataset. A distilled model is a smaller, faster, and more efficient version of a larger model, trained using knowledge distillation, where the smaller model learns to mimic the outputs of the larger one while retaining most of its performance. In Lesson 9, we'll learn more about the family of transformer models called encoders, which include BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's never a bad idea to remove models from memory when they aren't being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:34.245423Z",
     "iopub.status.busy": "2025-08-26T00:43:34.245230Z",
     "iopub.status.idle": "2025-08-26T00:43:34.915933Z",
     "shell.execute_reply": "2025-08-26T00:43:34.915078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(sentiment_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentiment Analysis with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "A system prompt is used to give instructions to an LLM while a user prompt is the specific input you want the LLM to respond to.  Here we define a system prompt for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:34.917831Z",
     "iopub.status.busy": "2025-08-26T00:43:34.917619Z",
     "iopub.status.idle": "2025-08-26T00:43:38.262730Z",
     "shell.execute_reply": "2025-08-26T00:43:38.261791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "response_zero_shot = llm_generate('gemini-flash-lite', user_prompt, system_prompt=system_prompt)\n",
    "print(response_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also handle batches of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:38.265132Z",
     "iopub.status.busy": "2025-08-26T00:43:38.264867Z",
     "iopub.status.idle": "2025-08-26T00:43:53.851512Z",
     "shell.execute_reply": "2025-08-26T00:43:53.850636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fast shipping and great customer support. Highly recommend!\n",
      "Sentiment: positive\n",
      "\n",
      "Text: The item arrived damaged and the return process was a nightmare.\n",
      "Sentiment: negative\n",
      "\n",
      "Text: I'm very satisfied with my purchase. Will buy again.\n",
      "Sentiment: positive\n",
      "\n",
      "Text: The website is user-friendly and the prices are unbeatable.\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Received the wrong item and customer service was unhelpful.\n",
      "Sentiment: negative\n",
      "\n",
      "Text: Fantastic experience from start to finish.\n",
      "Sentiment: positive\n",
      "\n",
      "Text: The product is okay, but not worth the price.\n",
      "Sentiment: negative\n",
      "\n",
      "Text: Excellent quality and quick delivery. Very happy!\n",
      "Sentiment: positive\n",
      "\n",
      "Text: The product works as expected, nothing more, nothing less.\n",
      "Sentiment: neutral\n",
      "\n",
      "Text: I have mixed feelings about the service; it was both good and bad.\n",
      "Sentiment: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customer_comments = [\n",
    "    \"Fast shipping and great customer support. Highly recommend!\",\n",
    "    \"The item arrived damaged and the return process was a nightmare.\",\n",
    "    \"I'm very satisfied with my purchase. Will buy again.\",\n",
    "    \"The website is user-friendly and the prices are unbeatable.\",\n",
    "    \"Received the wrong item and customer service was unhelpful.\",\n",
    "    \"Fantastic experience from start to finish.\",\n",
    "    \"The product is okay, but not worth the price.\",\n",
    "    \"Excellent quality and quick delivery. Very happy!\",\n",
    "    \"The product works as expected, nothing more, nothing less.\",\n",
    "    \"I have mixed feelings about the service; it was both good and bad.\"\n",
    "]\n",
    "\n",
    "user_prompts = [f\"Text: {comment}\\nSentiment:\" for comment in customer_comments]\n",
    "\n",
    "responses_zero_shot = llm_generate('gemini-flash-lite', user_prompts, system_prompt=system_prompt)\n",
    "\n",
    "for comment, response_zero_shot in zip(customer_comments, responses_zero_shot):\n",
    "    print(f\"Text: {comment}\\nSentiment: {response_zero_shot}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text classification more in Lesson 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to Write Better Prompts\n",
    "\n",
    "There are many prompt engineering resources available on the internet.  I encourage you to look at those as needed.  I've also had good luck asking ChatGPT how to craft prompts.  One particularly useful resource is [ChatGPT Prompt Engineering for Developers](https://app.datacamp.com/learn/courses/chatgpt-prompt-engineering-for-developers) on DataCamp.  It's not free, but it is cheap.  I've viewed parts of this course and found it to be a very good introduction to programatic prompt writing.  It's tailored to ChatGPT, but you can use the OpenAI API with Gemini as well.  Working through this short course (they say it takes 4 hours) is likely worthwhile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Practical examples of NER include:\n",
    "- **Business Application**: Extracting company names, dates, and monetary amounts from financial reports to automate data entry and analysis.\n",
    "- **Healthcare**: Identifying patient names, medical conditions, and treatment dates from clinical notes to improve patient record management.\n",
    "- **News Aggregation**: Categorizing and tagging entities like people, places, and events in news articles to enhance search and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for NER and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:53.853339Z",
     "iopub.status.busy": "2025-08-26T00:43:53.853145Z",
     "iopub.status.idle": "2025-08-26T00:43:56.291882Z",
     "shell.execute_reply": "2025-08-26T00:43:56.290706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Named Entity Recognition**\n",
      "\n",
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889 parameters\n",
      "\n",
      "[{'entity_group': 'MISC', 'score': 0.990973, 'word': 'Samsung Galaxy S24 Ultra', 'start': 14, 'end': 38},\n",
      "{'entity_group': 'ORG', 'score': 0.994846, 'word': 'Tech Haven', 'start': 44, 'end': 54}, {'entity_group': 'MISC',\n",
      "'score': 0.9928634, 'word': 'Google Pixel 8 Pro', 'start': 323, 'end': 341}, {'entity_group': 'ORG', 'score': 0.9964845,\n",
      "'word': 'Tech Haven', 'start': 863, 'end': 873}, {'entity_group': 'ORG', 'score': 0.9887396, 'word': 'Tech Haven',\n",
      "'start': 1089, 'end': 1099}, {'entity_group': 'PER', 'score': 0.9780477, 'word': 'Jamie', 'start': 1135, 'end': 1140}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\\n\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print_pipeline_info(ner_pipeline)\n",
    "print(\"\")\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That output is hard to read, but we can easily convert it to a Pandas data frame for display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:56.294124Z",
     "iopub.status.busy": "2025-08-26T00:43:56.293807Z",
     "iopub.status.idle": "2025-08-26T00:43:56.304805Z",
     "shell.execute_reply": "2025-08-26T00:43:56.303849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity_group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "end",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "11b5eb19-3cf9-4086-81ac-ffc5bcaa04d1",
       "rows": [
        [
         "0",
         "MISC",
         "0.990973",
         "Samsung Galaxy S24 Ultra",
         "14",
         "38"
        ],
        [
         "1",
         "ORG",
         "0.994846",
         "Tech Haven",
         "44",
         "54"
        ],
        [
         "2",
         "MISC",
         "0.9928634",
         "Google Pixel 8 Pro",
         "323",
         "341"
        ],
        [
         "3",
         "ORG",
         "0.9964845",
         "Tech Haven",
         "863",
         "873"
        ],
        [
         "4",
         "ORG",
         "0.9887396",
         "Tech Haven",
         "1089",
         "1099"
        ],
        [
         "5",
         "PER",
         "0.9780477",
         "Jamie",
         "1135",
         "1140"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990973</td>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.994846</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.992863</td>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>323</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996485</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>863</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.988740</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>1089</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.978048</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1135</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                      word  start   end\n",
       "0         MISC  0.990973  Samsung Galaxy S24 Ultra     14    38\n",
       "1          ORG  0.994846                Tech Haven     44    54\n",
       "2         MISC  0.992863        Google Pixel 8 Pro    323   341\n",
       "3          ORG  0.996485                Tech Haven    863   873\n",
       "4          ORG  0.988740                Tech Haven   1089  1099\n",
       "5          PER  0.978048                     Jamie   1135  1140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.DataFrame(ner_result)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"BERT\" model used here is the `dbmdz/bert-large-cased-finetuned-conll03-english` model, which has been fine-tuned on the CoNLL-2003 dataset for Named Entity Recognition (NER). This fine-tuning process allows the model to accurately identify and classify entities such as names of persons, organizations, locations, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:56.307086Z",
     "iopub.status.busy": "2025-08-26T00:43:56.306815Z",
     "iopub.status.idle": "2025-08-26T00:43:58.023959Z",
     "shell.execute_reply": "2025-08-26T00:43:58.023048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(ner_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for NER.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:58.025960Z",
     "iopub.status.busy": "2025-08-26T00:43:58.025755Z",
     "iopub.status.idle": "2025-08-26T00:44:45.483722Z",
     "shell.execute_reply": "2025-08-26T00:44:45.482911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\"entity\": \"Samsung Galaxy S24 Ultra\", \"type\": \"PRODUCT\"},\n",
      "  {\"entity\": \"Tech Haven\", \"type\": \"ORGANIZATION\"},\n",
      "  {\"entity\": \"Google Pixel 8 Pro\", \"type\": \"PRODUCT\"},\n",
      "  {\"entity\": \"$1,200\", \"type\": \"MONEY\"},\n",
      "  {\"entity\": \"Tech Haven\", \"type\": \"ORGANIZATION\"},\n",
      "  {\"entity\": \"Tech Haven\", \"type\": \"ORGANIZATION\"},\n",
      "  {\"entity\": \"Jamie\", \"type\": \"PERSON\"}\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "response_ner = llm_generate('gemini-flash-lite', user_prompt, system_prompt=system_prompt)\n",
    "print(response_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with String Output (Traditional Approach)\n",
    "\n",
    "Our LLM returned a string containing JSON. Below we parse this string to extract the JSON and display it as a DataFrame. Different LLMs may return different formats which require different parsing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:45.485905Z",
     "iopub.status.busy": "2025-08-26T00:44:45.485611Z",
     "iopub.status.idle": "2025-08-26T00:44:45.495786Z",
     "shell.execute_reply": "2025-08-26T00:44:45.494660Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Clean the response_ner string\n",
    "response_ner = response_ner.strip('```json\\n').strip('\\n```')\n",
    "\n",
    "# Convert the cleaned response to a DataFrame for display\n",
    "ner_result = json.loads(response_ner)\n",
    "\n",
    "# ner_result is already a list - use it directly\n",
    "df = pd.DataFrame(ner_result)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with JSON Mode (Modern Approach)\n",
    "\n",
    "Alternatively, we can use `mode='json'` in `llm_generate()` to get structured JSON output directly without needing to parse strings. This is more reliable and works with models that support structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "response_ner = llm_generate('gemini-flash-lite', user_prompt, system_prompt=system_prompt, mode='json')\n",
    "df = pd.DataFrame(response_ner)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the LLM is similar to that of the specialized model from HuggingFace.  If we want different output from the LLM we could include instructions for that in our system prompt.\n",
    "\n",
    "In Lesson 10 we'll learn more about Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Approaches\n",
    "\n",
    "Both approaches (string parsing and JSON mode) produce similar results, and both are comparable to the specialized HuggingFace model output. The JSON mode approach is generally more reliable since it provides structured output directly without needing string parsing. However, not all models support JSON mode equally well.\n",
    "\n",
    "If you want different output formats or entity types from the LLM, you can adjust the instructions in your system prompt.\n",
    "\n",
    "In Lesson 10 we'll learn more about Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Question Answering\n",
    "\n",
    "Question Answering (QA) is a subtask of information retrieval and natural language understanding that involves automatically answering questions posed by humans in a natural language. QA systems can be designed to answer questions based on a given context or a large corpus of documents. The goal is to provide accurate and relevant answers to user queries.\n",
    "\n",
    "Practical examples of QA include:\n",
    "- **Customer Support**: Providing instant answers to customer queries based on a knowledge base or FAQ, improving response times and customer satisfaction.\n",
    "- **Education**: Assisting students by answering questions related to their coursework or providing explanations for complex topics.\n",
    "- **Healthcare**: Offering medical professionals quick access to information from medical literature or patient records to support clinical decision-making.\n",
    "- **Search Engines**: Enhancing search results by directly providing answers to user queries, rather than just a list of relevant documents.\n",
    "\n",
    "Here we will let the HuggingFace transformers library provide its default model for QA and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:45.498408Z",
     "iopub.status.busy": "2025-08-26T00:44:45.498231Z",
     "iopub.status.idle": "2025-08-26T00:44:46.507507Z",
     "shell.execute_reply": "2025-08-26T00:44:46.506695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Question Answering**\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450 parameters\n",
      "\n",
      "{'score': 0.4041374623775482, 'start': 218, 'end': 231, 'answer': 'a stock\n",
      "issue'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\\n\")\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "print_pipeline_info(qa_pipeline)\n",
    "print(\"\")\n",
    "question = \"What is the main issue?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:46.509470Z",
     "iopub.status.busy": "2025-08-26T00:44:46.509292Z",
     "iopub.status.idle": "2025-08-26T00:44:47.175036Z",
     "shell.execute_reply": "2025-08-26T00:44:47.174188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(qa_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for QA.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:47.176975Z",
     "iopub.status.busy": "2025-08-26T00:44:47.176790Z",
     "iopub.status.idle": "2025-08-26T00:45:09.158225Z",
     "shell.execute_reply": "2025-08-26T00:45:09.157329Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. Be succinct.\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: What is the main issue?\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate('gemini-flash-lite', user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a lesson dedicated to question answering, but it's discussed in our NLP textbook in Chapter 7.  You could investigate this topic further in a project if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Translation\n",
    "\n",
    "The first transformer model in the paper \"Attention is All You Need\" was designed for the task of language translation. This model, known as the Transformer, introduced a novel architecture that relies entirely on self-attention mechanisms to process input sequences, making it highly effective for translation tasks. The Transformer model has since become the foundation for many state-of-the-art NLP models, including BERT, GPT, and others.\n",
    "\n",
    "Here we demonstrate how to use a pre-trained model from HuggingFace for translating English to Spanish.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:17.946147Z",
     "iopub.status.busy": "2025-08-26T00:45:17.945967Z",
     "iopub.status.idle": "2025-08-26T00:45:27.271748Z",
     "shell.execute_reply": "2025-08-26T00:45:27.270895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Translation**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbaggett/miniconda3/envs/ds776_env/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Helsinki-NLP/opus-mt-en-es, Size: 77,943,296 parameters\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ped√≠ el Samsung Galaxy S24 Ultra de Tech Haven, esperando la entrega del d√≠a\n",
      "siguiente, pero despu√©s de tres d√≠as, yo ni siquiera hab√≠a recibido una\n",
      "actualizaci√≥n de env√≠o. Despu√©s de esperar 45 minutos en espera, el servicio al\n",
      "cliente me dijo que hab√≠a un problema de existencias ‚Äî sin embargo nadie me\n",
      "hab√≠a informado! Cuando el paquete finalmente lleg√≥ una semana tarde, que\n",
      "conten√≠a un Google Pixel 8 Pro en su lugar. El representante de apoyo era\n",
      "apolog√©tico, pero dijo que un intercambio tomar√≠a otras dos semanas. Pagu√©\n",
      "$1.200 por el tel√©fono equivocado, trat√≥ con retrasos y mala comunicaci√≥n, y\n",
      "ahora tienen que esperar incluso m√°s tiempo. Para a√±adir insulto a la lesi√≥n, el\n",
      "representante de servicio al cliente con el que habl√© parec√≠a indiferente a mi\n",
      "frustraci√≥n. Tuve que explicar mi situaci√≥n varias veces antes de que incluso\n",
      "reconocieron el error. Toda la experiencia ha sido incre√≠blemente decepcionante\n",
      "y me ha dejado cuestionando si alguna vez deber√≠a comprar con Tech Haven de\n",
      "nuevo. Es desconcertante c√≥mo una empresa puede funcionar con tal falta de\n",
      "transparencia y eficiencia. Espero que esta retroalimentaci√≥n llega a alguien\n",
      "que puede hacer una diferencia, ya que ning√∫n cliente debe ir a trav√©s de lo que\n",
      "hice. Tech Haven!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\\n\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "print_pipeline_info(translation_pipeline)\n",
    "print(\"\")\n",
    "translation_result = translation_pipeline(text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you're better than I am at Spanish, but I can't read that well enough to know if it's a good translation.  However, let's now use a similar model to translate it from Spanish back into English and compare it to the orginal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:27.273604Z",
     "iopub.status.busy": "2025-08-26T00:45:27.273421Z",
     "iopub.status.idle": "2025-08-26T00:45:35.643353Z",
     "shell.execute_reply": "2025-08-26T00:45:35.642462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Helsinki-NLP/opus-mt-es-en, Size: 77,943,296 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back Translation to English: I ordered the Samsung Galaxy S24 Ultra from Tech\n",
      "Haven, waiting for delivery the next day, but after three days, I had not even\n",
      "received a shipping update. After waiting 45 minutes on hold, the customer\n",
      "service told me that there was a stock problem ‚Äî yet no one had informed me!\n",
      "When the package finally arrived a week late, containing a Google Pixel 8 Pro\n",
      "instead. The support representative was apologetic, but he said an exchange\n",
      "would take another two weeks. I paid $1,200 for the wrong phone, tried with\n",
      "delays and bad communication, and now they have to wait even longer. To add\n",
      "insult to the injury, the customer service representative with whom I spoke\n",
      "seemed indifferent to my frustration. I had to explain my situation several\n",
      "times before they even recognized the error. All the experience has been\n",
      "incredibly disappointing and has left me wondering if I should ever buy with\n",
      "Tech Haven again. It is disconcerting how a company can function with such a\n",
      "lack of transparency and efficiency. I hope this feedback comes to someone who\n",
      "can make a difference, as no customer should go through what I did. Tech Haven!\n",
      "\n",
      "Original Text: I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting\n",
      "next-day delivery, but after three days, I hadn‚Äôt even received a shipping\n",
      "update. After waiting 45 minutes on hold, customer service told me there was a\n",
      "stock issue‚Äîyet no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead. The support rep was apologetic but said an exchange would take another\n",
      "two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer. To add insult to injury, the customer service\n",
      "representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake.\n",
      "The entire experience has been incredibly disappointing and has left me\n",
      "questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and\n",
      "efficiency. I hope this feedback reaches someone who can make a difference, as\n",
      "no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n"
     ]
    }
   ],
   "source": [
    "# Extract the Spanish translation from the previous result\n",
    "spanish_translation = translation_result[0]['translation_text']\n",
    "\n",
    "# Translate the Spanish text back to English\n",
    "back_translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\", device=device)\n",
    "print_pipeline_info(back_translation_pipeline)\n",
    "back_translation_result = back_translation_pipeline(spanish_translation, max_length=300)\n",
    "print(f\"Back Translation to English: {back_translation_result[0]['translation_text']}\\n\")\n",
    "print(f\"Original Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Helsinki-NLP models are part of the OPUS-MT project, which provides pre-trained neural machine translation models for many language pairs. These models are based on the MarianMT architecture, a transformer-based model optimized for translation tasks. The MarianMT architecture leverages self-attention mechanisms to effectively process and translate text, making these models highly accurate and efficient for translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:35.645625Z",
     "iopub.status.busy": "2025-08-26T00:45:35.645391Z",
     "iopub.status.idle": "2025-08-26T00:45:36.397861Z",
     "shell.execute_reply": "2025-08-26T00:45:36.396978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(translation_pipeline)\n",
    "clear_pipeline(back_translation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation with an LLM and a Zero-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:36.399860Z",
     "iopub.status.busy": "2025-08-26T00:45:36.399625Z",
     "iopub.status.idle": "2025-08-26T00:47:39.438510Z",
     "shell.execute_reply": "2025-08-26T00:47:39.437689Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_translation = \"\"\"You are an expert translation model. Translate the following text from English to Spanish.\"\"\"\n",
    "user_prompt_translation = f\"Text: {text}\\nTranslation:\"\n",
    "\n",
    "response_translation = llm_generate('gemini-flash-lite',\n",
    "                                    user_prompt_translation, \n",
    "                                    system_prompt=system_prompt_translation,\n",
    "                                    max_tokens=500)\n",
    "print(response_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:47:39.441013Z",
     "iopub.status.busy": "2025-08-26T00:47:39.440769Z",
     "iopub.status.idle": "2025-08-26T00:48:56.864675Z",
     "shell.execute_reply": "2025-08-26T00:48:56.863824Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_back_translation = \"\"\"You are an expert translation model. Translate the following text from Spanish to English.\"\"\"\n",
    "user_prompt_back_translation = f\"Text: {spanish_translation}\\nTranslation:\"\n",
    "\n",
    "response_back_translation = llm_generate('gemini-flash-lite',\n",
    "                                         user_prompt_back_translation, \n",
    "                                         system_prompt=system_prompt_back_translation,\n",
    "                                         max_tokens=500)\n",
    "print(response_back_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't specifically study translation models in one of our lessons, the transformer models used for text summarization are similar in that they take an input sequence of text and produce an output sequence of text.  These models are called sequence to sequence transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Generation\n",
    "\n",
    "Of all the models we'll study, text-generation models are perhaps the most familiar since they are the machines that drive today's chatbots like ChatGPT, Gemini, Claude, and others. Given an input sequence that provides context, a text generation model predicts a likely next word, then does it again and again to generate a hopefully sensible response. These models are particularly useful for tasks such as drafting emails, writing code, creating conversational agents, and generating creative content like stories and poems.\n",
    "\n",
    "HuggingFace makes it simple to create a text generation pipeline.  Here we provide the original customer comment plus the beginning of customer service response and ask the modlel to generate 200 new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:48:56.866564Z",
     "iopub.status.busy": "2025-08-26T00:48:56.866382Z",
     "iopub.status.idle": "2025-08-26T00:49:06.521925Z",
     "shell.execute_reply": "2025-08-26T00:49:06.520973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Text Generation**\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openai-community/gpt2, Size: 124,439,808 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day\n",
      "delivery, but after three days, I hadn‚Äôt even received a shipping update. After\n",
      "waiting 45 minutes on hold, customer service told me there was a stock issue‚Äîyet\n",
      "no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead. The support rep was apologetic but said an exchange would take another\n",
      "two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer. To add insult to injury, the customer service\n",
      "representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake.\n",
      "The entire experience has been incredibly disappointing and has left me\n",
      "questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and\n",
      "efficiency. I hope this feedback reaches someone who can make a difference, as\n",
      "no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up. Please contact Tech\n",
      "Haven by e-mail or by phone to arrange a shipping replacement. Please note that\n",
      "I will not be able to resolve your problem. I will not be able to meet your\n",
      "current orders and will not be able to meet your future orders. You have placed\n",
      "an order on the website and there are no issues with the service. I have\n",
      "contacted Tech Haven to arrange an exchange but have not received a response.\n",
      "I'm sorry to hear that your order was mixed up. Please contact Tech Haven by\n",
      "e-mail or by phone to arrange a shipping replacement. Please note that I will\n",
      "not be able to resolve your problem. I will not be able to meet your current\n",
      "orders and will not be able to meet your future orders.\n",
      "\n",
      "I have been using my Google Pixel 7 Extreme for several weeks now. I bought it\n",
      "from Tech Haven and was impressed with the look and feel of the phone. I had\n",
      "never used a Pixel before, and I'm very impressed with the look and feel of it.\n",
      "I like the way the screen looks. No issues. I like the feel and feel of the\n",
      "display, as well as the battery life. I also like how the buttons are\n",
      "positioned. I feel like they are easy to use. I like that this is a product that\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n**Text Generation**\\n\")\n",
    "generator_pipeline = pipeline(\"text-generation\", device=device)\n",
    "print_pipeline_info(generator_pipeline)\n",
    "response = \"Dear Jamie, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator_pipeline(prompt, max_length=500)\n",
    "generated_text = outputs[0]['generated_text']\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the response includes the input prompt.  This is typical of text-generation models in HuggingFace, but it's easy to remove the input prompt from the output.  If you read the customer service response you can see that it's not very good.  GPT2, released by OpenAI in 2019, is a large transformer-based language model with 1.5 billion parameters. It was designed to generate coherent and contextually relevant text, but it can sometimes produce outputs that are not entirely accurate or appropriate.  Now there are much better text-generation models available in HuggingFace.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:49:06.524199Z",
     "iopub.status.busy": "2025-08-26T00:49:06.524005Z",
     "iopub.status.idle": "2025-08-26T00:49:07.037119Z",
     "shell.execute_reply": "2025-08-26T00:49:07.036274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(generator_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation with Other LLMs\n",
    "\n",
    "`llm_generate` makes it simple to experiment with different models for text generation.  Here's a generated customer service response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:49:07.038910Z",
     "iopub.status.busy": "2025-08-26T00:49:07.038724Z",
     "iopub.status.idle": "2025-08-26T00:51:19.198922Z",
     "shell.execute_reply": "2025-08-26T00:51:19.198113Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_generation = \"\"\"You are an expert customer service representative. Generate a professional and empathetic response to the following customer feedback. Address the issues mentioned and provide a resolution.\"\"\"\n",
    "user_prompt_generation = f\"Customer Feedback: {text}\\n\\nCustomer service response:\"\n",
    "\n",
    "response_generation = llm_generate('gemini-flash-lite',\n",
    "                                   user_prompt_generation, \n",
    "                                   system_prompt=system_prompt_generation,\n",
    "                                   max_tokens=500)\n",
    "print(response_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text generation models in Lesson 11.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Summarization\n",
    "\n",
    "**Natural Language Processing (NLP) summarization** is the process of condensing a longer text into a shorter, more concise version while retaining its key information. There are two main types of summarization: **extractive** and **abstractive**. **Extractive summarization** selects and highlights the most important sentences or phrases directly from the original text without altering their wording. In contrast, **abstractive summarization** generates a new, rephrased summary that conveys the core meaning of the original content in a more natural and human-like manner. While extractive methods rely on ranking techniques, abstractive approaches often leverage deep learning models for text generation.\n",
    "\n",
    "We'll focus on abstractive summarization using HuggingFace pipelines.  Here we use a summarization model to create a summary of our customer complaint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:19.201077Z",
     "iopub.status.busy": "2025-08-26T00:51:19.200896Z",
     "iopub.status.idle": "2025-08-26T00:51:25.096793Z",
     "shell.execute_reply": "2025-08-26T00:51:25.095614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Summarization**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Tech Haven sent a Samsung Galaxy S24 Ultra to Tech Haven,\n",
      "expecting next-day delivery . The package arrived a week late and contained a\n",
      "Google Pixel 8 Pro instead . The customer service rep was apologetic but said an\n",
      "exchange would take two weeks .'}]\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\", device=-1)\n",
    "print_pipeline_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=100, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:25.099662Z",
     "iopub.status.busy": "2025-08-26T00:51:25.099389Z",
     "iopub.status.idle": "2025-08-26T00:51:25.410301Z",
     "shell.execute_reply": "2025-08-26T00:51:25.409403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(summarization_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART (Bidirectional and Auto-Regressive Transformers) is a denoising autoencoder for pretraining sequence-to-sequence models. It combines the bidirectional context of BERT with the autoregressive nature of GPT, making it highly effective for various NLP tasks, including text generation and summarization (Don't worry, we're going to make sense of many of those terms in future lessons...). `sshleifer/distilbart-cnn-12-6` is a distilled version of the BART model, specifically fine-tuned on the CNN/DailyMail dataset for abstractive summarization tasks. This model is designed to be smaller and faster than the original BART model while retaining most of its performance, making it efficient for generating concise summaries of longer texts.`sshleifer/distilbart-cnn-12-6` is a distilled version of the BART model, specifically fine-tuned on the CNN/DailyMail dataset for abstractive summarization tasks. This model is designed to be smaller and faster than the original BART model while retaining most of its performance, making it efficient for generating concise summaries of longer texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:25.412367Z",
     "iopub.status.busy": "2025-08-26T00:51:25.412177Z",
     "iopub.status.idle": "2025-08-26T00:53:36.020751Z",
     "shell.execute_reply": "2025-08-26T00:53:36.019873Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_summarization = \"\"\"You are an expert summarization model. Summarize the following customer feedback in a concise manner.\"\"\"\n",
    "user_prompt_summarization = f\"Customer Feedback: {text}\\n\\nSummary:\"\n",
    "\n",
    "response_summarization = llm_generate('gemini-flash-lite',\n",
    "                                      user_prompt_summarization, \n",
    "                                      system_prompt=system_prompt_summarization,\n",
    "                                      max_tokens=150)\n",
    "print(response_summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notes on Using LLMs Programatically\n",
    "\n",
    "While LLMs can make great all-purpose NLP tools, their use has some drawbacks as well:\n",
    "\n",
    "1.  They're usually configured to give **human sounding responses** which may not be what you want depending on the task. You'll often have to experiment with the system prompt to get closer to what you want.\n",
    "\n",
    "2.  **LLMs don't always generate the same output.** We'll learn more about text-generation in Lesson 11, but by default LLMs include some randomness in the generated text. You can usually configure the LLM to use lower temperature values to get more deterministic results. In `llm_generate` you can pass `temperature=0` for more consistent outputs.\n",
    "\n",
    "3.  It can be **difficult to get an LLM to format the output** in the way that you want. Carefully crafting the system prompt can help, but often some post-processing of the generated text is also necessary. Recent LLMs such as GPT-4o, Claude, and Gemini can produce output following JSON schemas through their APIs, which we'll explore in later lessons.\n",
    "\n",
    "4.  **LLMs are usually slower than a specialized model.** Especially if you're running the LLM locally. While LLMs continue to improve, often fine-tuning a specialized model is still preferable if you have enough data and resources to do so, but if you don't have much training data or just need something quick, using an LLM programatically can be beneficial.\n",
    "\n",
    "5.  **Few-shot prompting can improve the results** from an LLM. Providing one or more examples in the prompt can improve the LLM response. You'll explore this a bit in the homework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestion:** Try changing the model in the LLM cells above to see how different models perform. For example, change `'gemini-flash-lite'` to `'gpt-4o-mini'`, `'claude-haiku'`, or `'llama-3.3-70b'` in any of the `llm_generate()` calls and rerun the cell to compare results. You can see all available models with `llm_list_models()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds776_env)",
   "language": "python",
   "name": "ds776_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a6c1986350b4284896cf5a579bb3a8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "10cc2ca6a64843409aa239e2f8f41afb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "770d1da8bddc4c7491f35d73be7a2710": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "81c7470b96b845b58123c99d7784dec4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c643400f0b0740759329f64ba59ec6cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d03d872c8a2f470dbc9b111716bfa27f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d17218daac884b819a72c55b2a5679a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d03d872c8a2f470dbc9b111716bfa27f",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_0a6c1986350b4284896cf5a579bb3a8a",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá10/10‚Äá[00:15&lt;00:00,‚Äá‚Äá1.63s/it]"
      }
     },
     "d41731cd0ee44bdbb881cfe963f96552": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81c7470b96b845b58123c99d7784dec4",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_e443047dffdb42bc9ad35d52008be058",
       "tabbable": null,
       "tooltip": null,
       "value": "Local‚ÄáGeneration:‚Äá100%"
      }
     },
     "e443047dffdb42bc9ad35d52008be058": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eb5f6b4c2d6c4f2b915d937cffbd829d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_10cc2ca6a64843409aa239e2f8f41afb",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c643400f0b0740759329f64ba59ec6cd",
       "tabbable": null,
       "tooltip": null,
       "value": 10
      }
     },
     "f65ef90340c64f5398337dc7bec9aa19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d41731cd0ee44bdbb881cfe963f96552",
        "IPY_MODEL_eb5f6b4c2d6c4f2b915d937cffbd829d",
        "IPY_MODEL_d17218daac884b819a72c55b2a5679a1"
       ],
       "layout": "IPY_MODEL_770d1da8bddc4c7491f35d73be7a2710",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
