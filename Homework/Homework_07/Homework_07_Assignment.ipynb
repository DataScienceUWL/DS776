{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Homework 7: Exploring Hugging Face Pipelines and LLM Prompting\n\nIn this assignment, you will explore different NLP tasks using Hugging Face's transformers pipelines and LLM-based prompting with `llm_generate()`. You will experiment with different models, zero-shot prompting, and compare results across approaches.\n\n**Total Points: 40**\n- Part 1 (Sentiment Analysis): 4 points\n- Part 2 (Named Entity Recognition): 6 points  \n- Part 3 (Text Generation): 6 points\n- Part 4 (Translation): 6 points\n- Part 5 (Summarization): 8 points\n- Part 6 (Sarcasm Detection): 8 points\n- Part 7 (Reflection): 2 points"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR IMPORTS HERE\n# Add any additional imports you need below this line\n\nimport os\nimport torch\nfrom transformers import pipeline\nfrom introdl import (\n    get_device,\n    wrap_print_text,\n    config_paths_keys,\n    llm_generate,\n    clear_pipeline,\n    print_pipeline_info,\n    display_markdown, \n    show_session_spending \n)\n# Wrap print to format text nicely at 120 characters\nprint = wrap_print_text(print, width=120)\n\ndevice = get_device()\n\n# Configure paths\npaths = config_paths_keys()\nDATA_PATH = paths['DATA_PATH']\nMODELS_PATH = paths['MODELS_PATH']"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes About Using LLMs Programmatically\n",
    "\n",
    "**Using `llm_generate()` for all LLM tasks:**\n",
    "- Use `llm_generate()` with `'gemini-flash-lite'` as your default model (fast and cost-effective)\n",
    "- For each task, also try at least **one other model** to compare results (e.g., `'gpt-4o-mini'`, `'mistral-medium'`, `'llama-3.3-70b'`)\n",
    "- You can pass `temperature=0` to get more deterministic (reproducible) responses\n",
    "- Use `mode='json'` when you need structured JSON output\n",
    "\n",
    "**System and User Prompts:**\n",
    "- Use the **system prompt** to set the overall behavior (e.g., \"You are a sentiment analysis expert\")\n",
    "- Use the **user prompt** for specific instructions and the text to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_07_Models/` | `Homework_07_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` ‚Äî never hardcode paths like `'Homework_07_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Texts for Tasks 1 and 2\n",
    "\n",
    "texts = [\n",
    "    \"The new AI technology developed by OpenAI is revolutionizing various industries, from healthcare to finance.\",\n",
    "    \"Marie Curie was a physicist and chemist who conducted research on radioactivity.\",\n",
    "    \"In 2023, NASA successfully landed another rover on Mars, aiming to explore signs of ancient life.\",\n",
    "    \"The recent advancements in quantum computing by IBM have the potential to solve complex problems that are currently unsolvable with classical computers.\",\n",
    "    \"Despite the company's efforts, the new product launch by XYZ Corp was a complete failure, leading to significant financial losses and a drop in stock prices.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1 - Sentiment Analysis (4 pts)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Use the default sentiment analysis pipeline from HuggingFace to determine the sentiment of each text. Use `clear_pipeline()` to free memory when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** Now use `llm_generate()` with `'gemini-flash-lite'` to perform sentiment analysis. Create appropriate system and user prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c.** Try a different model with `llm_generate()` for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1d.** Which approach (HuggingFace pipeline or which LLM model) best captures the sentiments? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2 - Named Entity Recognition (6 pts)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a.** Apply the default HuggingFace NER pipeline to each of the texts. Display results in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Use `llm_generate()` with `mode='json'` to get structured NER output. Use `'gemini-flash-lite'` and craft prompts to return JSON with entity information. Display results as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# Hint: Use mode='json' and describe the JSON structure you want in the prompt"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** Try a different LLM model with JSON mode and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3 - Text Generation (6 pts)\n\nThink of a short creative task (e.g., writing an advertisement, lyrics for a jingle, a product description, etc.)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Use the default HuggingFace text generation pipeline for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Use `llm_generate()` with two different models to perform the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model 1: gemini-flash-lite\n# YOUR CODE HERE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model 2: (your choice)\n# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Which approach produced the best result? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4 - Translation (6 pts)\n\nPick your own short text (at least 3 sentences) and translate it to another language (not Spanish) and back, then compare."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4a.** Use HuggingFace translation pipelines. Search for an appropriate model on HuggingFace Hub for your chosen language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# Translate to target language\n# Translate back to English\n# Compare"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Use `llm_generate()` with two different models to translate to your chosen language and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Which works better - the specialized HuggingFace model or the LLMs? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 5 - Summarization (8 pts)\n\nFor this task you'll generate summaries of [\"The Bitter Lesson\"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) by Rich Sutton. This is an important paper about deep learning that you should read!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text from \"The Bitter Lesson\"\n",
    "# You may need to: !pip install beautifulsoup4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the text from the webpage\n",
    "text = soup.get_text()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5a.** Use the default HuggingFace summarization pipeline. Note: \"The Bitter Lesson\" is too long for the default model. Split the text roughly in half and summarize each half separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b.** Use `llm_generate()` with `'gemini-flash-lite'` to summarize the entire article (no need to split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c.** Try a different LLM model for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5d.** Compare all three summaries. Which one seems best and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 6 - Sarcasm Detection (8 pts)\n\nThe Sarcasm News Headlines dataset contains headlines labeled as sarcastic (1) or not sarcastic (0)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sarcasm News Headlines Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"raquiba/Sarcasm_News_Headline\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df = df.rename(columns={'is_sarcastic': 'label'})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6a.** Use `llm_generate()` with `'gemini-flash-lite'` to classify the first 10 headlines as sarcastic or not.  You should build the prompts programatically as shown in the lesson notebooks. Compare to actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6b.** Try a different model for sarcasm detection and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6c.**  Arguably, many of the examples in the dataset are ironic and not sarcastic.  (Irony pertains to situations while sarcasm is a form of expression.)  Try prompting your 'gemini-flash-lite' to be an irony detector to see if it performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE (optional)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6d.** Which model and/or prompting approach performs best?  Give a brief explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 7 - Reflection (2 pts)\n\n1. What, if anything, did you find difficult to understand for the lesson? Why?\n\nüìù **YOUR ANSWER HERE:**\n\n2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n\nüìù **YOUR ANSWER HERE:**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds776_env)",
   "language": "python",
   "name": "ds776_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}