{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 3 Reading Quiz\n",
    "\n",
    "Read Inside Deep Learning Chapter X - ...\n",
    "\n",
    "#### 1. What is the primary purpose of data augmentation in training neural networks?\n",
    "A. To artificially expand the training dataset  \n",
    "B. To simplify the model's architecture  \n",
    "C. To increase the number of model parameters  \n",
    "D. To reduce overfitting by shrinking the dataset  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Which of the following is a common data augmentation technique for images?\n",
    "A. Label smoothing  \n",
    "B. Weight decay  \n",
    "C. Cropping and flipping  \n",
    "D. Batch normalization  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Data augmentation is particularly useful for:\n",
    "A. Increasing the accuracy of models on small datasets  \n",
    "B. Reducing the need for a validation set  \n",
    "C. Improving the performance of models on large datasets without overfitting  \n",
    "D. Simplifying model training by reducing computational requirements  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4. What is the role of a learning rate schedule in neural network training?\n",
    "A. To adjust the batch size  \n",
    "B. To control how the learning rate changes during training  \n",
    "C. To set a fixed learning rate for the entire training process  \n",
    "D. To reset weights after every epoch  \n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Which learning rate schedule gradually decreases the learning rate with smoother transitions to better final convergence?\n",
    "A. Step drop  \n",
    "B. Exponential decay  \n",
    "C. Cosine annealing  \n",
    "D. Constant schedule  \n",
    "\n",
    "---\n",
    "\n",
    "#### 6. What is the main reason for using momentum in stochastic gradient descent (SGD)?\n",
    "A. To escape small local minima  \n",
    "B. To stabilize exploding gradients  \n",
    "C. To reduce variance in weight updates  \n",
    "D. To incorporate the learning rate in updates  \n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Why is gradient clipping used in training neural networks?\n",
    "A. To control gradient explosion in deep networks  \n",
    "B. To speed up convergence  \n",
    "C. To eliminate the need for batch normalization  \n",
    "D. To reduce overfitting by limiting the gradient flow  \n",
    "\n",
    "---\n",
    "\n",
    "#### 8. Which optimizer combines momentum with adaptive learning rates?\n",
    "A. RMSProp  \n",
    "B. Adam  \n",
    "C. AdaGrad  \n",
    "D. SGD with momentum  \n",
    "\n",
    "---\n",
    "\n",
    "#### 9. What is the benefit of using a \"validation plateau\" learning rate schedule?\n",
    "A. It increases the learning rate when overfitting occurs  \n",
    "B. It reduces the learning rate based on validation performance  \n",
    "C. It adjusts the batch size according to validation loss  \n",
    "D. It fixes the learning rate during overfitting  \n",
    "\n",
    "---\n",
    "\n",
    "#### 10. How does SGD with momentum improve the optimization process?\n",
    "A. It reduces the need for data augmentation  \n",
    "B. It accelerates convergence by incorporating past gradients  \n",
    "C. It completely eliminates gradient updates  \n",
    "D. It prevents overfitting by using smaller batches  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
