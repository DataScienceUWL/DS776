{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7: Introduction to Transformers, Hugging Face, and Using LLMs Effectively\n",
    "\n",
    "### Topics\n",
    "* Transformer architecture overview\n",
    "* Self-attention mechanism\n",
    "* Hugging Face library introduction\n",
    "* Effective use of large language models (LLMs) and prompt engineering basics\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "The student will be able to:\n",
    "\n",
    "1. **Describe Transformer Architecture**: Identify the core components of transformers, including the encoder-decoder structure and the role of self-attention.\n",
    "\n",
    "2. **Explain Self-Attention Basics**: Outline how self-attention works, including the function of queries, keys, and values, and the advantages it provides for handling context.\n",
    "\n",
    "3. **Use Pre-trained Models**: Load and use a pre-trained Hugging Face model for a simple NLP task to gain familiarity with the Hugging Face transformers library.\n",
    "\n",
    "4. **Improve Prompting Techniques**: Experiment with prompt engineering to enhance response quality when interacting with LLMs, learning to refine prompts for clarity and relevance.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read **Chapter 1: Hello Transformers** in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_07 directory and watch the embedded videos in the recommended order. Link to [CoCalc](https://cocalc.com)\n",
    "\n",
    "### Assessment\n",
    "Complete the exercises in your homework notebook in CoCalc (50 points). Upload the HTML version of your notebook to the assignment in Canvas to get credit.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
