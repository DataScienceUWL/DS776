{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR IMPORTS HERE ===\n# Add any additional imports you need below this line\n\nfrom introdl import (\n    config_paths_keys,\n    get_device,\n    wrap_print_text\n)\n\n# Wrap print to format text nicely at 120 characters\nprint = wrap_print_text(print, width=120)\n\ndevice = get_device()\n\n# Configure paths\npaths = config_paths_keys()\nDATA_PATH = paths['DATA_PATH']\nMODELS_PATH = paths['MODELS_PATH']\n# === END YOUR IMPORTS ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8:  Sarcasm Detection\n\nUsing the [\"Sarcasm_News_Headline\" dataset](https://huggingface.co/datasets/raquiba/Sarcasm_News_Headline) on HuggingFace you're going to try several approaches to sarcasm detection (text classification) and write a summary at the end.\n\n**Total Points: 50**\n- Reading Questions: 8 points\n- Download and split dataset: 2 points\n- Approach 1 (TF-IDF + ML Model): 7 points\n- Approach 2 (Pretrained Model): 6 points\n- Approach 3 (Fine-tune DistilBERT): 7 points\n- Approach 4 Part 1 (LLM Zero-Shot): 7 points\n- Approach 4 Part 2 (LLM Few-Shot): 7 points\n- Summarize and Compare: 4 points\n- Reflection: 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Questions (8 points)\n\nAnswer the following questions based on Chapter 3: Text Classification from *Natural Language Processing with Transformers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (2 points):** What are the three main advantages of DistilBERT over the original BERT model? How does DistilBERT achieve these improvements while maintaining most of BERT's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (2 points):** Compare the three main tokenization strategies discussed in the chapter: character tokenization, word tokenization, and subword tokenization (WordPiece). What are the key advantages and disadvantages of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (2 points):** Explain the difference between the feature extraction and fine-tuning approaches for transfer learning. When would you choose feature extraction over fine-tuning, and what are the trade-offs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (2 points):** The chapter demonstrates using loss-based sorting as an error analysis technique to identify mislabeled examples and difficult cases. Explain how this technique works and why sorting by prediction loss is effective for finding problematic examples in your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and split the dataset (2 points)\n\nWhile the dataset has \"train\" and \"test\" splits, ignore the \"test\" split since it almost entirely duplicates the \"train\" split.\n\nInstead, use train_test_split with a seed of 42 to generate an 80/20 split of the original \"train\" split into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Sarcasm News Headline dataset\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\n\n# Your code here:\n# 1. Load the dataset using load_dataset(\"raquiba/Sarcasm_News_Headline\")\n# 2. Get the 'train' split (ignore 'test' as it duplicates 'train')\n# 3. Extract headlines and labels into lists or arrays\n# 4. Use train_test_split with test_size=0.2 and random_state=42\n# 5. Print the sizes of train and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 1 - TF-IDF Vectors + ML Model (7 points)\n\nInclude code to create TF-IDF Vectors that represent each headline. Use these vectors to train a classification model (it doesn't have to be Logistic Regression). Make predictions on the test set and generate a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create TF-IDF vectors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Your code here:\n# 1. Create a TfidfVectorizer (consider max_features parameter)\n# 2. Fit on training headlines and transform both train and test\n# 3. Print the shape of the resulting vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train a classification model\nfrom sklearn.linear_model import LogisticRegression\n# Or try: from sklearn.ensemble import RandomForestClassifier\n# Or try: from sklearn.naive_bayes import MultinomialNB\n\n# Your code here:\n# 1. Create your classifier (e.g., LogisticRegression)\n# 2. Train on the TF-IDF vectors and labels\n# 3. Make predictions on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluate with classification report\nfrom sklearn.metrics import classification_report\n\n# Your code here:\n# 1. Generate classification report comparing predictions to test labels\n# 2. Print the report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 2 - Use a Pretrained Model from HuggingFace (6 points)\n\nBefore fine-tuning your own model, search HuggingFace Hub for existing models trained on sarcasm detection.\n\n**Task:**\n1. Find a pretrained model for sarcasm detection on HuggingFace (search for \"sarcasm\")\n2. Use the `pipeline()` API to load the model\n3. Make predictions on your test set\n4. Generate a classification report\n\n**Hints:**\n- Search HuggingFace for \"sarcasm detection\" or \"sarcasm classification\"\n- Look for models with high downloads or recent updates\n- Check the model card to verify it matches your task (news headlines vs tweets)\n- Some models to explore: `helinivan/english-sarcasm-detector`\n\n**Example code structure:**\n```python\nfrom transformers import pipeline\n\n# Load pretrained model\nclassifier = pipeline(\"text-classification\", model=\"model-name-here\")\n\n# Make predictions\n# predictions = ...\n\n# Generate classification report\n```\n\n**What to report:**\n- Which model did you choose and why?\n- What accuracy does it achieve on your test set?\n- How does it compare to your TF-IDF approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Search and load a pretrained sarcasm detection model\nfrom transformers import pipeline\n\n# Your code here:\n# 1. Search HuggingFace for \"sarcasm detection\" models\n# 2. Choose a model (e.g., \"helinivan/english-sarcasm-detector\")\n# 3. Load it using: classifier = pipeline(\"text-classification\", model=\"...\")\n# 4. Test on 1-2 examples to verify it works\n\n# Example:\n# classifier = pipeline(\"text-classification\", model=\"your-model-name\")\n# test = classifier([\"This is a test headline.\"])\n# print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Make predictions on test set\n\n# Your code here:\n# 1. Use the pipeline to predict on all test headlines\n# 2. Extract the predicted labels (may need to map model's labels to 0/1)\n# 3. Handle batch processing if needed (pipeline can take lists)\n\n# Hint: The model might return labels like \"SARCASM\" and \"NOT_SARCASM\"\n# You may need to map these to 0 and 1 to match your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate classification report\nfrom sklearn.metrics import classification_report\n\n# Your code here:\n# 1. Generate classification report\n# 2. Print the report\n# 3. Note the model name and accuracy for comparison later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 3 - Fine-tune DistilBERT with Classification Head (7 points)\n\nInclude code for set up, training, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare dataset for HuggingFace\nfrom datasets import Dataset\nimport pandas as pd\n\n# Your code here:\n# 1. Create DataFrames or dictionaries with 'text' and 'label' columns\n# 2. Convert to HuggingFace Dataset format using Dataset.from_pandas() or Dataset.from_dict()\n# 3. Create a DatasetDict with 'train' and 'test' splits\n\n# Example structure:\n# train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n# test_df = pd.DataFrame({'text': test_texts, 'label': test_labels})\n# train_dataset = Dataset.from_pandas(train_df)\n# test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load tokenizer and tokenize dataset\nfrom transformers import AutoTokenizer\n\n# Your code here:\n# 1. Load DistilBERT tokenizer: AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n# 2. Create a tokenize function that tokenizes the 'text' field\n# 3. Use dataset.map() to tokenize both train and test datasets\n# 4. Set truncation=True and padding=True\n\n# Example:\n# def tokenize_function(examples):\n#     return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n# \n# tokenized_train = train_dataset.map(tokenize_function, batched=True)\n# tokenized_test = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load model and set up training\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\n# Your code here:\n# 1. Load DistilBERT model: AutoModelForSequenceClassification.from_pretrained(\n#       \"distilbert-base-uncased\", num_labels=2)\n# 2. Create TrainingArguments with:\n#    - output_dir for saving checkpoints\n#    - num_train_epochs (2-3 is typical)\n#    - per_device_train_batch_size (8 or 16)\n#    - evaluation_strategy=\"epoch\"\n# 3. Create Trainer with model, args, train_dataset, eval_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n\n# Your code here:\n# 1. Call trainer.train()\n# 2. This will take several minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make predictions and evaluate\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\n# Your code here:\n# 1. Use trainer.predict() on the test dataset\n# 2. Extract predictions: np.argmax(predictions.predictions, axis=1)\n# 3. Generate classification report\n# 4. Print the report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 4 - Part 1: Use an LLM Model and Zero-Shot Prompt (7 points)\n\nUsing the `llm_classifier` helper function from the lesson apply your LLM classifier to the first 100 examples in the test set. Use a local model and an API-based model for comparison. For the API-based model some possibilities include:\n* Groq: \"llama3-70b-8192\", (rate_limit = 30 requests per minute on free tier)\n* Together.AI: \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\" (rate limit = 10)\n* Gemini: \"gemini-flash-lite\" (rate_limit = 30 on free tier)\n\nFeel free to try others if you have access.\n\nProduce classification reports for both the local and best API-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the llm_classifier helper\nfrom Lesson_08_Helpers import llm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare test subset (first 100 examples)\n\n# Your code here:\n# 1. Select first 100 test headlines\n# 2. Get corresponding labels\n# 3. Print a few examples to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create zero-shot prompts for sarcasm detection\n\n# Your code here:\n# 1. Create a system_prompt (e.g., \"You are an expert at detecting sarcasm in news headlines.\")\n# 2. Create a prompt_template with:\n#    - Clear instructions to classify as 'sarcastic' or 'not sarcastic'\n#    - A placeholder {text} for the headline\n#    - Request to output ONLY the label\n\n# Example structure:\n# system_prompt = \"...\"\n# prompt_template = \"\"\"\n# Classify the following news headline as either 'sarcastic' or 'not sarcastic'.\n# Output ONLY one of: sarcastic, not sarcastic\n# \n# Headline: {text}\n# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use API-based model (e.g., gemini-flash-lite)\n\n# Your code here:\n# 1. Call llm_classifier with:\n#    - model_name='gemini-flash-lite' (or another API model)\n#    - texts (first 100 headlines)\n#    - system_prompt\n#    - prompt_template\n#    - estimate_cost=True to see API costs\n# 2. Store predictions\n\n# Example:\n# predictions_api = llm_classifier(\n#     'gemini-flash-lite',\n#     test_texts_100,\n#     system_prompt,\n#     prompt_template,\n#     estimate_cost=True\n# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert text predictions to 0/1 labels\n\n# Your code here:\n# 1. Map text predictions to 0/1\n#    'not sarcastic' \u2192 0\n#    'sarcastic' \u2192 1\n# 2. Handle any unexpected responses (strip whitespace, lowercase, etc.)\n\n# Example:\n# predictions_api_binary = []\n# for pred in predictions_api:\n#     pred_clean = pred.strip().lower()\n#     if 'not sarcastic' in pred_clean:\n#         predictions_api_binary.append(0)\n#     elif 'sarcastic' in pred_clean:\n#         predictions_api_binary.append(1)\n#     else:\n#         predictions_api_binary.append(0)  # default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate classification report for API model\nfrom sklearn.metrics import classification_report\n\n# Your code here:\n# 1. Generate classification report\n# 2. Print the report\n\nprint(\"API Model Results (gemini-flash-lite):\")\n# print(classification_report(...))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Try a local model (optional but recommended)\n\n# Your code here:\n# 1. Use llm_classifier with a local model\n#    Options: 'llama-3.2', or any locally hosted model\n# 2. Same system_prompt and prompt_template\n# 3. Convert predictions to 0/1\n# 4. Generate classification report\n\n# Note: Local models may be slower but are free\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 4 - Part 2: Use an LLM Model and Few-Shot Prompt (7 points)\n\nBuild a few-shot prompt with three to five examples of each class and apply the same models used for the zero-shot prompt. Produce classification reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create few-shot examples\n\n# Your code here:\n# 1. Select 3-5 examples of sarcastic headlines\n# 2. Select 3-5 examples of non-sarcastic headlines\n# 3. Format them as examples in your prompt\n\n# Example structure:\n# few_shot_examples = \"\"\"\n# Examples:\n# \n# Headline: \"Local man thinks he's qualified to run country after playing SimCity once\"\n# Classification: sarcastic\n# \n# Headline: \"Scientists discover new species of frog in Amazon rainforest\"\n# Classification: not sarcastic\n# \n# [Add more examples...]\n# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create few-shot prompt template\n\n# Your code here:\n# 1. Update prompt_template to include the few-shot examples\n# 2. Keep the same classification instructions\n# 3. Add the {text} placeholder for new headlines\n\n# Example:\n# prompt_template_fewshot = few_shot_examples + \"\"\"\n# Now classify this headline:\n# \n# Headline: {text}\n# Classification:\n# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply few-shot prompting with API model\n\n# Your code here:\n# 1. Use llm_classifier with the same API model\n# 2. Use the new few-shot prompt template\n# 3. Same 100 test headlines\n# 4. Convert predictions to 0/1\n# 5. Generate classification report\n\n# predictions_fewshot_api = llm_classifier(\n#     'gemini-flash-lite',\n#     test_texts_100,\n#     system_prompt,\n#     prompt_template_fewshot,\n#     estimate_cost=True\n# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply few-shot prompting with second model\n\n# Your code here:\n# 1. Try the same few-shot approach with your second model\n# 2. Generate classification report\n# 3. Compare zero-shot vs few-shot performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize and Compare All Approaches (4 points)\n\nCreate a summary table comparing all four approaches on the following dimensions:\n\n| Approach | Model/Method | Accuracy | Training Time | Inference Speed | Key Advantages | Key Disadvantages |\n|----------|--------------|----------|---------------|-----------------|----------------|-------------------|\n| 1. TF-IDF + ML | ... | ...% | ... | ... | ... | ... |\n| 2. Pretrained | ... | ...% | ... | ... | ... | ... |\n| 3. Fine-tuned DistilBERT | ... | ...% | ... | ... | ... | ... |\n| 4a. LLM Zero-shot | ... | ...% | ... | ... | ... | ... |\n| 4b. LLM Few-shot | ... | ...% | ... | ... | ... | ... |\n\n**Discussion Questions:**\n\n1. Which approach performed best? Why do you think this is?\n2. Compare Approach 2 (pretrained) vs Approach 3 (fine-tuned). Which was easier? Which performed better? When would you choose one over the other?\n3. How did few-shot prompting (4b) compare to zero-shot (4a)? Was the improvement worth the extra effort?\n4. If you were deploying a sarcasm detection system in production, which approach would you choose and why? Consider accuracy, speed, cost, and maintainability.\n\n\ud83d\udcdd **YOUR ANSWERS HERE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\nimport pandas as pd\n\n# Your code here:\n# 1. Create a DataFrame with columns:\n#    - Approach (name)\n#    - Model/Method\n#    - Accuracy\n#    - Training Time (approximate)\n#    - Inference Speed (Fast/Medium/Slow)\n# 2. Fill in your results from each approach\n# 3. Display the table\n\n# Example structure:\n# results = {\n#     'Approach': ['1. TF-IDF + ML', '2. Pretrained', '3. Fine-tuned DistilBERT', \n#                  '4a. LLM Zero-shot', '4b. LLM Few-shot'],\n#     'Model/Method': ['Logistic Regression', '...', 'distilbert-base-uncased', \n#                      'gemini-flash-lite', 'gemini-flash-lite'],\n#     'Accuracy': [0.XX, 0.XX, 0.XX, 0.XX, 0.XX],\n#     'Training Time': ['< 1 min', '0 (pretrained)', '5-10 min', '0', '0'],\n#     'Inference Speed': ['Very Fast', 'Fast', 'Fast', 'Slow (API)', 'Slow (API)']\n# }\n# \n# df_results = pd.DataFrame(results)\n# display(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection (2 points)\n\n1. What, if anything, did you find difficult to understand for the lesson? Why?\n\n\ud83d\udcdd **YOUR ANSWER HERE:**\n\n2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n\n\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n\nUncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}