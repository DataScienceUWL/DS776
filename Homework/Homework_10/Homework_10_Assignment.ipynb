{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "# Homework 10: Named Entity Recognition\n\n**Name:** [Your Name Here]  \n**Total Points: 40**\n\n## Submission Checklist\n- [ ] All code cells executed with output saved\n- [ ] All questions answered in markdown cells\n- [ ] Used `DATA_PATH` and `MODELS_PATH` variables (no hardcoded paths)\n- [ ] Per-entity metrics visualization created\n- [ ] Comparison of fine-tuned models vs LLM completed\n- [ ] Reflection questions answered\n- [ ] Notebook exported to HTML\n- [ ] Canvas filename includes `_GRADE_THIS_ONE`\n- [ ] Files uploaded to Canvas\n\n---\n\n**Point Breakdown:**\n- Part 1 (Named Entity Analysis): 6 pts\n- Part 2 (Fine-tune BERT Models): 12 pts\n- Part 3 (LLM for NER): 12 pts\n- Part 4 (Comparison): 6 pts\n- Part 5 (Reflection): 2 pts"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "# Step 1: Load the dataset\nfrom datasets import load_dataset\n\n# YOUR CODE HERE\n# 1. Load the \"hobbes99/mit-movie-ner-simplified\" dataset\n# 2. Examine the 'train' and 'valid' splits\n# 3. Print an example to see the structure\n# 4. Get the label names from the dataset features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Prepare for fine-tuning - Tokenization\nfrom transformers import AutoTokenizer\n\n# YOUR CODE HERE\n# 1. Load two tokenizers:\n#    - \"distilbert-base-uncased\"\n#    - \"bert-base-uncased\"\n# 2. Create a tokenization function that:\n#    - Tokenizes the tokens (use is_split_into_words=True)\n#    - Aligns the NER labels with tokenized output\n#    - Handles subword tokens (use -100 for ignored tokens)\n# 3. Apply tokenization to both train and valid splits\n\n# Hint: word_ids() method helps align labels with subword tokens"
  },
  {
   "cell_type": "markdown",
   "id": "0aqsq06cidrl",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_10_Models/` | `Homework_10_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` ‚Äî never hardcode paths like `'Homework_10_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "# Step 3: Set up metrics for evaluation\nimport evaluate\nimport numpy as np\n\n# YOUR CODE HERE\n# 1. Load the seqeval metric\n# 2. Create a compute_metrics function that:\n#    - Extracts predictions and labels\n#    - Removes ignored tokens (-100)\n#    - Converts numeric labels to string labels\n#    - Computes precision, recall, F1 overall and per-entity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Fine-tune distilbert-base-uncased\nfrom transformers import (\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForTokenClassification\n)\n\n# YOUR CODE HERE\n# 1. Load the model with num_labels matching your dataset\n# 2. Create TrainingArguments (output_dir, num_epochs, batch_size, etc.)\n# 3. Create a DataCollator for token classification\n# 4. Create Trainer with model, args, datasets, tokenizer, data_collator, compute_metrics\n# 5. Train the model"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "# Step 5: Evaluate distilbert and get per-entity metrics\n\n# YOUR CODE HERE\n# 1. Run evaluation on the valid set\n# 2. Get predictions for detailed analysis\n# 3. Calculate precision, recall, F1 for each entity type\n# 4. Create a visualization (bar chart) comparing metrics by entity\n\n# Hint: Use trainer.predict() and seqeval.compute() for detailed results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Extract entities from training set\n\n# YOUR CODE HERE\n# 1. Create empty lists or dictionaries to store:\n#    - Actors with positive sentiment\n#    - Actors with negative sentiment\n#    - Directors with positive sentiment\n#    - Directors with negative sentiment\n# 2. Loop through the training dataset\n# 3. For each example, extract:\n#    - The sentiment (0 or 1)\n#    - Tokens tagged as B-ACTOR or I-ACTOR\n#    - Tokens tagged as B-DIRECTOR or I-DIRECTOR\n# 4. Keep track of counts for each actor/director by sentiment\n\n# Hint: You'll need to:\n# - Convert token lists to strings (join consecutive I- tags with B- tag)\n# - Track how many times each actor appears in positive vs negative reviews\n# - Use label_list to convert numeric tags to string labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Calculate proportions and find top 3\n\n# YOUR CODE HERE\n# 1. For each actor/director, calculate:\n#    total_appearances = positive_count + negative_count\n#    positive_proportion = positive_count / total_appearances\n# 2. Sort by positive proportion (descending for positive, ascending for negative)\n# 3. Select top 3 for each category\n# 4. Display results with counts and proportions"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "# Step 6: Create visualization of per-entity performance\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# YOUR CODE HERE\n# 1. Extract precision, recall, F1 for each entity type\n# 2. Create a DataFrame with entity types and metrics\n# 3. Plot a grouped bar chart comparing precision, recall, F1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load the \"hobbes99/mit-movie-ner-simplified\" dataset\n",
    "# 2. Examine the 'train' and 'valid' splits\n",
    "# 3. Print an example to see the structure\n",
    "# 4. Get the label names from the dataset features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare for fine-tuning - Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load two tokenizers:\n",
    "#    - \"distilbert-base-uncased\"\n",
    "#    - \"bert-base-uncased\"\n",
    "# 2. Create a tokenization function that:\n",
    "#    - Tokenizes the tokens (use is_split_into_words=True)\n",
    "#    - Aligns the NER labels with tokenized output\n",
    "#    - Handles subword tokens (use -100 for ignored tokens)\n",
    "# 3. Apply tokenization to both train and valid splits\n",
    "\n",
    "# Hint: word_ids() method helps align labels with subword tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up metrics for evaluation\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load the seqeval metric\n",
    "# 2. Create a compute_metrics function that:\n",
    "#    - Extracts predictions and labels\n",
    "#    - Removes ignored tokens (-100)\n",
    "#    - Converts numeric labels to string labels\n",
    "#    - Computes precision, recall, F1 overall and per-entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fine-tune distilbert-base-uncased\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load the model with num_labels matching your dataset\n",
    "# 2. Create TrainingArguments (output_dir, num_epochs, batch_size, etc.)\n",
    "# 3. Create a DataCollator for token classification\n",
    "# 4. Create Trainer with model, args, datasets, tokenizer, data_collator, compute_metrics\n",
    "# 5. Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate distilbert and get per-entity metrics\n",
    "\n",
    "# Your code here:\n",
    "# 1. Run evaluation on the valid set\n",
    "# 2. Get predictions for detailed analysis\n",
    "# 3. Calculate precision, recall, F1 for each entity type\n",
    "# 4. Create a visualization (bar chart) comparing metrics by entity\n",
    "\n",
    "# Hint: Use trainer.predict() and seqeval.compute() for detailed results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create visualization of per-entity performance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Your code here:\n",
    "# 1. Extract precision, recall, F1 for each entity type\n",
    "# 2. Create a DataFrame with entity types and metrics\n",
    "# 3. Plot a grouped bar chart comparing precision, recall, F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Repeat for bert-base-uncased\n",
    "\n",
    "# Your code here:\n",
    "# 1. Tokenize the dataset with bert-base-uncased tokenizer\n",
    "# 2. Load bert-base-uncased model\n",
    "# 3. Create new Trainer with BERT model\n",
    "# 4. Train the BERT model\n",
    "# 5. Evaluate and compare with DistilBERT\n",
    "\n",
    "# (Follow same steps as DistilBERT - cells above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Run inference on movie reviews from internet\n",
    "from transformers import pipeline\n",
    "\n",
    "# Your code here:\n",
    "# 1. Find 2 movie reviews from the internet (IMDB, Rotten Tomatoes, etc.)\n",
    "# 2. Create a NER pipeline with your fine-tuned model\n",
    "# 3. Run inference on the reviews\n",
    "# 4. Display the extracted entities with their types\n",
    "\n",
    "# Hint: Use pipeline(\"ner\", model=your_model, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Compare DistilBERT vs BERT\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create a comparison table with:\n",
    "#    - Model name\n",
    "#    - Overall F1 score\n",
    "#    - Training time (approximate)\n",
    "#    - Model size (parameters)\n",
    "#    - Best/worst entity types\n",
    "# 2. Discuss which model performed better\n",
    "# 3. Analyze whether BERT's larger size justified better performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "# Step 7: Repeat for bert-base-uncased\n\n# YOUR CODE HERE\n# 1. Tokenize the dataset with bert-base-uncased tokenizer\n# 2. Load bert-base-uncased model\n# 3. Create new Trainer with BERT model\n# 4. Train the BERT model\n# 5. Evaluate and compare with DistilBERT\n\n# (Follow same steps as DistilBERT - cells above)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare validation subset\n",
    "from Lesson_10_Helpers import llm_ner_extractor\n",
    "\n",
    "# Your code here:\n",
    "# 1. Get first 100 examples from valid split\n",
    "# 2. Extract texts and true labels\n",
    "# 3. Start with 2-3 examples to test your prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Design prompt for LLM-based NER\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create system_prompt explaining the task\n",
    "# 2. Create prompt_template with:\n",
    "#    - Instructions to extract entities\n",
    "#    - Entity types to look for: Actor, Character, Director, Genre, Title, Year\n",
    "#    - Request structured output (JSON format recommended)\n",
    "#    - Include the {text} placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Test prompt on small subset with llm_ner_extractor\n\n# YOUR CODE HERE\n# 1. Use llm_ner_extractor function (similar to llm_classifier from Lesson 8)\n# 2. Start with a small API model or local model\n# 3. Test on 3-5 examples\n# 4. Refine your prompt based on results\n# 5. Parse JSON output and convert to BIO format\n\n# Hint: Use llm_generate() to test prompt formatting before scaling up"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Convert LLM outputs to BIO format\n\n# YOUR CODE HERE\n# 1. Parse JSON from LLM response\n# 2. Match entity spans to token positions\n# 3. Convert to BIO format (B-ACTOR, I-ACTOR, etc.)\n# 4. Handle errors/malformed JSON gracefully\n\n# Hint: llm_ner_extractor from Lesson_10_Helpers handles conversion for you"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Scale up to 100 examples and evaluate\n\n# YOUR CODE HERE\n# 1. Once prompt is refined, run on all 100 validation examples\n# 2. Convert predictions to BIO format\n# 3. Calculate metrics using seqeval (like Part 2)\n# 4. Generate classification report"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "# Step 8: Run inference on movie reviews from internet\nfrom transformers import pipeline\n\n# YOUR CODE HERE\n# 1. Find 2 movie reviews from the internet (IMDB, Rotten Tomatoes, etc.)\n# 2. Create a NER pipeline with your fine-tuned model\n# 3. Run inference on the reviews\n# 4. Display the extracted entities with their types\n\n# Hint: Use pipeline(\"ner\", model=your_model, aggregation_strategy=\"simple\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "# Step 9: Compare DistilBERT vs BERT\n\n# YOUR CODE HERE\n# 1. Create a comparison table with:\n#    - Model name\n#    - Overall F1 score\n#    - Training time (approximate)\n#    - Model size (parameters)\n#    - Best/worst entity types\n# 2. Discuss which model performed better\n# 3. Analyze whether BERT's larger size justified better performance"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "## Part 5 - Reflection (2 pts)\n\n1. What, if anything, did you find difficult to understand for the lesson? Why?\n\nüìù **YOUR ANSWER HERE:**\n\n2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n\nüìù **YOUR ANSWER HERE:**"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": "# Step 1: Prepare validation subset\nfrom Lesson_10_Helpers import llm_ner_extractor\n\n# YOUR CODE HERE\n# 1. Get first 100 examples from valid split\n# 2. Extract texts and true labels\n# 3. Start with 2-3 examples to test your prompt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Design prompt for LLM-based NER\n\n# YOUR CODE HERE\n# 1. Create system_prompt explaining the task\n# 2. Create prompt_template with:\n#    - Instructions to extract entities\n#    - Entity types to look for: Actor, Character, Director, Genre, Title, Year\n#    - Request structured output (JSON format recommended)\n#    - Include the {text} placeholder"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}