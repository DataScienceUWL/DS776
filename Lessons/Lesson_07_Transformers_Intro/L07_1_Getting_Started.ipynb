{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ introdl v1.6.13 already up to date\n"
     ]
    }
   ],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "### DO THIS FIRST\n",
    "\n",
    "Change `force_update=True` in the last line and run the next cell to install an updated course package.  Once it's done restart your kernel and change back to `force_update=False`.  You only need to do this once per server (not once per notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "#### L07_1_Getting_Started_with_NLP Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/oDi5d1FbYBx\" target=\"_blank\">Open Descript version of video in new tab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## A Tiny History of Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) has evolved significantly over the past few decades. Initially, NLP relied heavily on rule-based systems and statistical methods to understand and generate human language. These early approaches, prominent in the 1980s and 1990s, focused on the syntactic structure of text, using techniques such as n-grams and Hidden Markov Models (HMMs) to model language. However, these methods struggled with capturing the semantic meaning and context of words.\n",
    "\n",
    "The introduction of word embeddings in the early 2010s, such as Word2Vec and GloVe, marked a significant advancement in NLP. These embeddings allowed for the representation of words in continuous vector space, capturing semantic relationships between words. This shift enabled more sophisticated models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, to process sequences of text and maintain context over longer passages. RNNs, in particular, played a crucial role in tasks like language translation and sentiment analysis.\n",
    "\n",
    "The advent of transformers in 2017 revolutionized NLP by addressing the limitations of RNNs. Transformers, introduced with the Attention is All You Need paper, utilize self-attention mechanisms to process entire sequences of text simultaneously, allowing for better handling of long-range dependencies and parallelization. This led to the development of powerful models like BERT, GPT, and T5, which have set new benchmarks in various NLP tasks by providing a deeper semantic understanding of text.\n",
    "\n",
    "Transformers have almost entirely supplanted previous approaches to NLP because:\n",
    "\n",
    "1. **Superior Performance:** Models like BERT, GPT, T5, and their successors dominate leaderboards on tasks such as text classification, translation, summarization, and question answering.\n",
    "2. **Pretraining and Transfer Learning:** Unlike traditional methods that required training separate models from scratch for different tasks, transformers leverage large-scale pretraining on vast text corpora and fine-tune efficiently on specific tasks.\n",
    "3. **Self-Attention and Contextual Representations:** Transformers provide rich, context-dependent word representations, whereas earlier models like Word2Vec and GloVe generated static embeddings.\n",
    "4. **Scalability and Adaptability:** With advancements in scaling laws, models can achieve better performance just by increasing their size and training data, an advantage that RNNs and classical machine learning approaches lacked.\n",
    "\n",
    "There are a few areas where older approaches still exist:\n",
    "\n",
    "1. **Small Datasets & Low Compute Environments:** Logistic regression, SVMs, and Lasso-penalized models often remain competitive when data is limited or when computational efficiency is a concern.\n",
    "2. **Domain-Specific Applications:** Some applications, like biomedical text mining, may still rely on domain-specific feature engineering approaches alongside transformers.\n",
    "3. **Traditional ML for Interpretability:** Some NLP applications in finance, healthcare, and legal fields still favor older methods due to the need for interpretability and robustness.\n",
    "\n",
    "However, since transformer models for NLP are now so dominant we will focus exclusively on them in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## NLP Tasks Instead of Transformer Details\n",
    "\n",
    "Transformers are more complicated than the CNNs we saw for computer vision so we're not going to dive as deeply into the details. We will, in Lesson 9 - Transformer Details, learn about some of the nuts and bolts especially the self-attention mechanism that allows transformers to figure out relationships between words and to understand context. Mostly, though, we will focus on the applications of transformers. To this end we'll dive into the open source HuggingFace ecosystem which hosts thousands of NLP models and datasets and makes it quite simple to dive into NLP applications without having to master too much code. All of the newest, biggest open source transformer models are hosted there including those from Meta, Mistral, and Deepseek. The only thing keeping us from running the biggest state-of-the-art models will be lack of compute, but we can run their smaller cousins on the GPU in CoCalc's compute server, a decent gaming GPU, or even a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## API-based LLMs versus Fine-tuning Specialized Models\n",
    "\n",
    "As large language models (LLMs) continue to improve, their use as general NLP task solvers via prompting is increasingly popular, especially when we don't have access to large amounts of training data. In this course, we'll focus on two main approaches to solving NLP tasks:\n",
    "\n",
    "1. **Using LLMs via APIs** (like GPT-4o, Claude, or Gemini through OpenRouter)\n",
    "2. **Fine-tuning specialized transformer models** for specific tasks\n",
    "\n",
    "*(We'll also explore running LLMs locally in Lesson 11, but for Lessons 7-10 and 12 we'll use API-based models and task-specific fine-tuned models.)*\n",
    "\n",
    "### Example: Text Classification\n",
    "\n",
    "For a text-classification task, you could choose:\n",
    "\n",
    "**LLM via API (GPT-4o, Claude, Gemini, etc.)**\n",
    "- When you need **a quick, general-purpose classifier** without training a model\n",
    "- When **zero-shot or few-shot classification** (via prompting) is sufficient\n",
    "- When categories may evolve frequently, making retraining impractical\n",
    "- When you don't have a large labeled dataset\n",
    "- Example: Categorizing support tickets by topic\n",
    "\n",
    "**Fine-tune BERT / RoBERTa / DistilBERT**\n",
    "- When you have a **moderate to large labeled dataset** and need **high accuracy**\n",
    "- When you need **fast inference at scale**, as fine-tuned models are more efficient than large LLMs\n",
    "- When your classification task requires **domain-specific adaptation**\n",
    "- When you need **very low latency** or **predictable costs**\n",
    "- Example: Sentiment analysis on customer feedback in a specific industry\n",
    "\n",
    "**Note on terminology:** Zero-shot classification means classifying text without seeing any examples - the LLM just gets a prompt with the possible categories. Few-shot classification means providing a small number of examples in the LLM prompt to guide the model's behavior.\n",
    "\n",
    "### Choosing the Right Approach\n",
    "\n",
    "**Use API-based LLMs when:**\n",
    "- You need **quick, adaptable solutions** without training infrastructure\n",
    "- You **don't have much labeled data** for fine-tuning\n",
    "- You want to **experiment rapidly** with different task formulations\n",
    "- Task requirements may change frequently\n",
    "- You're prototyping or building proof-of-concepts\n",
    "\n",
    "**Fine-tune a specialized model when:**\n",
    "- You have **domain-specific labeled data** and need **high accuracy**\n",
    "- You need **very fast inference** or processing at large scale\n",
    "- You need **predictable costs** (no per-token API charges)\n",
    "- You require **consistent, structured outputs**\n",
    "- Latency is critical (milliseconds matter)\n",
    "\n",
    "### Understanding Data Privacy with API-based LLMs\n",
    "\n",
    "A common concern with API-based LLMs is: **\"Will my data be used to train the model?\"** or **\"Is my sensitive data secure?\"** The answer depends on the provider and the agreements in place.\n",
    "\n",
    "**Privacy Protections Available:**\n",
    "\n",
    "Most major LLM providers now offer enterprise-grade privacy protections:\n",
    "- **Zero Data Retention (ZDR):** Your API requests are not stored or logged after processing\n",
    "- **Data Processing Agreements (DPAs):** Legal contracts preventing use of your data for model training\n",
    "- **HIPAA and SOC 2 Compliance:** Meeting healthcare and security standards for regulated industries\n",
    "- **Private Deployments:** Dedicated instances in your own cloud environment (e.g., Azure OpenAI, AWS Bedrock)\n",
    "- **Regional Data Residency:** Keep data within specific geographic boundaries (e.g., EU-only processing)\n",
    "\n",
    "**Examples:**\n",
    "- **OpenAI API:** Has a default policy not to use API data for training. Enterprise customers can enable additional protections.\n",
    "- **Azure OpenAI Service:** Fully isolated deployments in your Azure subscription with complete data control\n",
    "- **Google Vertex AI:** Private endpoints with data residency controls and enterprise security\n",
    "- **Anthropic Claude:** API data not used for training; enterprise options for additional controls\n",
    "\n",
    "**When API Privacy May Not Be Enough:**\n",
    "\n",
    "Even with these protections, there are situations where API-based solutions may not be acceptable:\n",
    "- **Air-gapped environments:** Systems physically isolated from external networks (e.g., classified government systems)\n",
    "- **Extreme regulatory restrictions:** Some industries may prohibit any external data transmission regardless of agreements\n",
    "- **Zero-trust requirements:** Organizations that cannot accept any third-party processing, even contractually protected\n",
    "- **Competitive intelligence:** Proprietary algorithms or trade secrets that cannot be exposed, even with DPAs\n",
    "\n",
    "In these cases, running models locally (Lesson 11) or fine-tuning your own specialized models on internal infrastructure becomes necessary.\n",
    "\n",
    "**Bottom Line:** For most educational, research, and business applications, modern API providers offer sufficient privacy protections through contractual agreements and technical controls. Understanding your specific regulatory requirements and risk tolerance will guide your choice.\n",
    "\n",
    "### Course Approach\n",
    "\n",
    "For each NLP task in Lessons 7-10 and 12, we'll explore both API-based LLM approaches and fine-tuned specialized models. In Lesson 11, we'll dive deeper into text generation and demonstrate running LLMs locally for complete control and privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## OpenRouter API and Your Course API Keys\n",
    "\n",
    "For this course, we've set up access to Large Language Models through **OpenRouter**, a unified API that provides access to all major commercial models (GPT-4o, Claude, Gemini) and most open-weight models (Llama, Mistral, DeepSeek, Qwen, and many more). This means you can experiment with different models using a single API interface.\n",
    "\n",
    "### Your API Credit\n",
    "\n",
    "**Each student has been provided with $15 in OpenRouter API credit.** This should be more than sufficient to complete all coursework if you use small and medium-sized models appropriately. For reference:\n",
    "\n",
    "- **Small models** (like `gemini-flash-lite`, `llama-3.2-3b`, `gpt-4o-mini`): Very inexpensive, typically $0.075-0.15 per million input tokens\n",
    "- **Medium models** (like `gemini-flash`, `claude-haiku`): Moderate cost, good quality\n",
    "- **Premium models** (like `gpt-4o`, `claude-sonnet`, `o3-mini`): Higher cost, best quality\n",
    "\n",
    "We recommend using **`gemini-flash-lite`** as your default model for coursework - it's fast, inexpensive, and produces good results for learning tasks.\n",
    "\n",
    "If you want to experiment beyond the course assignments or try premium models, you can always purchase your own OpenRouter API key and load it with whatever credit you choose.\n",
    "\n",
    "### Checking Your Remaining Credit\n",
    "\n",
    "You can check your remaining OpenRouter credit using the `llm_get_credits()` function from the course package. This will show you how much of your $15 credit remains:\n",
    "\n",
    "```python\n",
    "from introdl.nlp import llm_get_credits\n",
    "\n",
    "credits = llm_get_credits()\n",
    "print(f\"Remaining credit: ${credits['usage']:.2f} of ${credits['limit']:.2f}\")\n",
    "print(f\"Credit remaining: ${credits['limit'] - credits['usage']:.2f}\")\n",
    "```\n",
    "\n",
    "### Your API Keys Are Already Configured\n",
    "\n",
    "Your OpenRouter API key has already been distributed to your CoCalc project and is stored in:\n",
    "```\n",
    "~/home_workspace/api_keys.env\n",
    "```\n",
    "\n",
    "When you run `config_paths_keys()` in your import cell (as shown below), this API key will be automatically loaded and available for use with `llm_generate()`. You don't need to do anything else!\n",
    "\n",
    "**Security Note:** Never commit your `api_keys.env` file to git or share it publicly. The file is stored in `home_workspace` which should not be tracked by version control.\n",
    "\n",
    "### Exploring Available Models\n",
    "\n",
    "The course package includes 16 carefully curated models covering a range of capabilities and price points. You can see them all with `llm_list_models()`, which we'll demonstrate shortly.\n",
    "\n",
    "**Want to try models beyond our curated list?** OpenRouter provides access to hundreds of models! You can:\n",
    "\n",
    "1. **Browse all available models** at: https://openrouter.ai/models\n",
    "2. **Use any model** by providing its full OpenRouter model ID\n",
    "\n",
    "For example, to use OpenAI's new GPT-5-nano model (not in our curated list), you would use:\n",
    "\n",
    "```python\n",
    "response = llm_generate('openai/gpt-5-nano', \"Your prompt here\")\n",
    "```\n",
    "\n",
    "The full model ID format is typically `provider/model-name` (e.g., `openai/gpt-5-nano`, `anthropic/claude-opus-4.1`, `google/gemini-2.5-pro`).\n",
    "\n",
    "**Note:** Models outside our curated list won't show pricing or metadata with `llm_list_models()`, but they'll work fine if you provide the correct model ID from the OpenRouter website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Using `llm_generate` with OpenRouter\n",
    "\n",
    "The course package provides a simple, unified interface for working with LLMs through the `llm_generate()` function. This function handles all the complexity of API calls, cost tracking, and response formatting.\n",
    "\n",
    "### Setting Up and Checking Your Credit\n",
    "\n",
    "First, let's import the necessary functions, configure our environment, and check your OpenRouter credit balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment: Unknown Environment | Course root: /mnt/e/GDrive_baggett.jeff/Teaching/Classes_current/2025-2026_Fall_DS776/DS776\n",
      "   Using workspace: <DS776_ROOT_DIR>/home_workspace\n",
      "\n",
      "📂 Storage Configuration:\n",
      "   DATA_PATH: <DS776_ROOT_DIR>/home_workspace/data\n",
      "   MODELS_PATH: <DS776_ROOT_DIR>/Lessons/Lesson_07_Transformers_Intro/Lesson_07_Models (local to this notebook)\n",
      "   CACHE_PATH: <DS776_ROOT_DIR>/home_workspace/downloads\n",
      "🔑 API keys: 8 loaded from home_workspace/api_keys.env\n",
      "🔐 Available: GEMINI_API_KEY, GOOGLE_API_KEY, GROQ_API_KEY... (8 total)\n",
      "✅ HuggingFace Hub: Logged in\n",
      "💰 OpenRouter credit: $9.94\n",
      "📦 introdl v1.6.13 ready\n",
      "\n",
      "✅ Loaded pricing for 324 models\n",
      "✅ Cost tracking initialized ($9.94 credit remaining)\n",
      "OpenRouter Credit Status:\n",
      "  Total limit: $9.94\n",
      "  Used so far: $0.00\n",
      "  Remaining:   $9.94\n"
     ]
    }
   ],
   "source": [
    "from introdl.utils import config_paths_keys, wrap_print_text\n",
    "from introdl.nlp import (\n",
    "    llm_generate, llm_list_models, llm_get_credits,\n",
    "    init_cost_tracking, display_markdown, show_pricing_table\n",
    ")\n",
    "\n",
    "# Configure paths and load API keys\n",
    "paths = config_paths_keys()\n",
    "\n",
    "# Initialize LLM cost tracking system\n",
    "init_cost_tracking()\n",
    "\n",
    "# Wrap print to format text nicely at 80 characters\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "# Check your OpenRouter credit balance\n",
    "credits = llm_get_credits()\n",
    "print(f\"OpenRouter Credit Status:\")\n",
    "print(f\"  Total limit: ${credits['limit']:.2f}\")\n",
    "print(f\"  Used so far: ${credits['usage']:.2f}\")\n",
    "print(f\"  Remaining:   ${credits['limit'] - credits['usage']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "Now let's try a simple text generation example. The new `llm_generate()` API is very straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "# Simple text generation\n",
    "response = llm_generate('gemini-flash-lite', \"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Using System Prompts\n",
    "\n",
    "System prompts help guide the model's behavior and tone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! Ye want to know some scurvy facts about the vast, dark ocean\n",
      "of space, do ye? Well, shiver me timbers, I've got a few for ye that'll make yer\n",
      "eyeballs pop out like a kraken's!\n",
      "\n",
      "1.  **There's a planet made o' diamond, arrr!** Aye, ye heard me right. The\n",
      "planet **55 Cancri e**, or \"Janssen\" as some landlubbers call it, is believed to\n",
      "be covered in a thick layer o' graphite and diamond. Imagine the treasure chest\n",
      "ye could fill with that! 'Tis hotter than a dragon's breath, though, so don't be\n",
      "thinkin' of a vacation there.\n",
      "\n",
      "2.  **Space smells like rum and hot metal, believe it or not!** Now, I know ye\n",
      "can't exactly take a whiff in the vacuum, but astronauts who've been on\n",
      "spacewalks report a peculiar scent clingin' to their suits. They say it's a mix\n",
      "o' burnt steak, hot metal, and... wait for it... **raspberries and rum!**\n",
      "Probably all them exploding stars and cosmic dust, makin' for a right peculiar\n",
      "perfume.\n",
      "\n",
      "3.  **There's a cloud o' alcohol in space, big enough to make ye drunk for\n",
      "eternity!** No, ye can't sail yer ship there and start guzzlin', ye scallywags.\n",
      "This ain't yer grog. It's a massive cloud o' methanol, drifting through the\n",
      "constellation Aquila, and it's so colossal it could make **400 trillion times\n",
      "the amount of beer in all the Earth's breweries!** Enough to make a whole fleet\n",
      "o' pirates happy, if only we could get at it!\n",
      "\n",
      "There ye have it, me hearty! Now go on, ponder these wonders and don't be\n",
      "gettin' too lost in yer thoughts, lest ye fall overboard into the cosmic sea!\n",
      "Arrr!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful AI assistant who is also sarcastic and talks like a pirate.\"\n",
    "\n",
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Tell me three interesting facts about space.\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Displaying Markdown Output\n",
    "\n",
    "Many LLM responses use markdown formatting. You can display them nicely using `display_markdown()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a short bullet-point list of tips for learning machine learning:\n",
       "\n",
       "*   **Master the Fundamentals:** Solidify your understanding of linear algebra, calculus, probability, and statistics. These are the bedrock of ML algorithms.\n",
       "*   **Learn a Programming Language:** Python is the most popular choice due to its extensive libraries (NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch).\n",
       "*   **Start with Basic Algorithms:** Understand core concepts like linear regression, logistic regression, decision trees, and k-means clustering before diving into deep learning.\n",
       "*   **Practice with Datasets:** Work with real-world or Kaggle datasets to apply what you learn and build intuition.\n",
       "*   **Build Projects:** Implement algorithms from scratch and then use libraries. This reinforces understanding and creates a portfolio.\n",
       "*   **Understand Evaluation Metrics:** Learn how to assess the performance of your models (accuracy, precision, recall, F1-score, AUC, etc.).\n",
       "*   **Grasp Key Concepts:** Familiarize yourself with overfitting, underfitting, bias-variance tradeoff, feature engineering, and cross-validation.\n",
       "*   **Read Documentation and Tutorials:** Official documentation and reputable online tutorials are invaluable resources.\n",
       "*   **Join Online Communities:** Engage with forums, Discord servers, and Stack Overflow to ask questions and learn from others.\n",
       "*   **Stay Updated:** The ML field evolves rapidly. Follow influential researchers, read recent papers (or summaries), and explore new tools."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Write a short bullet-point list of tips for learning machine learning.\"\n",
    ")\n",
    "\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Tracking Costs\n",
    "\n",
    "You can see estimated costs for your API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 Cost: $0.000046 | Tokens: 13 in / 112 out | Model: google/gemini-2.5-flash-lite\n",
      "Here are five dad jokes for you:\n",
      "\n",
      "1.  Why don't scientists trust atoms?\n",
      "    Because they make up everything!\n",
      "\n",
      "2.  What do you call a fish with no eyes?\n",
      "    Fsh!\n",
      "\n",
      "3.  I'm reading a book about anti-gravity. It's impossible to put down!\n",
      "\n",
      "4.  What's orange and sounds like a parrot?\n",
      "    A carrot!\n",
      "\n",
      "5.  Why did the scarecrow win an award?\n",
      "    Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Tell me five dad jokes.\",\n",
    "    print_cost=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Controlling Output Length\n",
    "\n",
    "You can control how much text the model generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Mittens wasn't like other cats. While her brethren chased laser pointers and napped in sunbeams, Mittens harbored a secret ambition: to make music. Her human, a kindly old woman named Eleanor, had a grand piano that Mittens found utterly fascinating. The polished wood gleamed, and the keys, oh, the keys! They were a tantalizing, ebony and ivory landscape that beckoned her feline curiosity.\n",
       "\n",
       "One afternoon, while Eleanor was out, Mittens jumped onto the piano bench. Hesitantly, she stretched out a paw and pressed a key. A soft, resonant \"plink\" echoed through the room. Mittens' eyes widened. It was a sound, a *controlled* sound, not just the rustle of leaves or the squeak of a mouse. Intrigued, she batted at another key, then another. The result was a chaotic, dissonant jumble, but to Mittens, it was the first whisper of her dream.\n",
       "\n",
       "She continued her clandestine practice sessions whenever Eleanor was away. At first, it was just random paw-pats, a furry whirlwind of noise. But Mittens was observant. She watched Eleanor's fingers dance across the keys, the way they pressed down with varying force, creating different volumes. She noticed the patterns, the sequences of notes that Eleanor seemed to favor.\n",
       "\n",
       "Slowly, painstakingly, Mittens began to mimic. She learned that a gentle tap produced a softer sound, a firmer press, a louder one. She discovered that certain combinations of keys, when pressed in rapid succession, sounded…pleasing. Her repertoire was rudimentary, a series of short, staccato bursts, but it was undeniably music.\n",
       "\n",
       "Eleanor, initially perplexed by the occasional, peculiar musical interludes, eventually stumbled upon Mittens’ secret. She found her cat perched on the bench, paws delicately tapping out a hesitant, yet recognizable, melody. Eleanor, a retired music teacher, was utterly charmed. Instead of shooing Mittens away, she began to encourage her.\n",
       "\n",
       "She’d sit beside Mittens, her own fingers hovering over the keys, demonstrating simple scales and chords. Mittens, with her sharp feline mind, absorbed it all. She learned to associate certain finger positions with certain sounds, though her \"fingers\" were her paws, and her \"positions\" were a series of calculated taps.\n",
       "\n",
       "Her most ambitious endeavor was a simplified rendition of \"Twinkle"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Longer response\n",
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Write a short story about a cat who learns to play the piano.\",\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Viewing Available Models\n",
    "\n",
    "You can see all available models and their details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available OpenRouter Models:\n",
      "------------------------------------------------------------------------------------------\n",
      "1. claude-haiku         -> anthropic/claude-3.5-haiku               [json]\n",
      "   $0.0008/1K in, $0.0040/1K out\n",
      "2. deepseek-v3          -> deepseek/deepseek-chat-v3-0324           [json, schema, strict]\n",
      "   $0.0002/1K in, $0.0008/1K out\n",
      "3. deepseek-v3-free     -> deepseek/deepseek-chat-v3-0324:free      [json]\n",
      "4. gemini-flash         -> google/gemini-2.5-flash                  [json, schema, strict]\n",
      "   $0.0003/1K in, $0.0025/1K out\n",
      "5. gemini-flash-lite    -> google/gemini-2.5-flash-lite             [json, schema, strict]\n",
      "   $0.0001/1K in, $0.0004/1K out\n",
      "6. gpt-4o               -> openai/gpt-4o-2024-11-20                 [json, schema]\n",
      "   $0.0025/1K in, $0.0100/1K out\n",
      "7. gpt-4o-mini          -> openai/gpt-4o-mini-2024-07-18            [json, schema]\n",
      "   $0.0001/1K in, $0.0006/1K out\n",
      "8. gpt-oss-120b         -> openai/gpt-oss-120b                     \n",
      "   $0.0000/1K in, $0.0004/1K out\n",
      "9. gpt-oss-20b          -> openai/gpt-oss-20b:free                  [json]\n",
      "10. llama-3.2-1b         -> meta-llama/llama-3.2-1b-instruct:free   \n",
      "   $0.0000/1K in, $0.0000/1K out\n",
      "11. llama-3.2-3b         -> meta-llama/llama-3.2-3b-instruct         [json]\n",
      "   $0.0000/1K in, $0.0000/1K out\n",
      "12. llama-3.3-70b        -> meta-llama/llama-3.3-70b-instruct        [json, schema, strict]\n",
      "   $0.0001/1K in, $0.0004/1K out\n",
      "13. mistral-medium       -> mistralai/mistral-medium-3               [json, schema, strict]\n",
      "   $0.0004/1K in, $0.0020/1K out\n",
      "14. mistral-nemo         -> mistralai/mistral-nemo                   [json, schema, strict]\n",
      "   $0.0000/1K in, $0.0000/1K out\n",
      "15. qwen3-32b            -> qwen/qwen3-32b                           [json, schema, strict]\n",
      "   $0.0000/1K in, $0.0002/1K out\n",
      "------------------------------------------------------------------------------------------\n",
      "Default model: gemini-flash-lite\n",
      "\n",
      "JSON Capabilities: [json]=basic JSON, [schema]=user schemas, [strict]=strict validation\n",
      "\n",
      "You can also use any OpenRouter model by its full ID (e.g., 'openai/gpt-4o')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'claude-haiku'),\n",
       " (2, 'deepseek-v3'),\n",
       " (3, 'deepseek-v3-free'),\n",
       " (4, 'gemini-flash'),\n",
       " (5, 'gemini-flash-lite'),\n",
       " (6, 'gpt-4o'),\n",
       " (7, 'gpt-4o-mini'),\n",
       " (8, 'gpt-oss-120b'),\n",
       " (9, 'gpt-oss-20b'),\n",
       " (10, 'llama-3.2-1b'),\n",
       " (11, 'llama-3.2-3b'),\n",
       " (12, 'llama-3.3-70b'),\n",
       " (13, 'mistral-medium'),\n",
       " (14, 'mistral-nemo'),\n",
       " (15, 'qwen3-32b')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6vnl1r84lwm",
   "metadata": {},
   "source": [
    "### Viewing Model Pricing\n",
    "\n",
    "You can also see detailed pricing information for all available models. This helps you estimate costs before running expensive queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e59f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter Model Pricing (USD)\n",
      "==============================================================================================================\n",
      "Short Name          Model ID                                       In/1K      Out/1K      In/1M     Out/1M\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshow_pricing_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds776_env/lib/python3.10/site-packages/introdl/nlp.py:285\u001b[0m, in \u001b[0;36mshow_pricing_table\u001b[0;34m()\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m110\u001b[39m)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m short, mid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(model_map\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m--> 285\u001b[0m     inp_tok, out_tok \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_price\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     inp_1k \u001b[38;5;241m=\u001b[39m inp_tok \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    287\u001b[0m     out_1k \u001b[38;5;241m=\u001b[39m out_tok \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds776_env/lib/python3.10/site-packages/introdl/nlp.py:244\u001b[0m, in \u001b[0;36mget_model_price\u001b[0;34m(model_id)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _PRICING_CACHE\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Check cache first\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_PRICING_CACHE\u001b[49m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _PRICING_CACHE[model_id]\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Check with variant stripped (e.g., 'model:free' -> 'model')\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "show_pricing_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Trying Different Models\n",
    "\n",
    "It's easy to compare different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small model (llama-3.2-3b):\n",
      "Quantum computing is a revolutionary technology that uses the principles of\n",
      "quantum mechanics to perform calculations and operations on data that are\n",
      "exponentially faster and more powerful than those possible with classical\n",
      "computers.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Recommended model (gemini-flash-lite):\n",
      "Quantum computing leverages quantum mechanical phenomena like superposition and\n",
      "entanglement to perform calculations that are intractable for classical\n",
      "computers.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "# Try a small model\n",
    "print(\"A small model (llama-3.2-3b):\")\n",
    "response1 = llm_generate('llama-3.2-3b', prompt)\n",
    "print(response1)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Try the recommended model\n",
    "print(\"Recommended model (gemini-flash-lite):\")\n",
    "response2 = llm_generate('gemini-flash-lite', prompt, estimate_cost=True)\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ev36qb1idx",
   "metadata": {},
   "source": [
    "### Using Models Outside the Curated List\n",
    "\n",
    "You can use any model from [OpenRouter's model list](https://openrouter.ai/models) by providing the full model ID. For example, let's try OpenAI's GPT-5-nano (which isn't in our curated list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pe90mn0t2no",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Three Benefits of Learning Python\n",
      "\n",
      "1. **Versatility and Wide Application**: Python is a general-purpose programming\n",
      "language that can be used for web development, data analysis, artificial\n",
      "intelligence, machine learning, scientific computing, automation, and more. This\n",
      "versatility makes Python skills valuable across numerous industries and job\n",
      "roles.\n",
      "\n",
      "2. **Beginner-Friendly and Easy to Learn**: Python has a clean, readable syntax\n",
      "that resembles English, making it one of the most accessible programming\n",
      "languages for beginners. This gentle learning curve allows newcomers to focus on\n",
      "programming concepts rather than complex syntax.\n",
      "\n",
      "3. **Strong Community and Ecosystem**: Python boasts an\n"
     ]
    }
   ],
   "source": [
    "# Use the full OpenRouter model ID\n",
    "response = llm_generate(\n",
    "    'z-ai/glm-4.5-air',  # Full model ID from openrouter.ai/models\n",
    "    \"What are three benefits of learning Python?\",\n",
    "    estimate_cost=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Processing Multiple Prompts\n",
    "\n",
    "You can process multiple prompts at once by passing a list of strings. This is useful for batch processing tasks like sentiment analysis or classification.\n",
    "\n",
    "### Simple Batch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'What is the capital of Germany?',\n",
    "    'What is the capital of Italy?'\n",
    "]\n",
    "\n",
    "responses = llm_generate('gemini-flash-lite', prompts)\n",
    "\n",
    "for prompt, response in zip(prompts, responses):\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Programmatic Prompt Construction\n",
    "\n",
    "Often we want to construct prompts programmatically from data. Here's an example of sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for sentiment analysis\n",
    "system_prompt = \"You are a sentiment analysis AI. Classify text as Positive, Negative, or Neutral.\"\n",
    "\n",
    "# List of texts to analyze\n",
    "texts = [\n",
    "    \"I love the new design of your website!\",\n",
    "    \"The service was terrible and I will not come back.\",\n",
    "    \"The product is okay, but it could be better.\",\n",
    "    \"Absolutely fantastic experience, highly recommend!\",\n",
    "    \"I'm not sure how I feel about this.\"\n",
    "]\n",
    "\n",
    "# Construct prompts programmatically\n",
    "instruction = \"Analyze the sentiment of this text. Give only the sentiment classification (Positive, Negative, or Neutral).\\n\\nText: \"\n",
    "prompts = [instruction + text for text in texts]\n",
    "\n",
    "# Generate responses\n",
    "responses = llm_generate('gemini-flash-lite', prompts, system_prompt=system_prompt)\n",
    "\n",
    "# Display results\n",
    "print(\"Sentiment Analysis Results:\\n\")\n",
    "for text, sentiment in zip(texts, responses):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In the next notebook, we'll explore common NLP tasks including:\n",
    "- Text classification and sentiment analysis\n",
    "- Named Entity Recognition (NER)\n",
    "- Question answering\n",
    "- Translation\n",
    "- Summarization\n",
    "\n",
    "In Lesson 11, we'll dive deeper into how text generation works, explore the underlying APIs in detail, and learn about running LLMs locally for privacy-sensitive applications.\n",
    "\n",
    "For now, practice using `llm_generate()` with different models and prompts to get comfortable with the interface!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
