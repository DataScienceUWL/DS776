{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Homework 06 Assignment\n",
    "**Name:** [Student Name Here]  \n",
    "**Total Points:** 50\n",
    "\n",
    "## Submission Checklist\n",
    "- [ ] All code cells executed with output saved\n",
    "- [ ] All questions answered\n",
    "- [ ] Notebook converted to HTML (use the Homework_06_Utilities notebook)\n",
    "- [ ] Canvas notebook filename includes `_GRADE_THIS_ONE`\n",
    "- [ ] Files uploaded to Canvas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "# Computer Vision: Segmentation and Object Detection\n",
    "\n",
    "For this assignment there are two primary tasks exploring advanced computer vision applications:\n",
    "\n",
    "1. **UNet and UNet++ for Nuclei Segmentation**: Explore semantic segmentation using UNet architectures on a biomedical imaging task described in the textbook.\n",
    "2. **YOLO v11 for Pedestrian Detection**: Fine-tune a state-of-the-art YOLO model for object detection and compare results to the Faster R-CNN model from the lesson.\n",
    "\n",
    "Both tasks will help you understand the differences between segmentation (pixel-level classification) and detection (bounding box prediction) approaches in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR IMPORTS HERE ===\n",
    "# Add any additional imports you need below this line\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision import tv_tensors\n",
    "from pathlib import Path\n",
    "\n",
    "# Import local modules\n",
    "from Lesson_06_Helpers import display_yolo_predictions, prepare_penn_fudan_yolo\n",
    "\n",
    "from introdl.utils import config_paths_keys\n",
    "\n",
    "# Configure paths\n",
    "paths = config_paths_keys()\n",
    "DATA_PATH = paths['DATA_PATH']\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "# === END YOUR IMPORTS ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## [20 pts] UNet and UNet++ Segmentation\n",
    "\n",
    "You're going to use the segmentation models pytorch package as we did in the lesson to fine-tune and evaluate UNet and UNet++ models on the nuclei segmentation task shown in the textbook.\n",
    "\n",
    "We've already prepared the data downloading process for you. The following cells contain most of a custom dataset class and transforms to get you started. You'll need to complete the code sections marked with `# === YOUR CODE HERE ===` to read images and masks, add appropriate augmentation transforms, and implement the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once to download the Nuclei Segmentation dataset\n",
    "\n",
    "from Lesson_06_Helpers import download_and_extract_nuclei_data\n",
    "\n",
    "# Call the function\n",
    "download_and_extract_nuclei_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Complete the NucleiDataset class and data loading setup\n",
    "# - Complete the dataset class by filling in the marked sections:\n",
    "#   * Read image from image_path, convert to float and scale to [0,1]\n",
    "#   * Read mask from mask_path, map values >0 to 1, rest to 0, convert to float\n",
    "# - Add appropriate augmentation transforms for training:\n",
    "#   * Consider: RandomHorizontalFlip, RandomVerticalFlip, RandomRotation\n",
    "#   * Use transforms that work with both images and masks simultaneously\n",
    "# - Create training and validation datasets\n",
    "# - Create DataLoaders with batch_size=8\n",
    "\n",
    "class NucleiDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str or Path): Path to the dataset (train or val folder).\n",
    "            transform (callable, optional): Optional transforms to apply to both image and mask.\n",
    "        \"\"\"\n",
    "        self.root = Path(root)  # Convert to pathlib Path object\n",
    "        self.transform = transform\n",
    "        self.data = []  # List to store (image_tensor, mask_tensor) tuples\n",
    "\n",
    "        # Load all image and mask files\n",
    "        all_imgs = sorted((self.root / \"images\").iterdir())\n",
    "        all_masks = sorted((self.root / \"masks\").iterdir())\n",
    "\n",
    "        # Ensure that the number of images and masks are the same\n",
    "        assert len(all_imgs) == len(all_masks), \"The number of images and masks must be the same\"        \n",
    "\n",
    "        # Read and store images and masks as tensors in memory\n",
    "        for img_path, mask_path in zip(all_imgs, all_masks):\n",
    "            # === YOUR CODE: Read images and masks as tensors ===\n",
    "            # Read image from image_path, convert to float and scale to [0,1]\n",
    "            image = # TODO: Complete this line\n",
    "            \n",
    "            # Read mask from mask_path, any entries bigger than 0 map to 1, rest to 0, convert to float\n",
    "            mask = # TODO: Complete this line\n",
    "\n",
    "            # Store as tuple\n",
    "            self.data.append((tv_tensors.Image(image), tv_tensors.Mask(mask)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.data[idx]\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# === YOUR CODE: Define transforms and create datasets ===\n",
    "# Add your augmentation transforms here for training\n",
    "train_transforms = transforms.Compose([\n",
    "    # TODO: Add appropriate augmentation transforms\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define transforms for validation (without augmentation)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# TODO: Load datasets and create dataloaders\n",
    "# train_dataset = ...\n",
    "# val_dataset = ...\n",
    "# train_loader = ...\n",
    "# val_loader = ...\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "Now setup and train UNet and UNet++ models with a pretrained ResNet50 backbone as we did in the lesson. Model your code on the code in the \"Better Training\" part of the notebook. You should set different learning rates for the encoder and decoder and use OneCycleLR as we did. We found that 12 epochs of fine-tuning worked reasonably well.\n",
    "\n",
    "For each model display convergence graphs of the loss and IoU and sample images along with the ground truth and predicted masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Setup and train UNet and UNet++ models\n",
    "# - Install required packages: !pip install segmentation-models-pytorch\n",
    "# - Import segmentation_models_pytorch as smp\n",
    "# - Create UNet model with ResNet50 encoder, pretrained weights\n",
    "# - Create UNet++ model with ResNet50 encoder, pretrained weights\n",
    "# - Set different learning rates for encoder (lower) and decoder (higher)\n",
    "# - Use OneCycleLR scheduler\n",
    "# - Train each model for 12 epochs\n",
    "# - Track loss and IoU metrics\n",
    "# - Save model checkpoints\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "üìù **Answer the following followup questions:**\n",
    "\n",
    "1. Which model performs better? Support your answer with specific metrics.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. Use AI to write a short summary of the difference between UNet and UNet++.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "3. Report the highest value of the IoU metric on the validation set. Interpret that value in the context of this problem. What is it telling you about the predicted masks for the cell nuclei?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## [20 pts] YOLO v11 Pedestrian Detection\n",
    "\n",
    "YOLO (You Only Look Once) models are a family of object detection models known for their speed and accuracy. Unlike traditional object detection methods that use a sliding window approach, YOLO models frame object detection as a single regression problem, directly predicting bounding boxes and class probabilities from full images in one evaluation. This makes YOLO models extremely fast, making them suitable for real-time applications.\n",
    "\n",
    "YOLO models consist of a single convolutional network that simultaneously predicts multiple bounding boxes and class probabilities for those boxes. The architecture is divided into several key components:\n",
    "\n",
    "1. **Backbone**: This is typically a convolutional neural network (CNN) that extracts essential features from the input image.\n",
    "2. **Neck**: This part of the network aggregates and combines features from different stages of the backbone. It often includes components like Feature Pyramid Networks (FPN) or Path Aggregation Networks (PAN).\n",
    "3. **Head**: The final part of the network, which predicts the bounding boxes, objectness scores, and class probabilities. It usually consists of convolutional layers that output the final detection results.\n",
    "\n",
    "YOLO models are quite easy to load and train because they provide pre-trained weights and a straightforward API for customization and fine-tuning. The hardest part may be preparing the data in the format that the API expects, but we've done that for you.\n",
    "\n",
    "**Installation Note:** You'll need to install required packages. Add this to a code cell and run it once on each server you use:\n",
    "```python\n",
    "!pip install ultralytics torchmetrics\n",
    "```\n",
    "\n",
    "Run the cell below once to prepare the Penn Fudan Pedestrian dataset in YOLO format. This dataset uses the same splits we used in the lesson to allow you to compare the results to the Faster R-CNN model we trained there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run this once per platform, but it's safe to run multiple times\n",
    "prepare_penn_fudan_yolo(DATA_PATH)\n",
    "\n",
    "# the dataset will be here:\n",
    "dataset_path = DATA_PATH / \"PennFudanPedYOLO\"\n",
    "\n",
    "# you may wish to set an output path for the model\n",
    "output_path = MODELS_PATH / \"PennFudanPedYOLO\"\n",
    "\n",
    "# the YAML file for the dataset is here:\n",
    "yaml_path = dataset_path / \"dataset.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "Visit the ultralytics website to learn about YOLO11. You can watch a short video to learn more about it. Below, implement code to load and train a YOLO11 model using the 'yolo11s.pt' pretrained weights. Pass `project=output_path` to `model.train()` to store the output in your models directory. After training you might want to look at some of the images created in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Install required packages and train YOLO11 model\n",
    "# - Install ultralytics and torchmetrics: !pip install ultralytics torchmetrics\n",
    "# - Import YOLO from ultralytics\n",
    "# - Load YOLO11 model with 'yolo11s.pt' pretrained weights\n",
    "# - Train the model on the Penn Fudan dataset\n",
    "# - Use project=output_path to save results in your models directory\n",
    "# - Train for appropriate number of epochs (experiment with 10-20)\n",
    "# - Monitor training progress and metrics\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "You can run the following cell to show selected images and boxes from the validation set. You can replace `indices=selected_indices` with `num_samples=3` to display 3 randomly selected images. The selected images we chose should align with the images we showed in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = [28,29,33]\n",
    "display_yolo_predictions(yaml_path, model, indices=selected_indices, show_confidence=True, conf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "üìù **Answer the following followup questions:**\n",
    "\n",
    "1. Find and plot an image with a false positive box in the validation data.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. How is the process of fine-tuning the YOLO model different than for the Faster R-CNN model in the lesson? Is it easier or harder? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "3. What did you get for mAP50 and mAP50:95 on the validation data with your YOLO model?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "4. How do those values compare to values in the lesson?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "5. How do the predicted boxes compare qualitatively to the boxes predicted by Faster R-CNN in the lesson? Do they align better or worse with the ground truth boxes?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "6. Thoroughly explain what your mAP50 value tells you about the performance of your YOLO model at detecting pedestrians in the validation data.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## [8 pts] Questions from Chapter 13 and Computer Vision Concepts\n",
    "\n",
    "**Question 1 (3 pts):** Based on your experience with both segmentation (UNet/UNet++) and object detection (YOLO) in this homework:\n",
    "- Explain the fundamental difference between semantic segmentation and object detection in terms of their outputs and applications.\n",
    "- Which approach would be more suitable for autonomous driving applications, and why?\n",
    "- How do the computational requirements typically compare between these two approaches?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "**Question 2 (3 pts):** Transfer learning concepts from Chapter 13 apply to both tasks in this homework:\n",
    "- How did transfer learning help in both the segmentation and detection tasks? What pretrained weights did you use?\n",
    "- Why is transfer learning particularly effective for computer vision tasks compared to training from scratch?\n",
    "- Based on Figure 13.1 concepts, explain why using different learning rates for encoder vs decoder (in segmentation models) is a good transfer learning strategy.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "**Question 3 (2 pts):** Model evaluation metrics:\n",
    "- Explain what IoU (Intersection over Union) measures in segmentation tasks and why it's more informative than simple pixel accuracy.\n",
    "- What does mAP50 measure in object detection, and why is it considered a comprehensive evaluation metric?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## [2 pts] Reflection\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for this lesson? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Cleanup Note\n",
    "\n",
    "**Note:** YOLO downloads pretrained model files (like `yolo11s.pt` or `yolo11n.pt`) to your current directory on first use. These files are safe to delete after training is complete if you want to save space - they will be re-downloaded if needed in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}