{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9: Transformer Internals - Self-Attention and Positional Encoding\n",
    "\n",
    "### Topics\n",
    "* In-depth self-attention mechanics\n",
    "* Multi-headed attention and context capture\n",
    "* Positional encoding and sequence structure\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand Self-Attention Calculations**: Demonstrate how self-attention works by calculating attention scores for a simple sequence, showing the process of computing queries, keys, and values.\n",
    "   \n",
    "2. **Explain Multi-Headed Attention**: Describe the purpose of multi-headed attention in transformers and explain how multiple attention heads provide richer contextual understanding.\n",
    "\n",
    "3. **Discuss Positional Encoding**: Explain the role of positional encoding in maintaining sequence order within transformers and understand its mathematical formulation.\n",
    "\n",
    "4. **Experiment with Attention Mechanisms**: Use Hugging Face models to observe how multi-headed attention and positional encoding affect outputs.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 3: Transformer Anatomy* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each of the notebooks in the Lesson_08 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "### Homework Ideas\n",
    "\n",
    "1. **Manual Self-Attention Calculation**: Create a notebook where students manually compute self-attention scores for a small example sequence. This will involve calculating dot products between queries, keys, and values and applying softmax to derive attention weights, reinforcing their understanding of the self-attention process.\n",
    "\n",
    "2. **Visualize Multi-Headed Attention**: Guide students to visualize the attention weights from a transformer model using Hugging Faceâ€™s `transformers` library. They can experiment with different attention heads to observe how each head focuses on different parts of the input, documenting the variations and their interpretations.\n",
    "\n",
    "3. **Experiment with Positional Encoding**: Have students modify positional encodings in a simple sequence (e.g., reordering words or changing positions) and observe how transformers handle these variations. They can analyze attention weights and model predictions to understand how positional encoding affects sequence interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
