{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to install the course package and its dependencies, add --user to install in CoCalc base environment\n",
    "# you don't need run this cell every time unless you want to upgrade the package to the latest version\n",
    "\n",
    "# from github\n",
    "# !pip install -q git+https://github.com/DataScienceUWL/DS776.git#subdirectory=introdl\n",
    "\n",
    "# from local file\n",
    "! pip install -q ../introdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "\n",
    "The code in this notebook is largely from the class textbook: “Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf (O’Reilly). Copyright 2022 Lewis Tunstall, Leandro von Werra, and Thomas Wolf, 978-1-098-13679-6.\"\n",
    "\n",
    "It is reproduced here to allow you to easily explore the code.  You can download the original notebook from their Github Repo: https://github.com/nlp-with-transformers/notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add path settings here including HF_HOME\n",
    "\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Disable all info / warning messages\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "from introdl.utils import wrap_print_text\n",
    "\n",
    "# overload the print method to print with wrapping text\n",
    "print = wrap_print_text(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tour of Transformer Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8faf5797561412786abfd14ea9daf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3da7733c1b4581b6bf4fd076ce7b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94245a88278f4494afcf326e2d65c7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cec271beb374bd9b4bce6e69bcf9880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert/distilbert-base-uncased-finetuned-sst-2-english'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6497d0d4f7184def9607a95bc383463c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a44a230811466ba041841abb4409fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa204ec61c9a4c7f917688c020e5a52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd85a391d2dd4c409b2334ba202cdbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879011</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556570</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590255</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879011         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556570           Mega    208  212\n",
       "4          PER  0.590255         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498350        ##icons    259  264\n",
       "7         MISC  0.775362       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dbmdz/bert-large-cased-finetuned-conll03-english'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09040182fe4041cdb0951934ac2b42fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e141f62a0a245ab8fe317f46784f0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58988a8fa534a5d8f1350f32b693603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a1d0b4c4be4f80a12410ab92a912fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569600625bf044ef9082809e1c931f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert/distilbert-base-cased-distilled-squad'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbc5aa46a0a4dd1a3437ebaf1aee0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4479fff64ff5441983d6ceacb47c29dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a484be188d54980b098d6657b399dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2befc96582f439dafa80e8e16604cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923a6fc19fd04fb69d464ca3b86f7dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\transformers\\generation\\utils.py:1244: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in\n",
      "Germany. Unfortunately, when I opened the package, I discovered to my horror\n",
      "that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sshleifer/distilbart-cnn-12-6'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querida Amazona, la semana pasada ordené una figura de acción Optimus Prime de\n",
      "su tienda en línea en Alemania. Desafortunadamente, cuando abrí el paquete,\n",
      "descubrí para mi horror que me habían enviado una figura de acción de Megatron\n",
      "en su lugar! Como un enemigo de toda la vida de los Decepticons, espero que\n",
      "pueda entender mi dilema. Para resolver el problema, exijo un intercambio de\n",
      "Megatron por la figura Optimus Prime que ordené. Adjunto son copias de mis\n",
      "registros relativos a esta compra. Espero escuchar de usted pronto.\n",
      "Sinceramente, Bumblebee.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the English-to-Spanish translator\n",
    "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "# Translate the text\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "\n",
    "# Print the translated text\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Helsinki-NLP/opus-mt-en-es'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42) # Set the seed to get reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22412b24385c4ae0bb40fabe176b8f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603e4b0756c1469fb134d6629d32136f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7146b2588a2e4815bdeaf443a54b7e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe9b26828c94f3c870a812ab264480d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725883cfdc11449da00c534d072465aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc034b325cfa4285a1a103c1064c145f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782da1e2e9234561bb245bc6fff76e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online\n",
      "store in Germany. Unfortunately, when I opened the package, I discovered to my\n",
      "horror that I had been sent an action figure of Megatron instead! As a lifelong\n",
      "enemy of the Decepticons, I hope you can understand my dilemma. To resolve the\n",
      "issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered.\n",
      "Enclosed are copies of my records concerning this purchase. I expect to hear\n",
      "from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. The only thing\n",
      "I can give you is information about the particular products you received with\n",
      "your request, and I will provide that information to you when I can.\n",
      "\n",
      "You are extremely welcome to come look at your package. The first thing to do is\n",
      "to bring that package over to your local store.\n",
      "\n",
      "I will note to you\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a more recent model\n",
    "\n",
    "The \"Llama\" series of text generation models are produced by Meta (Facebook).  They aren't intended to be competitive with ChatGPT or Claude.  Rather they are efficient models intended to be used by developers and researchers.  When they are fine-tuned for particular applications they can be quite good.  We'll use the [smallest Llama-3.2 model](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) in this class.  To download and use the model yourself you'll need to first sign up for a HuggingFace account (free) and then request access to the model at the link above.  Once you're granted access you can download the model directly from HuggingFace using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online\n",
      "store in Germany. Unfortunately, when I opened the package, I discovered to my\n",
      "horror that I had been sent an action figure of Megatron instead! As a lifelong\n",
      "enemy of the Decepticons, I hope you can understand my dilemma. To resolve the\n",
      "issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered.\n",
      "Enclosed are copies of my records concerning this purchase. I expect to hear\n",
      "from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. I am happy to\n",
      "help you resolve the issue. I have checked our records, and I can confirm that\n",
      "you did indeed order an Optimus Prime action figure, but unfortunately, it\n",
      "appears that there was a miscommunication with our shipping team. I am going to\n",
      "go ahead and process an exchange for the Megatron action figure. I would\n"
     ]
    }
   ],
   "source": [
    "model_original = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_quantized = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
    "generator = pipeline(\"text-generation\", model=model_original)\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, generator, max_length=200):\n",
    "    outputs = generator(prompt, max_length=max_length)\n",
    "    return outputs[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the University of Wisconsin - La Crosse, a public research university\n",
      "located in La Crosse, Wisconsin, USA.\n",
      "The University of Wisconsin - La Crosse (UW-La Crosse) is a public research\n",
      "university located in La Crosse, Wisconsin, USA. It is part of the University of\n",
      "Wisconsin System and is known for its strong programs in education, nursing, and\n",
      "the arts.\n",
      "\n",
      "Here are some key facts about the University of Wisconsin - La Crosse:\n",
      "\n",
      "* **Academic programs:** UW-La Crosse offers over 70 undergraduate majors and 15\n",
      "graduate programs, including programs in education, nursing, business, and the\n",
      "arts.\n",
      "* **Student body:** The university has a diverse student body of around 12,000\n",
      "students, with a student-faculty ratio of 14:1.\n",
      "* **Research:** UW-La Crosse is classified as a High Research Activity\n",
      "institution by the Carnegie Foundation, and offers students opportunities to\n",
      "engage in research projects with faculty mentors.\n",
      "* **Camp\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(\"Describe the University of Wisconsin - La Crosse\", generator)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-win_amd64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jinja2->torch->bitsandbytes) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-win_amd64.whl (68.5 MB)\n",
      "   ---------------------------------------- 0.0/68.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/68.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.4/68.5 MB 10.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 11.8/68.5 MB 28.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 15.5/68.5 MB 25.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 23.1/68.5 MB 27.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 26.2/68.5 MB 25.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 29.9/68.5 MB 24.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 33.8/68.5 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 37.7/68.5 MB 22.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 41.4/68.5 MB 22.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 45.4/68.5 MB 22.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 49.3/68.5 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 53.2/68.5 MB 21.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 57.4/68.5 MB 21.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 61.3/68.5 MB 21.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 65.5/68.5 MB 21.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  68.4/68.5 MB 21.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 68.5/68.5 MB 20.7 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e752e44148a43f3b6482997b7dac40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--rautaditya--llama-3.2-1b-4bit-gptq. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe46934b8f94ba2b49d207d8bfcf89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is the meaning of life? Is it to be found in a single moment? Or is it to be found in a lifetime of moments? If you’re going to ask that question, you’re going to have to find out what you’re going to do with yourself. You’re going to have to do something. And you’re going to have to do it well.\\nI’m not going to give you a list of things to do. I’m not going to give you a list of'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "ModelFolder = \"rautaditya/llama-3.2-1b-4bit-gptq\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ModelFolder)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ModelFolder,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "prompt = \"What is the meaning of life?\"\n",
    "generated_text = pipe(prompt, max_length=100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is the meaning of life? Is it a question that has been answered? Do we have a right to ask the question? I don’t know the answers to any of these questions. I’m sure that there are many different answers to these questions, but I think that the answer to the question of what is the meaning of life is that it is the only question that we can ever ask. I think that the only thing that we can ever ask is what is the meaning of life?'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the meaning of life?\"\n",
    "generated_text = pipe(prompt, max_length=100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? The meaning of life is the question that most people ask themselves, and for many, it is a question that has never been answered. But there is a certain sense in which the question is not really a question at all. It is more like a statement of fact: \"The meaning of life is the meaning of life.\"\n",
      "The meaning of life is the meaning of life, and the meaning of life is the meaning of life. The meaning of life is the meaning of life, and the meaning of life is the meaning of life. The meaning of life is the meaning of life, and the meaning of life is the meaning of life. The meaning of life is the meaning of life, and the meaning of life is the meaning of life. The meaning of life is the meaning of life, and the meaning of life is the meaning of life.\n",
      "The meaning of life is the meaning of life, and the meaning of life is the meaning of life. The\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, generator, max_length=200):\n",
    "    outputs = generator(prompt, max_length=max_length)\n",
    "    return outputs[0]['generated_text']\n",
    "\n",
    "output = generate_text(\"What is the meaning of life?\", pipe)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.660827000537946"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "(np.sqrt(125)-1)*np.pi/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
