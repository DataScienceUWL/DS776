{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TOKEN = os.environ.get('HF_TOKEN', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the model name or path\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "def load_llama_model(model_name=MODEL_NAME, device='cuda', token=TOKEN):\n",
    "    \"\"\"\n",
    "    Load the LLaMA model and tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,use_auth_token=token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",  # Automatically selects FP16 if GPU is used\n",
    "        device_map=\"auto\",    # Automatically maps the model to GPU\n",
    "        use_auth_token=token\n",
    "    )\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_llama_response(prompt, model, tokenizer, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate a response to a given prompt using the LLaMA model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_llama_model(model):\n",
    "    \"\"\"\n",
    "    Unload the model and clear GPU memory.\n",
    "    \"\"\"\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import TextWrapper\n",
    "\n",
    "def wrap_print_text(print):\n",
    "    \"\"\"Adapted from: https://stackoverflow.com/questions/27621655/how-to-overload-print-function-to-expand-its-functionality/27621927\"\"\"\n",
    "\n",
    "    def wrapped_func(text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        wrapper = TextWrapper(\n",
    "            width=80,\n",
    "            break_long_words=True,\n",
    "            break_on_hyphens=False,\n",
    "            replace_whitespace=False,\n",
    "        )\n",
    "        return print(\"\\n\".join(wrapper.fill(line) for line in text.split(\"\\n\")))\n",
    "\n",
    "    return wrapped_func\n",
    "\n",
    "# Wrap the print function\n",
    "print = wrap_print_text(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate a response to a given prompt using the LLaMA model.\n",
    "    \"\"\"\n",
    "    # Ensure the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,  # Ensures padding is applied if needed\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(\"How many moons does Jupiter have?\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many moons does Jupiter have? As of my last update in 2023, Jupiter has a\n",
      "total of 92 confirmed moons. The largest moon of Jupiter is Ganymede, which is\n",
      "also the largest moon in the solar system.\n",
      "\n",
      "Here's a list of the moons of Jupiter, with their sizes in kilometers (km) and\n",
      "their orbital periods in days (days):\n",
      "\n",
      "1. **Io** - 4,266 km\n",
      "2. **Europa** - 4,879 km\n",
      "3. **Ganymede** - 5,262 km\n",
      "4. **Callisto** - 4,821 km\n",
      "5. **Amalthea** - 1,433 km\n",
      "6. **Thebe** - 1,457 km\n",
      "7. **Leda** - 1,434 km\n",
      "8. ** Himalia** - 1,394 km\n",
      "9. **Elara** - 1,394 km\n",
      "10. **Car\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
