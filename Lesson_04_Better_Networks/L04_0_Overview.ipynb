Lesson 04 - Overview
Outcomes
Understand and apply ReLU and LeakyReLU activations to address vanishing gradient problems and enhance network convergence.
Implement batch and layer normalization to stabilize training and improve network performance.
Analyze and utilize residual connections to enable deeper network architectures by mitigating vanishing gradient issues.
Readings and Videos
Read Sections 6.1-6.4 from Inside Deep Learning

Course Notebooks with Videos Open each of the notebooks included the lesson folder and watch the embedded video. You can read along and work through the code examples as you want. The notebooks are numbered in the order they should be used.

Assessments
Complete the reading quiz in Canvas (10 points).
Complete the exercises in your the homework notebook in CoCalc (40 points).