{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "### DO THIS FIRST\n",
    "\n",
    "Change `force_update=True` in the last line and run the next cell to install an updated course package.  Once it's done restart your kernel and change back to `force_update=False`.  You only need to do this once per server (not once per notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "#### L07_1_Getting_Started_with_NLP Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/oDi5d1FbYBx\" target=\"_blank\">Open Descript version of video in new tab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## A Tiny History of Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) has evolved significantly over the past few decades. Initially, NLP relied heavily on rule-based systems and statistical methods to understand and generate human language. These early approaches, prominent in the 1980s and 1990s, focused on the syntactic structure of text, using techniques such as n-grams and Hidden Markov Models (HMMs) to model language. However, these methods struggled with capturing the semantic meaning and context of words.\n",
    "\n",
    "The introduction of word embeddings in the early 2010s, such as Word2Vec and GloVe, marked a significant advancement in NLP. These embeddings allowed for the representation of words in continuous vector space, capturing semantic relationships between words. This shift enabled more sophisticated models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, to process sequences of text and maintain context over longer passages. RNNs, in particular, played a crucial role in tasks like language translation and sentiment analysis.\n",
    "\n",
    "The advent of transformers in 2017 revolutionized NLP by addressing the limitations of RNNs. Transformers, introduced with the Attention is All You Need paper, utilize self-attention mechanisms to process entire sequences of text simultaneously, allowing for better handling of long-range dependencies and parallelization. This led to the development of powerful models like BERT, GPT, and T5, which have set new benchmarks in various NLP tasks by providing a deeper semantic understanding of text.\n",
    "\n",
    "Transformers have almost entirely supplanted previous approaches to NLP because:\n",
    "\n",
    "1. **Superior Performance:** Models like BERT, GPT, T5, and their successors dominate leaderboards on tasks such as text classification, translation, summarization, and question answering.\n",
    "2. **Pretraining and Transfer Learning:** Unlike traditional methods that required training separate models from scratch for different tasks, transformers leverage large-scale pretraining on vast text corpora and fine-tune efficiently on specific tasks.\n",
    "3. **Self-Attention and Contextual Representations:** Transformers provide rich, context-dependent word representations, whereas earlier models like Word2Vec and GloVe generated static embeddings.\n",
    "4. **Scalability and Adaptability:** With advancements in scaling laws, models can achieve better performance just by increasing their size and training data, an advantage that RNNs and classical machine learning approaches lacked.\n",
    "\n",
    "There are a few areas where older approaches still exist:\n",
    "\n",
    "1. **Small Datasets & Low Compute Environments:** Logistic regression, SVMs, and Lasso-penalized models often remain competitive when data is limited or when computational efficiency is a concern.\n",
    "2. **Domain-Specific Applications:** Some applications, like biomedical text mining, may still rely on domain-specific feature engineering approaches alongside transformers.\n",
    "3. **Traditional ML for Interpretability:** Some NLP applications in finance, healthcare, and legal fields still favor older methods due to the need for interpretability and robustness.\n",
    "\n",
    "However, since transformer models for NLP are now so dominant we will focus exclusively on them in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## NLP Tasks Instead of Transformer Details\n",
    "\n",
    "Transformers are more complicated than the CNNs we saw for computer vision so we're not going to dive as deeply into the details. We will, in Lesson 9 - Transformer Details, learn about some of the nuts and bolts especially the self-attention mechanism that allows transformers to figure out relationships between words and to understand context. Mostly, though, we will focus on the applications of transformers. To this end we'll dive into the open source HuggingFace ecosystem which hosts thousands of NLP models and datasets and makes it quite simple to dive into NLP applications without having to master too much code. All of the newest, biggest open source transformer models are hosted there including those from Meta, Mistral, and Deepseek. The only thing keeping us from running the biggest state-of-the-art models will be lack of compute, but we can run their smaller cousins on the GPU in CoCalc's compute server, a decent gaming GPU, or even a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## API-based LLMs versus Fine-tuning Specialized Models\n\nAs large language models (LLMs) continue to improve, their use as general NLP task solvers via prompting is increasingly popular, especially when we don't have access to large amounts of training data. In this course, we'll focus on two main approaches to solving NLP tasks:\n\n1. **Using LLMs via APIs** (like GPT-4o, Claude, or Gemini through OpenRouter)\n2. **Fine-tuning specialized transformer models** for specific tasks\n\n*(We'll also explore running LLMs locally in Lesson 11, but for Lessons 7-10 and 12 we'll use API-based models and task-specific fine-tuned models.)*\n\n### Example: Text Classification\n\nFor a text-classification task, you could choose:\n\n**LLM via API (GPT-4o, Claude, Gemini, etc.)**\n- When you need **a quick, general-purpose classifier** without training a model\n- When **zero-shot or few-shot classification** (via prompting) is sufficient\n- When categories may evolve frequently, making retraining impractical\n- When you don't have a large labeled dataset\n- Example: Categorizing support tickets by topic\n\n**Fine-tune BERT / RoBERTa / DistilBERT**\n- When you have a **moderate to large labeled dataset** and need **high accuracy**\n- When you need **fast inference at scale**, as fine-tuned models are more efficient than large LLMs\n- When your classification task requires **domain-specific adaptation**\n- When you need **very low latency** or **predictable costs**\n- Example: Sentiment analysis on customer feedback in a specific industry\n\n**Note on terminology:** Zero-shot classification means classifying text without seeing any examples - the LLM just gets a prompt with the possible categories. Few-shot classification means providing a small number of examples in the LLM prompt to guide the model's behavior.\n\n### Choosing the Right Approach\n\n**Use API-based LLMs when:**\n- You need **quick, adaptable solutions** without training infrastructure\n- You **don't have much labeled data** for fine-tuning\n- You want to **experiment rapidly** with different task formulations\n- Task requirements may change frequently\n- You're prototyping or building proof-of-concepts\n\n**Fine-tune a specialized model when:**\n- You have **domain-specific labeled data** and need **high accuracy**\n- You need **very fast inference** or processing at large scale\n- You need **predictable costs** (no per-token API charges)\n- You require **consistent, structured outputs**\n- Latency is critical (milliseconds matter)\n\n### Understanding Data Privacy with API-based LLMs\n\nA common concern with API-based LLMs is: **\"Will my data be used to train the model?\"** or **\"Is my sensitive data secure?\"** The answer depends on the provider and the agreements in place.\n\n**Privacy Protections Available:**\n\nMost major LLM providers now offer enterprise-grade privacy protections:\n- **Zero Data Retention (ZDR):** Your API requests are not stored or logged after processing\n- **Data Processing Agreements (DPAs):** Legal contracts preventing use of your data for model training\n- **HIPAA and SOC 2 Compliance:** Meeting healthcare and security standards for regulated industries\n- **Private Deployments:** Dedicated instances in your own cloud environment (e.g., Azure OpenAI, AWS Bedrock)\n- **Regional Data Residency:** Keep data within specific geographic boundaries (e.g., EU-only processing)\n\n**Examples:**\n- **OpenAI API:** Has a default policy not to use API data for training. Enterprise customers can enable additional protections.\n- **Azure OpenAI Service:** Fully isolated deployments in your Azure subscription with complete data control\n- **Google Vertex AI:** Private endpoints with data residency controls and enterprise security\n- **Anthropic Claude:** API data not used for training; enterprise options for additional controls\n\n**When API Privacy May Not Be Enough:**\n\nEven with these protections, there are situations where API-based solutions may not be acceptable:\n- **Air-gapped environments:** Systems physically isolated from external networks (e.g., classified government systems)\n- **Extreme regulatory restrictions:** Some industries may prohibit any external data transmission regardless of agreements\n- **Zero-trust requirements:** Organizations that cannot accept any third-party processing, even contractually protected\n- **Competitive intelligence:** Proprietary algorithms or trade secrets that cannot be exposed, even with DPAs\n\nIn these cases, running models locally (Lesson 11) or fine-tuning your own specialized models on internal infrastructure becomes necessary.\n\n**Bottom Line:** For most educational, research, and business applications, modern API providers offer sufficient privacy protections through contractual agreements and technical controls. Understanding your specific regulatory requirements and risk tolerance will guide your choice.\n\n### Course Approach\n\nFor each NLP task in Lessons 7-10 and 12, we'll explore both API-based LLM approaches and fine-tuned specialized models. In Lesson 11, we'll dive deeper into text generation and demonstrate running LLMs locally for complete control and privacy."
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## OpenRouter API and Your Course API Keys\n\nFor this course, we've set up access to Large Language Models through **OpenRouter**, a unified API that provides access to all major commercial models (GPT-4o, Claude, Gemini) and most open-weight models (Llama, Mistral, DeepSeek, Qwen, and many more). This means you can experiment with different models using a single API interface.\n\n### Your API Credit\n\n**Each student has been provided with $15 in OpenRouter API credit.** This should be more than sufficient to complete all coursework if you use small and medium-sized models appropriately. For reference:\n\n- **Small models** (like `gemini-flash-lite`, `llama-3.2-3b`, `gpt-4o-mini`): Very inexpensive, typically $0.075-0.15 per million input tokens\n- **Medium models** (like `gemini-flash`, `claude-haiku`): Moderate cost, good quality\n- **Premium models** (like `gpt-4o`, `claude-sonnet`, `o3-mini`): Higher cost, best quality\n\nWe recommend using **`gemini-flash-lite`** as your default model for coursework - it's fast, inexpensive, and produces good results for learning tasks.\n\nIf you want to experiment beyond the course assignments or try premium models, you can always purchase your own OpenRouter API key and load it with whatever credit you choose.\n\n### Checking Your Remaining Credit\n\nYou can check your remaining OpenRouter credit using the `llm_get_credits()` function from the course package. This will show you how much of your $15 credit remains:\n\n```python\nfrom introdl.nlp import llm_get_credits\n\ncredits = llm_get_credits()\nprint(f\"Remaining credit: ${credits['usage']:.2f} of ${credits['limit']:.2f}\")\nprint(f\"Credit remaining: ${credits['limit'] - credits['usage']:.2f}\")\n```\n\n### Your API Keys Are Already Configured\n\nYour OpenRouter API key has already been distributed to your CoCalc project and is stored in:\n```\n~/home_workspace/api_keys.env\n```\n\nWhen you run `config_paths_keys()` in your import cell (as shown below), this API key will be automatically loaded and available for use with `llm_generate()`. You don't need to do anything else!\n\n**Security Note:** Never commit your `api_keys.env` file to git or share it publicly. The file is stored in `home_workspace` which should not be tracked by version control.\n\n### Exploring Available Models\n\nThe course package includes 16 carefully curated models covering a range of capabilities and price points. You can see them all with `llm_list_models()`, which we'll demonstrate shortly.\n\n**Want to try models beyond our curated list?** OpenRouter provides access to hundreds of models! You can:\n\n1. **Browse all available models** at: https://openrouter.ai/models\n2. **Use any model** by providing its full OpenRouter model ID\n\nFor example, to use OpenAI's new GPT-5-nano model (not in our curated list), you would use:\n\n```python\nresponse = llm_generate('openai/gpt-5-nano', \"Your prompt here\")\n```\n\nThe full model ID format is typically `provider/model-name` (e.g., `openai/gpt-5-nano`, `anthropic/claude-opus-4.1`, `google/gemini-2.5-pro`).\n\n**Note:** Models outside our curated list won't show pricing or metadata with `llm_list_models()`, but they'll work fine if you provide the correct model ID from the OpenRouter website."
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Using `llm_generate` with OpenRouter\n\nThe course package provides a simple, unified interface for working with LLMs through the `llm_generate()` function. This function handles all the complexity of API calls, cost tracking, and response formatting.\n\n### Setting Up and Checking Your Credit\n\nFirst, let's import the necessary functions, configure our environment, and check your OpenRouter credit balance:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "from introdl.utils import config_paths_keys, wrap_print_text\nfrom introdl.nlp import (\n    llm_generate, llm_list_models, llm_get_credits,\n    llm_configure, display_markdown, show_pricing_table\n)\n\n# Configure paths and load API keys\npaths = config_paths_keys()\n\n# Initialize LLM cost tracking system\nllm_configure()\n\n# Wrap print to format text nicely at 80 characters\nprint = wrap_print_text(print)\n\n# Check your OpenRouter credit balance\ncredits = llm_get_credits()\nprint(f\"OpenRouter Credit Status:\")\nprint(f\"  Total limit: ${credits['limit']:.2f}\")\nprint(f\"  Used so far: ${credits['usage']:.2f}\")\nprint(f\"  Remaining:   ${credits['limit'] - credits['usage']:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "### Simple Example\n\nNow let's try a simple text generation example. The new `llm_generate()` API is very straightforward:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text generation\n",
    "response = llm_generate('gemini-flash-lite', \"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Using System Prompts\n",
    "\n",
    "System prompts help guide the model's behavior and tone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful AI assistant who is also sarcastic and talks like a pirate.\"\n",
    "\n",
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Tell me three interesting facts about space.\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Displaying Markdown Output\n",
    "\n",
    "Many LLM responses use markdown formatting. You can display them nicely using `display_markdown()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Write a short bullet-point list of tips for learning machine learning.\"\n",
    ")\n",
    "\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Tracking Costs\n",
    "\n",
    "You can see estimated costs for your API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Tell me five dad jokes.\",\n",
    "    estimate_cost=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Controlling Output Length\n",
    "\n",
    "You can control how much text the model generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longer response\n",
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Write a short story about a cat who learns to play the piano.\",\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Viewing Available Models\n",
    "\n",
    "You can see all available models and their details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_list_models()"
   ]
  },
  {
   "cell_type": "code",
   "id": "6vnl1r84lwm",
   "source": "### Viewing Model Pricing\n\nYou can also see detailed pricing information for all available models. This helps you estimate costs before running expensive queries:",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0xt7634qud7a",
   "source": "show_pricing_table()",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Trying Different Models\n",
    "\n",
    "It's easy to compare different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "# Try a free model\n",
    "print(\"Free model (llama-3.2-3b):\")\n",
    "response1 = llm_generate('llama-3.2-3b', prompt)\n",
    "print(response1)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Try the recommended model\n",
    "print(\"Recommended model (gemini-flash-lite):\")\n",
    "response2 = llm_generate('gemini-flash-lite', prompt, estimate_cost=True)\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ev36qb1idx",
   "source": "### Using Models Outside the Curated List\n\nYou can use any model from [OpenRouter's model list](https://openrouter.ai/models) by providing the full model ID. For example, let's try OpenAI's GPT-5-nano (which isn't in our curated list):",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pe90mn0t2no",
   "source": "# Use the full OpenRouter model ID\nresponse = llm_generate(\n    'openai/gpt-5-nano',  # Full model ID from openrouter.ai/models\n    \"What are three benefits of learning Python?\",\n    estimate_cost=True\n)\n\nprint(response)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Processing Multiple Prompts\n",
    "\n",
    "You can process multiple prompts at once by passing a list of strings. This is useful for batch processing tasks like sentiment analysis or classification.\n",
    "\n",
    "### Simple Batch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'What is the capital of Germany?',\n",
    "    'What is the capital of Italy?'\n",
    "]\n",
    "\n",
    "responses = llm_generate('gemini-flash-lite', prompts)\n",
    "\n",
    "for prompt, response in zip(prompts, responses):\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Programmatic Prompt Construction\n",
    "\n",
    "Often we want to construct prompts programmatically from data. Here's an example of sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for sentiment analysis\n",
    "system_prompt = \"You are a sentiment analysis AI. Classify text as Positive, Negative, or Neutral.\"\n",
    "\n",
    "# List of texts to analyze\n",
    "texts = [\n",
    "    \"I love the new design of your website!\",\n",
    "    \"The service was terrible and I will not come back.\",\n",
    "    \"The product is okay, but it could be better.\",\n",
    "    \"Absolutely fantastic experience, highly recommend!\",\n",
    "    \"I'm not sure how I feel about this.\"\n",
    "]\n",
    "\n",
    "# Construct prompts programmatically\n",
    "instruction = \"Analyze the sentiment of this text. Give only the sentiment classification (Positive, Negative, or Neutral).\\n\\nText: \"\n",
    "prompts = [instruction + text for text in texts]\n",
    "\n",
    "# Generate responses\n",
    "responses = llm_generate('gemini-flash-lite', prompts, system_prompt=system_prompt)\n",
    "\n",
    "# Display results\n",
    "print(\"Sentiment Analysis Results:\\n\")\n",
    "for text, sentiment in zip(texts, responses):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In the next notebook, we'll explore common NLP tasks including:\n",
    "- Text classification and sentiment analysis\n",
    "- Named Entity Recognition (NER)\n",
    "- Question answering\n",
    "- Translation\n",
    "- Summarization\n",
    "\n",
    "In Lesson 11, we'll dive deeper into how text generation works, explore the underlying APIs in detail, and learn about running LLMs locally for privacy-sensitive applications.\n",
    "\n",
    "For now, practice using `llm_generate()` with different models and prompts to get comfortable with the interface!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}