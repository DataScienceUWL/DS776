{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Course Syllabus\n",
    "\n",
    "## Course Objectives: (These need rewording)\n",
    "- Understand the conceptual framework of deep learning models/applications but maybe not all the details.\n",
    "- Be able to utilize pretrained and foundational models, perhaps with finetuning.\n",
    "- Be able to manage a simple deep learning workflow:  build and split dataset, train or fine tune model, save model, load model, inference, warm restarts, possibly monitoring experiments, possibly deployments\n",
    "- Assessing Model Performance and Limitations (Data Bias)\n",
    "- Computer Vision Applications.  NLP Applications.\n",
    "\n",
    "## Textbooks and Other Resources\n",
    "\n",
    "### Required Books\n",
    "- [\"Inside Deep Learning: Math, Algorithms, Models\" by Edward Raff](https://www.amazon.com/Inside-Deep-Learning-Algorithms-Models/dp/1617298638/ref=sr_1_1?crid=32FZFJXN81JCC&dib=eyJ2IjoiMSJ9.pVb-gd6L20N_JNl6QLReqW_Ru_I-npjeZRSDzDujZg8qkJUpi9vRQ11tK-Jgv8lJ_9lJ1BDFVrq5KsdvYr5tTp3P4Hynb4DU_dMAgDRBpaU.ZXz9Dxm_mU9LEpRx8Jzgjul9xWXc51ZWqoDiGCQh-gg&dib_tag=se&keywords=inside+deep+learning+math%2C+algorithms%2C+models&qid=1729870668&sprefix=inside+deep+lea%2Caps%2C137&sr=8-1)\n",
    "- [\"Natural Language Processing with Transformers, Revised Edition\" by Tunstall, et al](https://www.amazon.com/Natural-Language-Processing-Transformers-Revised/dp/1098136799/ref=sr_1_1?crid=3CVB0BRZHS7GI&dib=eyJ2IjoiMSJ9.n87OC6jhL6N-C2KDf_e-0bXcjttnGT8821PDe5oHt6z9QpOBt1y-wdQTqxUx94wbrYfbvOcm14q2FYDNw9bD0oGmvMbTLDE8v4P0Zb0WAp08rxbUkdsp0uVxm55I-M4Z3Vk85S2-F0nzD07rym-NJKNx6BcIMyZNkUn6WoL7AJ9HGN8Wr_iOxRbgyu1lSgCj3IbiFEYQaH36C4bo0GDjMBvdttOga5FQxOUvKyDSSCc.QEAnSA-iAS3aC3D2zzqjB8j9f98PPvlrJS5EHlhjcmU&dib_tag=se&keywords=natural+language+processing+with+transformers&qid=1729870809&sprefix=natural+langua%2Caps%2C136&sr=8-1)\n",
    "\n",
    "###  Free Online Books\n",
    "- [Dive Into Deep Learning](https://d2l.ai/)  Shows how to code in multiple frameworks, but you'll need to use their packages to get things up and running.  \n",
    "- [Understanding Deep Learning](https://udlbook.github.io/udlbook/)  Explains a lot of the mathematical details about various deep learning models.  Great diagrams, but not light reading.\n",
    "- [Deep Learning](https://www.deeplearningbook.org/)  From MIT.  Good book for the first half of the class.\n",
    "\n",
    "### Other Resources:\n",
    "- **Documentation:**\n",
    "    - **Hugging Face Documentation:** [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "    - **PyTorch Documentation:** [PyTorch](https://pytorch.org/docs/stable/index.html)\n",
    "- **Tutorials:**\n",
    "    - **PyTorch Tutorials:** The [official PyTorch tutorials](https://pytorch.org/tutorials/) are quite good.\n",
    "\n",
    "\n",
    "- **Classes:**\n",
    "    - [Math for Machine Learning and Data Science](https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science?) Coursera three course specialization: linear algebra, calculus, and probability and statistics.  This 3-month specialization is highly recommended for a deeper dive.\n",
    "    - **Introduction to Deep Learning from MIT**  [Free public course](http://introtodeeplearning.com/) updated annually.  The beginning lecture is very accessible.\n",
    "    - **Neural Networks and Deep Learning.** Andrew Ng is a big name in AI and I love listening to him.  This [playlist](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0) is all the videos from the first course of his Deep Learning Specialization on Coursera.  This is a really great resource for a deep learning introduction.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics by Week\n",
    "\n",
    "1.\tFully Connected Networks\n",
    "2.\tCNNs\n",
    "3.\tBetter Training\n",
    "4.\tBetter Networks\n",
    "5.\tObject Detection and Segmentation\n",
    "6.\tTransfer Learning\n",
    "7.\tClassical NLP Techniques and Foundations\n",
    "8.\tIntroduction to Transformers\n",
    "9.\tText Classification\n",
    "10.\tNamed Entity Recognition\n",
    "11.\tText Generation\n",
    "12.\tQuestion Answering\n",
    "13. Project \n",
    "14. Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lesson 1: Introduction to Deep Learning and Fully Connected Networks\n",
    "\n",
    "### Topics\n",
    "* Introduction to PyTorch\n",
    "* Fully-connected neural networks for regression and classification\n",
    "* Modeling process: prepare data, prepare model, train model, evaluate model, make predictions\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand deep learning fundamentals**, including the structure of neural networks.\n",
    "2. **Utilize PyTorch datasets and data loaders** to efficiently handle and preprocess data for training deep learning models.\n",
    "3. **Develop and train basic fully connected neural networks** in PyTorch, applying optimization techniques and appropriate loss functions.\n",
    "4. **Evaluate model performance** by plotting loss functions and other metrics.\n",
    "5. **Make predictions for new data** by applying a trained neural network to new inputs.\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* **Course Intro Notebook / Video** (Still to come)\n",
    "\n",
    "* **(Optional) Review Neural Networks from DS740**\n",
    "\n",
    "    * You might want to review the first 14 slides of the [Lesson on Neural networks in DS740](https://media.uwex.edu/content/ds/ds740_r23/ds740_artificial-neural-networks.sbproj/).  We're covering similar material this week.  Don't review the material about neural networks in R since we'll be using Python.\n",
    "\n",
    "* **Readings from Inside Deep Learning (IDL)**\n",
    "\n",
    "    * **Chapter 1: The Mechanics of Learning**\n",
    "        - **Read Sections 1.2, 1.4, and 1.5**. Skim the other sections. No need to understand the detailed code or the backpropagation algorithm, but ensure you understand how the gradient is used in training.\n",
    "\n",
    "    * **Chapter 2: Fully Connected Networks**\n",
    "        - **Section 2.1**: Focus on understanding the training loop structure and process. Skip the code details but grasp the concept.\n",
    "            - Don’t worry about the math notation at the bottom of page 40. It's shorthand for a fully connected linear layer. If you want to learn more about matrix multiplication see the videos listed under Auxiliary Materials below.\n",
    "        - **Section 2.2**: Understand how activation functions introduce nonlinearity into networks.\n",
    "        - **Section 2.3**: Grasp the basics of softmax and cross-entropy, especially how the loss function changes for classification tasks. An example will be explained in a notebook and video.\n",
    "        - **Section 2.4**: Note the key concepts; they will be reinforced in video lectures.\n",
    "        - **Section 2.5**: Understand the importance of batch training, particularly for large datasets that won’t fit in memory.\n",
    "\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included in the lesson folder and watch the embedded video.  You can read along and work through the code examples as needed.  The notebooks for this lesson are in the Lesson_01 directory.  The notebooks are numbered in the order they should be used.\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "### Auxiliary Materials\n",
    "\n",
    "- **Background Mathematics from Dr. Anne Hsu:**\n",
    "    * [Matrices and Vectors](https://www.youtube.com/watch?v=sM2Mm6aT_HI)\n",
    "    * [Derivatives and Gradients 1](https://www.youtube.com/watch?v=Fiw0_w4AykA)\n",
    "    * [Derivatives and Gradients 2](https://www.youtube.com/watch?v=qORZmKCB0g8)\n",
    "    * [Playlist for entire Intro to Deep Learning](https://www.youtube.com/@drannehsu/playlists)\n",
    "- **Object-Oriented Programming** It helps to have a basic familiarity with classes, inheritance, and methods for finding your way around in PyTorch.  [Real Python has a great tutorial](https://realpython.com/python3-object-oriented-programming/) on the basics of OOP with many code examples that you can either read or watch (40 minutes).\n",
    "- **Deep Learning Basics: Introduction and Overview**  Lex Fridman is a well known podcast host for AI and Data Science.  Here he gives an [introductory lecture](https://youtu.be/O5xeyoRL95U?si=SrM7RLWB_iBPMiK4) for MIT's public deep learning class.  Since it was recorded in 2019 it doesn't include the latest on transformer architectures that are driving the current boom in AI (ChatGPT, etc.), it's still a great introduction that discusses many applications of deep learning.  Watch this if you want a good overview.  I also recommend his podcast.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Convolutional Neural Networks\n",
    "\n",
    "### Topics\n",
    "* Image data\n",
    "* Convolutional Layers\n",
    "* Pooling Layers\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand the structure and role of Convolutional Neural Networks (CNNs)** in processing spatial data like images.\n",
    "2. **Design and implement basic CNN architectures** in PyTorch, including convolutional layers, pooling, and activation functions.\n",
    "3. **Understand how padding, stride, kernel size, and the number of output channels** interact to determine the dimensionality of the output in each convolutional layer.\n",
    "### Readings and Videos\n",
    "\n",
    "* Read Chapter 3, through Section 3.5, in Inside Deep Learning.  You can read 3.6 if you wish, we'll get into that material in the next lesson.\n",
    "* [Andrew Ng on Convolution over Volumes](https://www.youtube.com/watch?v=KTB_OFoAQcc&ab_channel=DeepLearningAI) This is one of many videos Andrew Ng has made to support his deep learning course.  Watch this to solidify your understanding convolutions after doing the reading.  About 11 minutes.\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_02 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Better Training\n",
    "\n",
    "## Topics:\n",
    "* Overfitting\n",
    "* Optimizers\n",
    "* Regularization\n",
    "* Splitting Data\n",
    "* Data Augmentation\n",
    "* Learning Rate Schedulers\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Apply data augmentation techniques** to improve model generalization, especially with small datasets.\n",
    "2. **Understand and implement learning rate schedules**, including exponential decay, step drop, and cosine annealing.\n",
    "3. **Optimize training** using modern techniques like SGD with momentum, Adam, and gradient clipping.\n",
    "4. **Implement early stopping** to prevent overfitting and improve training efficiency by halting training when performance plateaus.\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* Read Sections 3.6, 5.1-5.3 in Inside Deep Learning.\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_03 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Overfitting:** What is it?  What are common approaches to preventing it?  Why is a common problem in deep learning?\n",
    "2. **Optimizers:** Learn about alternatives to SGD.  Adaptive learning rates.  Momentum.\n",
    "3. **Regularization:** L2 and L1 regulurization.  L2 is built into many optimizers.  L1 needs a term added to loss function.\n",
    "4. **Learning Rate Schedulers:** What do they do?  General advice.\n",
    "5. **Data Augmentation:** What is it?  Why is it helpful?  How to choose augmentations.\n",
    "\n",
    "### Auxiliary Materials\n",
    "- [Optimization Techniques in Deep Learning (Video)](https://www.youtube.com/watch?v=G_wG9tCVA3k) - Explanation of various optimization algorithms.\n",
    "- [Regularization in Deep Learning (Video)](https://www.youtube.com/watch?v=ZLUtucsaE5g) - Detailed lecture on regularization methods.\n",
    "- [PyTorch Optimization Techniques (Video)](https://www.youtube.com/watch?v=t9aUoUnwhvI) - Implementing different optimization algorithms in PyTorch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4: Better Networks\n",
    "\n",
    "### Topics\n",
    "* Improved Activation Functions\n",
    "* Batch and Layer Normalization\n",
    "* Skip and Residual Connections\n",
    "\n",
    "### Outcomes\n",
    "* **Understand and apply ReLU and LeakyReLU activations** to address vanishing gradient problems and enhance network convergence.\n",
    "* **Implement batch and layer normalization** to stabilize training and improve network performance.\n",
    "* **Analyze and utilize residual connections** to enable deeper network architectures by mitigating vanishing gradient issues.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read Sections 6.1-6.4 from Inside Deep Learning\n",
    "* Course Notebooks with Videos Open each of the notebooks included the lesson folder and watch the embedded video. You can read along and work through the code examples as you want. The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "* Complete the reading quiz in Canvas (10 points).\n",
    "* Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "### Auxiliary Materials\n",
    "* **Activation Functions** There are more than the three activation functions we discussed in our video.  [All the Activation Functions](https://dublog.net/blog/all-the-activations/) is a quick summary of most of activation functions used in deep learning.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Object Detection and Segmentation\n",
    "\n",
    "### **Week Overview: Object Detection (Chapter 8)**\n",
    "\n",
    "**Objective:** This week focuses on introducing key concepts and techniques in object detection, such as image segmentation, transposed convolutions, and bounding box-based detection using Faster R-CNN. Students will gain a thorough understanding of object detection models and implement them using PyTorch.\n",
    "\n",
    "### **Lecture Breakdown:**\n",
    "\n",
    "1. **Lecture 1: Fundamentals of Object Detection and Image Segmentation**\n",
    "   - **Topics Covered:**\n",
    "     - Overview of object detection and its real-world applications.\n",
    "     - Introduction to image segmentation as a foundational task.\n",
    "     - Pixel-wise classification and creating segmentation masks.\n",
    "     - Building segmentation models using U-Net architecture.\n",
    "   - **Jupyter Notebook 1:** Implement a simple U-Net for image segmentation on a dataset like the PASCAL VOC dataset.\n",
    "     - **Learning Goal:** Understand per-pixel classification and learn to build a basic segmentation model using transposed convolutions.\n",
    "\n",
    "2. **Lecture 2: Bounding Boxes and Object Detection with Faster R-CNN**\n",
    "   - **Topics Covered:**\n",
    "     - Transition from pixel-wise predictions to bounding box predictions.\n",
    "     - Understanding and implementing Faster R-CNN for detecting objects in images.\n",
    "     - Working with region proposals and applying bounding boxes to locate objects.\n",
    "   - **Jupyter Notebook 2:** Implement a basic object detection pipeline using Faster R-CNN with PyTorch’s built-in functionalities.\n",
    "     - **Learning Goal:** Gain hands-on experience with region proposal-based object detection and bounding box predictions.\n",
    "\n",
    "3. **Lecture 3: Advanced Object Detection Techniques and Filtering Results**\n",
    "   - **Topics Covered:**\n",
    "     - Strategies for suppressing overlapping bounding boxes (Non-Max Suppression).\n",
    "     - Reducing false positives in detection models.\n",
    "     - Understanding and utilizing pretrained object detection models for faster and more accurate results.\n",
    "   - **Jupyter Notebook 3:** Fine-tune a pretrained Faster R-CNN model and perform detection on custom images.\n",
    "     - **Learning Goal:** Learn to adapt pretrained models for custom tasks and implement post-processing techniques to refine detections.\n",
    "\n",
    "### **Assignment: Implementing a Custom Object Detector**\n",
    "\n",
    "**Title:** \"Building an Object Detector with U-Net and Faster R-CNN\"\n",
    "\n",
    "- **Description:** \n",
    "  The assignment tasks students to implement and train two models—one for segmentation and another for object detection. First, they will build a segmentation model using U-Net to identify objects at a pixel level. Then, they will implement and fine-tune a Faster R-CNN model for bounding box-based detection.\n",
    "\n",
    "- **Requirements:**\n",
    "  - **Image Segmentation Model:** Train a U-Net to segment objects in a dataset like PASCAL VOC or a simpler dataset. Visualize segmentation masks and calculate accuracy metrics.\n",
    "  - **Object Detection Model:** Implement or fine-tune a Faster R-CNN using a dataset with bounding box annotations. Evaluate and visualize the results, highlighting detected objects.\n",
    "  - Submit a report explaining the approaches taken for segmentation and detection, including challenges faced and how results were refined.\n",
    "\n",
    "- **Learning Outcomes:** \n",
    "  - Develop a deeper understanding of segmentation and object detection tasks.\n",
    "  - Gain experience with both custom and pretrained models for different detection tasks.\n",
    "  - Understand and implement post-processing techniques to improve object detection results.\n",
    "\n",
    "### **Resources Needed:**\n",
    "- **Lecture Videos:** Pre-recorded videos covering theoretical concepts and code walkthroughs.\n",
    "- **Jupyter Notebooks:** Detailed code for U-Net and Faster R-CNN implementations.\n",
    "- **Assignment Data:** Example datasets like PASCAL VOC or a custom dataset with both segmentation masks and bounding box annotations.\n",
    "\n",
    "This plan provides a comprehensive introduction to object detection, emphasizing hands-on experience with segmentation and bounding box-based detection using widely used architectures.\n",
    "\n",
    "**Textbook Support:**\n",
    "- **Chapter 8 - Object Detection and Segmentation**:\n",
    "  - Segmentation (Section 8.1).\n",
    "  - Image segmentation with U-Net.\n",
    "  - Object detection with bounding boxes (Section 8.4).\n",
    "  - Implementation of Faster R-CNN.\n",
    "\n",
    "**Video Lectures:**\n",
    "- [PyTorch Image Segmentation Tutorial with U-NET: everything from scratch baby](https://www.youtube.com/watch?v=IHq1t7NxS8k)\n",
    "- [Implement and Train U-NET From Scratch for Image Segmentation - PyTorch](https://www.youtube.com/watch?v=HS3Q_90hnDg)\n",
    "\n",
    "**Tutorials:**\n",
    "- **Image Segmentation with U-Net**:\n",
    "  - [PyImageSearch U-Net Tutorial](https://pyimagesearch.com/2019/08/12/u-net-image-segmentation-in-keras/)\n",
    "- **Object Detection with Faster R-CNN**:\n",
    "  - [PyTorch Faster R-CNN Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n",
    "These resources and activities are designed to provide a comprehensive understanding of object detection and segmentation, supported by practical implementations and real-world applications.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources from Alex\n",
    "\n",
    "Stuff on word embeddings like word2vec\n",
    "https://youtu.be/gQddtTdmG_8?si=4_L6tXrFiO5y7QXG\n",
    "https://youtu.be/f7o8aDNxf7k?si=rmfwG4GrM_FSM1zi\n",
    "\n",
    "* [Transformers, explained: Understand the model behind GPT, BERT, and T5](https://youtu.be/SZorAJ4I-sA?si=WSvlE5EDSM0lcSO9) - Light video intro to Transformers from Google Cloud Tech\n",
    "* **Videos from Umar Jamil**\n",
    "    * [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://youtu.be/bCz4OMemCcA?si=O0JG_SIfLDCIAZ6I)\n",
    "    * [BERT explained: Training, Inference, BERT vs GPT/LLamA, Fine tuning, CLS token](https://youtu.be/90mGPxR2GgY?si=zVvGBy3yPWm6VJ2N)\n",
    "* [LLM Visualization](https://bbycroft.net/llm) Looks cool, but no audio?\n",
    "* [EXBERT - Interactive BERT Visualizer](https://huggingface.co/spaces/exbert-project/exbert)\n",
    "* [Paper - EXBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models](https://arxiv.org/pdf/1910.05276)\n",
    "\n",
    "* [Video - Transformer models and BERT model: Overview](https://youtu.be/t45S_MwAcOw?si=jHgwRAEKZDsOW3xi) A solid explanation of encoder / decoder, but not super detailed.  More detail than the video at the top of this list.\n",
    "* Hugging Face encoder/decode videos\n",
    "    * [Encoders](https://youtu.be/MUqNwgPjJvQ?si=e2lFDk1gjgOluStP)\n",
    "    * [Decoders](https://youtu.be/d_ixlCubqQw?si=x88NFT7Ce0MgKu0M)\n",
    "\n",
    "* [A Hacker's Guide to Language Models by Jeremy Howard](https://youtu.be/jkrNMKz9pWU?si=-gdapXOPSY5S0Ikz) Consider starting at 17:21 for weaker audiences.\n",
    "    * For the notebook used in this talk, see https://github.com/fastai/lm-hackers.\n",
    "    * 00:00:00 Introduction & Basic Ideas of Language Models\n",
    "    * 00:18:05 Limitations & Capabilities of GPT-4\n",
    "    * 00:31:28 AI Applications in Code Writing, Data Analysis & OCR\n",
    "    * 00:38:50 Practical Tips on Using OpenAI API\n",
    "    * 00:46:36 Creating a Code Interpreter with Function Calling\n",
    "    * 00:51:57 Using Local Language Models & GPU Options\n",
    "    * 00:59:33 Fine-Tuning Models & Decoding Tokens\n",
    "    * 01:05:37 Testing & Optimizing Models\n",
    "    * 01:10:32 Retrieval Augmented Generation\n",
    "    * 01:20:08 Fine-Tuning Models\n",
    "    * 01:26:00 Running Models on Macs\n",
    "    * 01:27:42 Llama.cpp & Its Cross-Platform Abilities\n",
    "\n",
    "* the 3Blue1Brown videos are superb\n",
    "    * [The 3Blue1Brown Website](https://www.3blue1brown.com/topics/neural-networks)\n",
    "    * [How large language models work, a visual intro to transformers | Chapter 5, Deep Learning](https://youtu.be/wjZofJX0v4M?si=-hwfraw0EONYBuiC)\n",
    "    * [Attention in transformers, visually explained | Chapter 6, Deep Learning](https://youtu.be/eMlx5fFNoYc?si=w5zr5VKyI70k_wcW)\n",
    "* Chris Manning at Stanford (NLP Class)\n",
    "    * [Lecture 1 | Natural Language Processing with Deep Learning](https://www.youtube.com/watch?v=OQQ-W_63UgQ&ab_channel=StanfordUniversitySchoolofEngineering) The bit around 21:00 is a great explanation of machine learning vs deep learning.\n",
    "    * [Lecture 2 | Word Vector Representations: word2vec](https://youtu.be/ERibwqs9p38?si=YbYUWjUf1Rf0U7Qz) Probably too high level.\n",
    "* [Mirella Lapata Turing Lecture about Generative AI](https://youtu.be/fwaDtRbfioU?si=W_UxNiB5ZMGWRbHc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
