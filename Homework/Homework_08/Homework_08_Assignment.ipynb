{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR IMPORTS HERE ===\n",
    "# Add any additional imports you need below this line\n",
    "\n",
    "from introdl import (\n",
    "    config_paths_keys,\n",
    "    get_device,\n",
    "    wrap_print_text\n",
    ")\n",
    "\n",
    "# Wrap print to format text nicely at 120 characters\n",
    "print = wrap_print_text(print, width=120)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Configure paths\n",
    "paths = config_paths_keys()\n",
    "DATA_PATH = paths['DATA_PATH']\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "# === END YOUR IMPORTS ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Homework 8: Sarcasm Detection\n\n**Name:** [Your Name Here]  \n**Total Points: 40**\n\n## Submission Checklist\n- [ ] All code cells executed with output saved\n- [ ] All questions answered in markdown cells\n- [ ] Used `DATA_PATH` and `MODELS_PATH` variables (no hardcoded paths)\n- [ ] Comparison table completed\n- [ ] Reflection questions answered\n- [ ] Notebook exported to HTML\n- [ ] Canvas filename includes `_GRADE_THIS_ONE`\n- [ ] Files uploaded to Canvas\n\n---\n\nUsing the [\"Sarcasm_News_Headline\" dataset](https://huggingface.co/datasets/raquiba/Sarcasm_News_Headline) on HuggingFace you're going to try several approaches to sarcasm detection (text classification) and write a summary at the end.\n\n**Point Breakdown:**\n- Download and split dataset: 2 points\n- Approach 1 (TF-IDF + ML Model): 5 points\n- Approach 2 (Pretrained Model): 6 points\n- Approach 3 (Fine-tune DistilBERT): 7 points\n- Approach 4 Part 1 (LLM Zero-Shot): 7 points\n- Approach 4 Part 2 (LLM Few-Shot): 7 points\n- Summarize and Compare: 4 points\n- Reflection: 2 points"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and split the dataset (2 points)\n",
    "\n",
    "While the dataset has \"train\" and \"test\" splits, ignore the \"test\" split since it almost entirely duplicates the \"train\" split.\n",
    "\n",
    "Instead, use train_test_split with a seed of 42 to generate an 80/20 split of the original \"train\" split into training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pcj64ey2fv",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_08_Models/` | `Homework_08_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` ‚Äî never hardcode paths like `'Homework_08_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Sarcasm News Headline dataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load the dataset using load_dataset(\"raquiba/Sarcasm_News_Headline\")\n",
    "# 2. Get the 'train' split (ignore 'test' as it duplicates 'train')\n",
    "# 3. Extract headlines and labels into lists or arrays\n",
    "# 4. Use train_test_split with test_size=0.2 and random_state=42\n",
    "# 5. Print the sizes of train and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Apply Approach 1 - TF-IDF Vectors + ML Model (5 points)\n\nInclude code to create TF-IDF Vectors that represent each headline. Use these vectors to train a classification model (it doesn't have to be Logistic Regression). Make predictions on the test set and generate a classification report."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create a TfidfVectorizer (consider max_features parameter)\n",
    "# 2. Fit on training headlines and transform both train and test\n",
    "# 3. Print the shape of the resulting vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train a classification model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Or try: from sklearn.ensemble import RandomForestClassifier\n",
    "# Or try: from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create your classifier (e.g., LogisticRegression)\n",
    "# 2. Train on the TF-IDF vectors and labels\n",
    "# 3. Make predictions on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluate with classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Your code here:\n",
    "# 1. Generate classification report comparing predictions to test labels\n",
    "# 2. Print the report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 2 - Use a Pretrained Model from HuggingFace (6 points)\n",
    "\n",
    "Before fine-tuning your own model, search HuggingFace Hub for existing models trained on sarcasm detection.\n",
    "\n",
    "**Task:**\n",
    "1. Find a pretrained model for sarcasm detection on HuggingFace (search for \"sarcasm\")\n",
    "2. Use the `pipeline()` API to load the model\n",
    "3. Make predictions on your test set\n",
    "4. Generate a classification report\n",
    "5. You may have to experiment a little, but find a model that achieves at least 92% accuracy on the first 100 texts in the test set.\n",
    "\n",
    "**Hints:**\n",
    "- Search HuggingFace for \"sarcasm detection\" or \"sarcasm classification\"\n",
    "- Look for models with high downloads or recent updates\n",
    "- Check the model card to verify it matches your task (news headlines vs tweets)\n",
    "- Try searching for 'sarcasm detector' or 'sarcasm detection'. \n",
    "\n",
    "**Example code structure:**\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pretrained model\n",
    "classifier = pipeline(\"text-classification\", model=\"model-name-here\")\n",
    "\n",
    "# Make predictions\n",
    "# predictions = ...\n",
    "\n",
    "# Generate classification report\n",
    "```\n",
    "\n",
    "**What to report:**\n",
    "- Which model did you choose and why?\n",
    "- What accuracy does it achieve on your test set?\n",
    "- How does it compare to your TF-IDF approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Search and load a pretrained sarcasm detection model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Your code here:\n",
    "# 1. Search HuggingFace for \"sarcasm detection\" models\n",
    "# 2. Choose a model (e.g., \"helinivan/english-sarcasm-detector\")\n",
    "# 3. Load it using: classifier = pipeline(\"text-classification\", model=\"...\")\n",
    "# 4. Test on 1-2 examples to verify it works\n",
    "\n",
    "# Example:\n",
    "# classifier = pipeline(\"text-classification\", model=\"your-model-name\")\n",
    "# test = classifier([\"This is a test headline.\"])\n",
    "# print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Make predictions on test set\n",
    "\n",
    "# Your code here:\n",
    "# 1. Use the pipeline to predict on all test headlines\n",
    "# 2. Extract the predicted labels (may need to map model's labels to 0/1)\n",
    "# 3. Handle batch processing if needed (pipeline can take lists)\n",
    "\n",
    "# Hint: The model might return labels like \"SARCASM\" and \"NOT_SARCASM\"\n",
    "# You may need to map these to 0 and 1 to match your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Your code here:\n",
    "# 1. Generate classification report\n",
    "# 2. Print the report\n",
    "# 3. Note the model name and accuracy for comparison later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 3 - Fine-tune DistilBERT with Classification Head (7 points)\n",
    "\n",
    "Include code for set up, training, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare dataset for HuggingFace\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create DataFrames or dictionaries with 'text' and 'label' columns\n",
    "# 2. Convert to HuggingFace Dataset format using Dataset.from_pandas() or Dataset.from_dict()\n",
    "# 3. Create a DatasetDict with 'train' and 'test' splits\n",
    "\n",
    "# Example structure:\n",
    "# train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n",
    "# test_df = pd.DataFrame({'text': test_texts, 'label': test_labels})\n",
    "# train_dataset = Dataset.from_pandas(train_df)\n",
    "# test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load tokenizer and tokenize dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load DistilBERT tokenizer: AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# 2. Create a tokenize function that tokenizes the 'text' field\n",
    "# 3. Use dataset.map() to tokenize both train and test datasets\n",
    "# 4. Set truncation=True and padding=True\n",
    "\n",
    "# Example:\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "# \n",
    "# tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "# tokenized_test = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load model and set up training\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\n# Your code here:\n# 1. Load DistilBERT model: AutoModelForSequenceClassification.from_pretrained(\n#       \"distilbert-base-uncased\", num_labels=2)\n# 2. Create TrainingArguments with:\n#    - output_dir for saving checkpoints (use MODELS_PATH / 'distilbert_sarcasm')\n#    - num_train_epochs (2-3 is typical)\n#    - per_device_train_batch_size (8 or 16)\n#    - evaluation_strategy=\"epoch\"\n#    - save_total_limit=1  # IMPORTANT: Only keep best checkpoint to save storage!\n#    - load_best_model_at_end=True\n# 3. Create Trainer with model, args, train_dataset, eval_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n",
    "\n",
    "# Your code here:\n",
    "# 1. Call trainer.train()\n",
    "# 2. This will take several minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make predictions and evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Your code here:\n",
    "# 1. Use trainer.predict() on the test dataset\n",
    "# 2. Extract predictions: np.argmax(predictions.predictions, axis=1)\n",
    "# 3. Generate classification report\n",
    "# 4. Print the report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 4 - Part 1: Use an LLM Model and Zero-Shot Prompt (7 points)\n",
    "\n",
    "Using the `llm_classifier` helper function from the lesson apply your LLM classifier to the first 100 examples in the test set. Use a local model and an API-based model for comparison. For the API-based model some possibilities include:\n",
    "* Groq: \"llama3-70b-8192\", (rate_limit = 30 requests per minute on free tier)\n",
    "* Together.AI: \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\" (rate limit = 10)\n",
    "* Gemini: \"gemini-flash-lite\" (rate_limit = 30 on free tier)\n",
    "\n",
    "Feel free to try others if you have access.\n",
    "\n",
    "Produce classification reports for both the local and best API-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the llm_classifier helper\n",
    "from Lesson_08_Helpers import llm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare test subset (first 100 examples)\n",
    "\n",
    "# Your code here:\n",
    "# 1. Select first 100 test headlines\n",
    "# 2. Get corresponding labels\n",
    "# 3. Print a few examples to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create zero-shot prompts for sarcasm detection\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create a system_prompt (e.g., \"You are an expert at detecting sarcasm in news headlines.\")\n",
    "# 2. Create a prompt_template with:\n",
    "#    - Clear instructions to classify as 'sarcastic' or 'not sarcastic'\n",
    "#    - A placeholder {text} for the headline\n",
    "#    - Request to output ONLY the label\n",
    "\n",
    "# Example structure:\n",
    "# system_prompt = \"...\"\n",
    "# prompt_template = \"\"\"\n",
    "# Classify the following news headline as either 'sarcastic' or 'not sarcastic'.\n",
    "# Output ONLY one of: sarcastic, not sarcastic\n",
    "# \n",
    "# Headline: {text}\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use API-based model (e.g., gemini-flash-lite)\n",
    "\n",
    "# Your code here:\n",
    "# 1. Call llm_classifier with:\n",
    "#    - model_name='gemini-flash-lite' (or another API model)\n",
    "#    - texts (first 100 headlines)\n",
    "#    - system_prompt\n",
    "#    - prompt_template\n",
    "#    - estimate_cost=True to see API costs\n",
    "# 2. Store predictions\n",
    "\n",
    "# Example:\n",
    "# predictions_api = llm_classifier(\n",
    "#     'gemini-flash-lite',\n",
    "#     test_texts_100,\n",
    "#     system_prompt,\n",
    "#     prompt_template,\n",
    "#     estimate_cost=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert text predictions to 0/1 labels\n",
    "\n",
    "# Your code here:\n",
    "# 1. Map text predictions to 0/1\n",
    "#    'not sarcastic' ‚Üí 0\n",
    "#    'sarcastic' ‚Üí 1\n",
    "# 2. Handle any unexpected responses (strip whitespace, lowercase, etc.)\n",
    "\n",
    "# Example:\n",
    "# predictions_api_binary = []\n",
    "# for pred in predictions_api:\n",
    "#     pred_clean = pred.strip().lower()\n",
    "#     if 'not sarcastic' in pred_clean:\n",
    "#         predictions_api_binary.append(0)\n",
    "#     elif 'sarcastic' in pred_clean:\n",
    "#         predictions_api_binary.append(1)\n",
    "#     else:\n",
    "#         predictions_api_binary.append(0)  # default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate classification report for API model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Your code here:\n",
    "# 1. Generate classification report\n",
    "# 2. Print the report\n",
    "\n",
    "print(\"API Model Results (gemini-flash-lite):\")\n",
    "# print(classification_report(...))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Try a local model (optional but recommended)\n",
    "\n",
    "# Your code here:\n",
    "# 1. Use llm_classifier with a local model\n",
    "#    Options: 'llama-3.2', or any locally hosted model\n",
    "# 2. Same system_prompt and prompt_template\n",
    "# 3. Convert predictions to 0/1\n",
    "# 4. Generate classification report\n",
    "\n",
    "# Note: Local models may be slower but are free\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 4 - Part 2: Use an LLM Model and Few-Shot Prompt (7 points)\n",
    "\n",
    "Build a few-shot prompt with three to five examples of each class and apply the same models used for the zero-shot prompt. Produce classification reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create few-shot examples\n",
    "\n",
    "# Your code here:\n",
    "# 1. Select 3-5 examples of sarcastic headlines\n",
    "# 2. Select 3-5 examples of non-sarcastic headlines\n",
    "# 3. Format them as examples in your prompt\n",
    "\n",
    "# Example structure:\n",
    "# few_shot_examples = \"\"\"\n",
    "# Examples:\n",
    "# \n",
    "# Headline: \"Local man thinks he's qualified to run country after playing SimCity once\"\n",
    "# Classification: sarcastic\n",
    "# \n",
    "# Headline: \"Scientists discover new species of frog in Amazon rainforest\"\n",
    "# Classification: not sarcastic\n",
    "# \n",
    "# [Add more examples...]\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create few-shot prompt template\n",
    "\n",
    "# Your code here:\n",
    "# 1. Update prompt_template to include the few-shot examples\n",
    "# 2. Keep the same classification instructions\n",
    "# 3. Add the {text} placeholder for new headlines\n",
    "\n",
    "# Example:\n",
    "# prompt_template_fewshot = few_shot_examples + \"\"\"\n",
    "# Now classify this headline:\n",
    "# \n",
    "# Headline: {text}\n",
    "# Classification:\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply few-shot prompting with API model\n",
    "\n",
    "# Your code here:\n",
    "# 1. Use llm_classifier with the same API model\n",
    "# 2. Use the new few-shot prompt template\n",
    "# 3. Same 100 test headlines\n",
    "# 4. Convert predictions to 0/1\n",
    "# 5. Generate classification report\n",
    "\n",
    "# predictions_fewshot_api = llm_classifier(\n",
    "#     'gemini-flash-lite',\n",
    "#     test_texts_100,\n",
    "#     system_prompt,\n",
    "#     prompt_template_fewshot,\n",
    "#     estimate_cost=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply few-shot prompting with second model\n",
    "\n",
    "# Your code here:\n",
    "# 1. Try the same few-shot approach with your second model\n",
    "# 2. Generate classification report\n",
    "# 3. Compare zero-shot vs few-shot performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize and Compare All Approaches (4 points)\n",
    "\n",
    "Create a summary table comparing all four approaches on the following dimensions:\n",
    "\n",
    "| Approach | Model/Method | Accuracy | Training Time | Inference Speed | Key Advantages | Key Disadvantages |\n",
    "|----------|--------------|----------|---------------|-----------------|----------------|-------------------|\n",
    "| 1. TF-IDF + ML | ... | ...% | ... | ... | ... | ... |\n",
    "| 2. Pretrained | ... | ...% | ... | ... | ... | ... |\n",
    "| 3. Fine-tuned DistilBERT | ... | ...% | ... | ... | ... | ... |\n",
    "| 4a. LLM Zero-shot | ... | ...% | ... | ... | ... | ... |\n",
    "| 4b. LLM Few-shot | ... | ...% | ... | ... | ... | ... |\n",
    "\n",
    "**Discussion Questions:**\n",
    "\n",
    "1. Which approach performed best? Why do you think this is?\n",
    "2. Compare Approach 2 (pretrained) vs Approach 3 (fine-tuned). Which was easier? Which performed better? When would you choose one over the other?\n",
    "3. How did few-shot prompting (4b) compare to zero-shot (4a)? Was the improvement worth the extra effort?\n",
    "4. If you were deploying a sarcasm detection system in production, which approach would you choose and why? Consider accuracy, speed, cost, and maintainability.\n",
    "\n",
    "üìù **YOUR ANSWERS HERE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create a DataFrame with columns:\n",
    "#    - Approach (name)\n",
    "#    - Model/Method\n",
    "#    - Accuracy\n",
    "#    - Training Time (approximate)\n",
    "#    - Inference Speed (Fast/Medium/Slow)\n",
    "# 2. Fill in your results from each approach\n",
    "# 3. Display the table\n",
    "\n",
    "# Example structure:\n",
    "# results = {\n",
    "#     'Approach': ['1. TF-IDF + ML', '2. Pretrained', '3. Fine-tuned DistilBERT', \n",
    "#                  '4a. LLM Zero-shot', '4b. LLM Few-shot'],\n",
    "#     'Model/Method': ['Logistic Regression', '...', 'distilbert-base-uncased', \n",
    "#                      'gemini-flash-lite', 'gemini-flash-lite'],\n",
    "#     'Accuracy': [0.XX, 0.XX, 0.XX, 0.XX, 0.XX],\n",
    "#     'Training Time': ['< 1 min', '0 (pretrained)', '5-10 min', '0', '0'],\n",
    "#     'Inference Speed': ['Very Fast', 'Fast', 'Fast', 'Slow (API)', 'Slow (API)']\n",
    "# }\n",
    "# \n",
    "# df_results = pd.DataFrame(results)\n",
    "# display(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection (2 points)\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for the lesson? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}