{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 03: HuggingFace Downloads\n",
    "\n",
    "**Purpose:** Actually download HuggingFace models and datasets, then verify they went to the correct locations.\n",
    "\n",
    "**What we'll download:**\n",
    "1. A small transformer model (distilbert-base-uncased)\n",
    "2. A small dataset (imdb, just first 100 samples)\n",
    "\n",
    "**Expected Locations:**\n",
    "- **CoCalc Home:** `~/home_workspace/downloads/huggingface/` and `~/home_workspace/data/`\n",
    "- **Compute Server:** `~/cs_workspace/downloads/huggingface/` and `~/cs_workspace/data/`\n",
    "- **NOT:** `~/.cache/huggingface/`\n",
    "\n",
    "**Run this on:** Both CoCalc base and Compute Server\n",
    "\n",
    "---\n",
    "\n",
    "## IMPORTANT: Restart kernel before running this test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-download check: What's in ~/.cache?\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "home = Path.home()\n",
    "bad_cache = home / '.cache' / 'huggingface'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-DOWNLOAD CHECK: ~/.cache/huggingface\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if bad_cache.exists():\n",
    "    # Get total size\n",
    "    total_size = sum(f.stat().st_size for f in bad_cache.rglob('*') if f.is_file())\n",
    "    print(f\"WARNING: ~/.cache/huggingface exists ({total_size / 1024 / 1024:.1f} MB)\")\n",
    "    print(\"Contents:\")\n",
    "    for item in bad_cache.iterdir():\n",
    "        if item.is_dir():\n",
    "            size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())\n",
    "            print(f\"  {item.name}/: {size / 1024 / 1024:.1f} MB\")\n",
    "    print(\"\\nNote: This is pre-existing content, not from this test.\")\n",
    "else:\n",
    "    print(\"Good: ~/.cache/huggingface does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check expected cache locations\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPECTED CACHE LOCATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "hf_home = os.environ.get('HF_HOME', 'NOT SET')\n",
    "hf_datasets = os.environ.get('HF_DATASETS_CACHE', 'NOT SET')\n",
    "\n",
    "print(f\"HF_HOME: {hf_home}\")\n",
    "print(f\"HF_DATASETS_CACHE: {hf_datasets}\")\n",
    "\n",
    "# Create directories if needed\n",
    "if hf_home != 'NOT SET':\n",
    "    Path(hf_home).mkdir(parents=True, exist_ok=True)\n",
    "if hf_datasets != 'NOT SET':\n",
    "    Path(hf_datasets).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test A: Download a Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "print(f\"\\nDownloading model: {model_name}\")\n",
    "print(\"(This may take a minute on first run...)\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer downloaded successfully\")\n",
    "\n",
    "# Just download config, not full model (faster)\n",
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "print(f\"Config downloaded successfully\")\n",
    "print(f\"Model type: {config.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model download location\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL DOWNLOAD VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "home = Path.home()\n",
    "\n",
    "# Check expected location\n",
    "hf_home = Path(os.environ.get('HF_HOME', ''))\n",
    "expected_hub = hf_home / 'hub'\n",
    "\n",
    "if expected_hub.exists():\n",
    "    print(f\"\\nCorrect location ({expected_hub}):\")\n",
    "    for item in expected_hub.iterdir():\n",
    "        if 'distilbert' in item.name.lower():\n",
    "            size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())\n",
    "            print(f\"  FOUND: {item.name} ({size / 1024 / 1024:.1f} MB)\")\n",
    "\n",
    "# Check bad location\n",
    "bad_hub = home / '.cache' / 'huggingface' / 'hub'\n",
    "if bad_hub.exists():\n",
    "    new_distilbert = [d for d in bad_hub.iterdir() if 'distilbert' in d.name.lower()]\n",
    "    if new_distilbert:\n",
    "        print(f\"\\nWARNING: Model found in ~/.cache/huggingface/hub!\")\n",
    "        for d in new_distilbert:\n",
    "            print(f\"  {d.name}\")\n",
    "    else:\n",
    "        print(f\"\\nGood: No new distilbert in ~/.cache/huggingface/hub\")\n",
    "else:\n",
    "    print(f\"\\nGood: ~/.cache/huggingface/hub does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test B: Download a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small portion of a dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"\\nDownloading IMDB dataset (first 100 samples)...\")\n",
    "print(\"(This may take a minute on first run...)\\n\")\n",
    "\n",
    "# Just download a tiny slice\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:100]\")\n",
    "print(f\"Dataset downloaded successfully\")\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset download location\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET DOWNLOAD VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "home = Path.home()\n",
    "\n",
    "# Check expected location\n",
    "hf_datasets = Path(os.environ.get('HF_DATASETS_CACHE', ''))\n",
    "\n",
    "if hf_datasets.exists():\n",
    "    print(f\"\\nCorrect location ({hf_datasets}):\")\n",
    "    # Look for imdb or downloads folder\n",
    "    found = False\n",
    "    for item in hf_datasets.rglob('*'):\n",
    "        if 'imdb' in item.name.lower() and item.is_dir():\n",
    "            size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())\n",
    "            print(f\"  FOUND: {item.relative_to(hf_datasets)} ({size / 1024 / 1024:.1f} MB)\")\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        # List top-level items\n",
    "        for item in hf_datasets.iterdir():\n",
    "            if item.is_dir():\n",
    "                size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())\n",
    "                print(f\"  {item.name}/: {size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Check bad location\n",
    "bad_datasets = home / '.cache' / 'huggingface' / 'datasets'\n",
    "if bad_datasets.exists():\n",
    "    imdb_in_bad = list(bad_datasets.rglob('*imdb*'))\n",
    "    if imdb_in_bad:\n",
    "        print(f\"\\nWARNING: IMDB found in ~/.cache/huggingface/datasets!\")\n",
    "        for d in imdb_in_bad[:3]:  # Show first 3\n",
    "            print(f\"  {d}\")\n",
    "    else:\n",
    "        print(f\"\\nGood: No IMDB in ~/.cache/huggingface/datasets\")\n",
    "else:\n",
    "    print(f\"\\nGood: ~/.cache/huggingface/datasets does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL SUMMARY: HuggingFace Downloads\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "home = Path.home()\n",
    "\n",
    "# Check correct locations\n",
    "hf_home = Path(os.environ.get('HF_HOME', ''))\n",
    "hf_datasets = Path(os.environ.get('HF_DATASETS_CACHE', ''))\n",
    "\n",
    "print(\"\\nCorrect Locations:\")\n",
    "if hf_home.exists():\n",
    "    size = sum(f.stat().st_size for f in hf_home.rglob('*') if f.is_file())\n",
    "    print(f\"  HF_HOME: {hf_home} ({size / 1024 / 1024:.1f} MB)\")\n",
    "if hf_datasets.exists():\n",
    "    size = sum(f.stat().st_size for f in hf_datasets.rglob('*') if f.is_file())\n",
    "    print(f\"  HF_DATASETS_CACHE: {hf_datasets} ({size / 1024 / 1024:.1f} MB)\")\n",
    "\n",
    "# Check bad location\n",
    "bad_cache = home / '.cache' / 'huggingface'\n",
    "print(\"\\nBad Location (~/.cache/huggingface):\")\n",
    "if bad_cache.exists():\n",
    "    size = sum(f.stat().st_size for f in bad_cache.rglob('*') if f.is_file())\n",
    "    print(f\"  EXISTS with {size / 1024 / 1024:.1f} MB\")\n",
    "    print(\"  (May be pre-existing content, check timestamps)\")\n",
    "else:\n",
    "    print(\"  Does not exist - PERFECT!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"If downloads went to correct locations, the fix is working!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Test_04:** Test torchvision model downloads\n",
    "- **Test_05:** Full verification of all cache locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
