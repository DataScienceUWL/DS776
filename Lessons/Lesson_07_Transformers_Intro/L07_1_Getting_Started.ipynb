{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ introdl v1.6.21 already up to date\n"
     ]
    }
   ],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "### DO THIS FIRST\n",
    "\n",
    "Change `force_update=True` in the last line and run the next cell to install an updated course package.  Once it's done restart your kernel and change back to `force_update=False`.  You only need to do this once per server (not once per notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "#### L07_1_Getting_Started_with_NLP Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/oDi5d1FbYBx\" target=\"_blank\">Open Descript version of video in new tab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## A Tiny History of Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) has evolved significantly over the past few decades. Initially, NLP relied heavily on rule-based systems and statistical methods to understand and generate human language. These early approaches, prominent in the 1980s and 1990s, focused on the syntactic structure of text, using techniques such as n-grams and Hidden Markov Models (HMMs) to model language. However, these methods struggled with capturing the semantic meaning and context of words.\n",
    "\n",
    "The introduction of word embeddings in the early 2010s, such as Word2Vec and GloVe, marked a significant advancement in NLP. These embeddings allowed for the representation of words in continuous vector space, capturing semantic relationships between words. This shift enabled more sophisticated models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, to process sequences of text and maintain context over longer passages. RNNs, in particular, played a crucial role in tasks like language translation and sentiment analysis.\n",
    "\n",
    "The advent of transformers in 2017 revolutionized NLP by addressing the limitations of RNNs. Transformers, introduced with the Attention is All You Need paper, utilize self-attention mechanisms to process entire sequences of text simultaneously, allowing for better handling of long-range dependencies and parallelization. This led to the development of powerful models like BERT, GPT, and T5, which have set new benchmarks in various NLP tasks by providing a deeper semantic understanding of text.\n",
    "\n",
    "Transformers have almost entirely supplanted previous approaches to NLP because:\n",
    "\n",
    "1. **Superior Performance:** Models like BERT, GPT, T5, and their successors dominate leaderboards on tasks such as text classification, translation, summarization, and question answering.\n",
    "2. **Pretraining and Transfer Learning:** Unlike traditional methods that required training separate models from scratch for different tasks, transformers leverage large-scale pretraining on vast text corpora and fine-tune efficiently on specific tasks.\n",
    "3. **Self-Attention and Contextual Representations:** Transformers provide rich, context-dependent word representations, whereas earlier models like Word2Vec and GloVe generated static embeddings.\n",
    "4. **Scalability and Adaptability:** With advancements in scaling laws, models can achieve better performance just by increasing their size and training data, an advantage that RNNs and classical machine learning approaches lacked.\n",
    "\n",
    "There are a few areas where older approaches still exist:\n",
    "\n",
    "1. **Small Datasets & Low Compute Environments:** Logistic regression, SVMs, and Lasso-penalized models often remain competitive when data is limited or when computational efficiency is a concern.\n",
    "2. **Domain-Specific Applications:** Some applications, like biomedical text mining, may still rely on domain-specific feature engineering approaches alongside transformers.\n",
    "3. **Traditional ML for Interpretability:** Some NLP applications in finance, healthcare, and legal fields still favor older methods due to the need for interpretability and robustness.\n",
    "\n",
    "However, since transformer models for NLP are now so dominant we will focus exclusively on them in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## NLP Tasks Instead of Transformer Details\n",
    "\n",
    "Transformers are more complicated than the CNNs we saw for computer vision so we're not going to dive as deeply into the details. We will, in Lesson 9 - Transformer Details, learn about some of the nuts and bolts especially the self-attention mechanism that allows transformers to figure out relationships between words and to understand context. Mostly, though, we will focus on the applications of transformers. To this end we'll dive into the open source HuggingFace ecosystem which hosts thousands of NLP models and datasets and makes it quite simple to dive into NLP applications without having to master too much code. All of the newest, biggest open source transformer models are hosted there including those from Meta, Mistral, and Deepseek. The only thing keeping us from running the biggest state-of-the-art models will be lack of compute, but we can run their smaller cousins on the GPU in CoCalc's compute server, a decent gaming GPU, or even a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## API-based LLMs versus Fine-tuning Specialized Models\n",
    "\n",
    "As large language models (LLMs) continue to improve, their use as general NLP task solvers via prompting is increasingly popular, especially when we don't have access to large amounts of training data. In this course, we'll focus on two main approaches to solving NLP tasks:\n",
    "\n",
    "1. **Using LLMs via APIs** (like GPT-4o, Claude, or Gemini through OpenRouter)\n",
    "2. **Fine-tuning specialized transformer models** for specific tasks\n",
    "\n",
    "*(We'll also explore running LLMs locally in Lesson 11, but for Lessons 7-10 and 12 we'll use API-based models and task-specific fine-tuned models.)*\n",
    "\n",
    "### Example: Text Classification\n",
    "\n",
    "For a text-classification task, you could choose:\n",
    "\n",
    "**LLM via API (GPT-4o, Claude, Gemini, etc.)**\n",
    "- When you need **a quick, general-purpose classifier** without training a model\n",
    "- When **zero-shot or few-shot classification** (via prompting) is sufficient\n",
    "- When categories may evolve frequently, making retraining impractical\n",
    "- When you don't have a large labeled dataset\n",
    "- Example: Categorizing support tickets by topic\n",
    "\n",
    "**Fine-tune BERT / RoBERTa / DistilBERT**\n",
    "- When you have a **moderate to large labeled dataset** and need **high accuracy**\n",
    "- When you need **fast inference at scale**, as fine-tuned models are more efficient than large LLMs\n",
    "- When your classification task requires **domain-specific adaptation**\n",
    "- When you need **very low latency** or **predictable costs**\n",
    "- Example: Sentiment analysis on customer feedback in a specific industry\n",
    "\n",
    "**Note on terminology:** Zero-shot classification means classifying text without seeing any examples - the LLM just gets a prompt with the possible categories. Few-shot classification means providing a small number of examples in the LLM prompt to guide the model's behavior.\n",
    "\n",
    "### Choosing the Right Approach\n",
    "\n",
    "**Use API-based LLMs when:**\n",
    "- You need **quick, adaptable solutions** without training infrastructure\n",
    "- You **don't have much labeled data** for fine-tuning\n",
    "- You want to **experiment rapidly** with different task formulations\n",
    "- Task requirements may change frequently\n",
    "- You're prototyping or building proof-of-concepts\n",
    "\n",
    "**Fine-tune a specialized model when:**\n",
    "- You have **domain-specific labeled data** and need **high accuracy**\n",
    "- You need **very fast inference** or processing at large scale\n",
    "- You need **predictable costs** (no per-token API charges)\n",
    "- You require **consistent, structured outputs**\n",
    "- Latency is critical (milliseconds matter)\n",
    "\n",
    "### Understanding Data Privacy with API-based LLMs\n",
    "\n",
    "A common concern with API-based LLMs is: **\"Will my data be used to train the model?\"** or **\"Is my sensitive data secure?\"** The answer depends on the provider and the agreements in place.\n",
    "\n",
    "**Privacy Protections Available:**\n",
    "\n",
    "Most major LLM providers now offer enterprise-grade privacy protections:\n",
    "- **Zero Data Retention (ZDR):** Your API requests are not stored or logged after processing\n",
    "- **Data Processing Agreements (DPAs):** Legal contracts preventing use of your data for model training\n",
    "- **HIPAA and SOC 2 Compliance:** Meeting healthcare and security standards for regulated industries\n",
    "- **Private Deployments:** Dedicated instances in your own cloud environment (e.g., Azure OpenAI, AWS Bedrock)\n",
    "- **Regional Data Residency:** Keep data within specific geographic boundaries (e.g., EU-only processing)\n",
    "\n",
    "**Examples:**\n",
    "- **OpenAI API:** Has a default policy not to use API data for training. Enterprise customers can enable additional protections.\n",
    "- **Azure OpenAI Service:** Fully isolated deployments in your Azure subscription with complete data control\n",
    "- **Google Vertex AI:** Private endpoints with data residency controls and enterprise security\n",
    "- **Anthropic Claude:** API data not used for training; enterprise options for additional controls\n",
    "\n",
    "**When API Privacy May Not Be Enough:**\n",
    "\n",
    "Even with these protections, there are situations where API-based solutions may not be acceptable:\n",
    "- **Air-gapped environments:** Systems physically isolated from external networks (e.g., classified government systems)\n",
    "- **Extreme regulatory restrictions:** Some industries may prohibit any external data transmission regardless of agreements\n",
    "- **Zero-trust requirements:** Organizations that cannot accept any third-party processing, even contractually protected\n",
    "- **Competitive intelligence:** Proprietary algorithms or trade secrets that cannot be exposed, even with DPAs\n",
    "\n",
    "In these cases, running models locally (Lesson 11) or fine-tuning your own specialized models on internal infrastructure becomes necessary.\n",
    "\n",
    "**Bottom Line:** For most educational, research, and business applications, modern API providers offer sufficient privacy protections through contractual agreements and technical controls. Understanding your specific regulatory requirements and risk tolerance will guide your choice.\n",
    "\n",
    "### Course Approach (with changes for Fall 2025)\n",
    "\n",
    "For each NLP task in Lessons 7-10 and 12, we'll explore both API-based LLM approaches and fine-tuned specialized models. In Lesson 11, we'll dive deeper into text generation and demonstrate running LLMs locally for complete control and privacy.\n",
    "\n",
    "*You may notice some slight differences between the lesson notebooks and some of the recorded videos in the NLP portion of the class.  When the course was developed we ran very small LLMs locally on the compute servers in CoCalc.  It was slow and we were limited to very small models.  This semester we're providing some credits on OpenRouter to allow you to use various models (even the biggest, newest ones ... but you may need to pay for more credits).  We'll explain more about OpenRouter further below. First we want to remind you to get a Hugging Face token.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mve92ib1jhc",
   "metadata": {},
   "source": [
    "## Hugging Face Token Setup\n",
    "\n",
    "Before we get into using LLMs via APIs, you'll need to set up access to **Hugging Face**, which hosts thousands of transformer models and datasets. Many models and datasets require authentication to download, so you'll need a free Hugging Face token.\n",
    "\n",
    "### Why You Need a Hugging Face Token\n",
    "\n",
    "Hugging Face requires authentication for:\n",
    "- **Gated models** - Popular models like Meta's Llama require accepting terms before downloading\n",
    "- **Datasets** - Some datasets have usage agreements or privacy controls\n",
    "- **Upload/sharing** - If you want to share your fine-tuned models\n",
    "- **Rate limiting** - Authenticated users get higher rate limits for downloads\n",
    "\n",
    "The token is **completely free** - you just need to create an account.\n",
    "\n",
    "### Step 1: Create a Hugging Face Account\n",
    "\n",
    "1. Go to [https://huggingface.co](https://huggingface.co)\n",
    "2. Click \"Sign Up\" in the top right corner\n",
    "3. Create your account (you can use your university email or personal email)\n",
    "\n",
    "### Step 2: Generate an Access Token\n",
    "\n",
    "1. Once logged in, click your profile picture in the top right\n",
    "2. Select **\"Access Tokens\"** from the dropdown menu.\n",
    "3. Click **\"Create new token\"** button \n",
    "4. Choose token-type **\"Read\"**\n",
    "5. Give your token a name (e.g., \"DS776 Course Token\")\n",
    "7. Click **\"Create token\"**\n",
    "8. **Copy the token** - you won't be able to see it again (but you could create another token)\n",
    "\n",
    "### Step 3: Add Token to Your `api_keys.env` File\n",
    "\n",
    "Your `api_keys.env` file is located at `~/home_workspace/api_keys.env`. Open this file and add a new line:\n",
    "\n",
    "```\n",
    "HF_TOKEN=hf_YourTokenHere\n",
    "```\n",
    "\n",
    "Replace `hf_YourTokenHere` with the token you copied from Hugging Face.\n",
    "\n",
    "**Important Notes:**\n",
    "- The token format starts with `hf_` followed by random characters\n",
    "- Don't include quotes or spaces around the token\n",
    "- Save the file after adding the token\n",
    "- Never commit this file to git or share it publicly\n",
    "\n",
    "### Step 4: Verify It Works\n",
    "\n",
    "When you run `config_paths_keys()` (in the import cell below), you should see:\n",
    "\n",
    "```\n",
    "✅ HuggingFace Hub: Logged in\n",
    "```\n",
    "\n",
    "This confirms your token was loaded successfully and you can now access gated models and datasets.\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "If you see `❌ HuggingFace Hub: Not logged in`, check:\n",
    "- Did you save the `api_keys.env` file after adding the token?\n",
    "- Is the token on a new line with format `HF_TOKEN=hf_...`?\n",
    "- Did you copy the complete token (starts with `hf_`)?\n",
    "- Try restarting your kernel and running the import cell again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## OpenRouter API and Your Course API Keys\n",
    "\n",
    "For this course, we've set up access to LLMs through **OpenRouter**, a unified API that provides access to all major commercial models (GPT-4o, Claude, Gemini) and most open-weight models (Llama, Mistral, DeepSeek, Qwen, and many more). This means you can experiment with different models using a single API interface.\n",
    "\n",
    "### Your API Credit\n",
    "\n",
    "**Each student has been provided with $15 in OpenRouter API credit.** This should be more than sufficient to complete all coursework if you use small and medium-sized models appropriately. For reference:\n",
    "\n",
    "- **Small models** (like `gemini-flash-lite`, `llama-3.2-3b`, `gpt-4o-mini`): Very inexpensive, typically $0.075-0.15 per million input tokens\n",
    "- **Medium models** (like `gemini-flash`, `claude-haiku`): Moderate cost, good quality\n",
    "- **Premium models** (like `gpt-4o`, `claude-sonnet`, `o3-mini`): Higher cost, best quality\n",
    "\n",
    "We recommend using **`gemini-flash-lite`** as your default model for coursework - it's fast, inexpensive, and produces good results for learning tasks.\n",
    "\n",
    "If you want to experiment beyond the course assignments or try premium models, you can always purchase your own OpenRouter API key and load it with whatever credit you choose.\n",
    "\n",
    "### Checking Your Remaining Credit\n",
    "\n",
    "You can check your remaining OpenRouter credit using the `llm_get_credits()` function from the course package. This will show you how much of your $15 credit remains:\n",
    "\n",
    "```python\n",
    "from introdl.nlp import llm_get_credits\n",
    "\n",
    "credits = llm_get_credits()\n",
    "print(f\"Remaining credit: ${credits['usage']:.2f} of ${credits['limit']:.2f}\")\n",
    "print(f\"Credit remaining: ${credits['limit'] - credits['usage']:.2f}\")\n",
    "```\n",
    "\n",
    "### Your API Keys Are Already Configured\n",
    "\n",
    "Your OpenRouter API key has already been distributed to your CoCalc project and is stored in:\n",
    "```\n",
    "~/home_workspace/api_keys.env\n",
    "```\n",
    "\n",
    "When you run `config_paths_keys()` in your import cell (as shown below), this API key will be automatically loaded and available for use with `llm_generate()`. You don't need to do anything else!\n",
    "\n",
    "**Security Note:** Never commit your `api_keys.env` file to git or share it publicly. The file is stored in `home_workspace`.\n",
    "\n",
    "### Exploring Available Models\n",
    "\n",
    "The course package includes 16 carefully curated models covering a range of capabilities and price points. You can see them all with `llm_list_models()`, which we'll demonstrate shortly.\n",
    "\n",
    "**Want to try models beyond our curated list?** OpenRouter provides access to hundreds of models! You can:\n",
    "\n",
    "1. **Browse all available models** at: https://openrouter.ai/models\n",
    "2. **Use any model** by providing its full OpenRouter model ID\n",
    "\n",
    "For example, to use OpenAI's new GPT-5-nano model (not in our curated list), you would use:\n",
    "\n",
    "```python\n",
    "response = llm_generate('openai/gpt-5-nano', \"Your prompt here\")\n",
    "```\n",
    "\n",
    "The full model ID format is typically `provider/model-name` (e.g., `openai/gpt-5-nano`, `anthropic/claude-opus-4.1`, `google/gemini-2.5-pro`).\n",
    "\n",
    "**Note:** Models outside our curated list won't show pricing or metadata with `llm_list_models()`, but they'll work fine if you provide the correct model ID from the OpenRouter website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Using `llm_generate` with OpenRouter\n",
    "\n",
    "The course package provides a simple, unified interface for working with LLMs through the `llm_generate()` function. This function handles all the complexity of API calls, cost tracking, and response formatting.  (Don't worry, we'll dive into some of those details in Lesson 11.)\n",
    "\n",
    "### Setting Up and Checking Your Credit\n",
    "\n",
    "First, let's import the necessary functions, configure our environment, and check your OpenRouter credit balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment: Unknown Environment | Course root: /mnt/e/GDrive_baggett.jeff/Teaching/Classes_current/2025-2026_Fall_DS776/DS776\n",
      "   Using workspace: <DS776_ROOT_DIR>/home_workspace\n",
      "\n",
      "📂 Storage Configuration:\n",
      "   DATA_PATH: <DS776_ROOT_DIR>/home_workspace/data\n",
      "   MODELS_PATH: <DS776_ROOT_DIR>/Lessons/Lesson_07_Transformers_Intro/Lesson_07_Models (local to this notebook)\n",
      "   CACHE_PATH: <DS776_ROOT_DIR>/home_workspace/downloads\n",
      "🔑 API keys: 9 loaded from home_workspace/api_keys.env\n",
      "🔐 Available: ANTHROPIC_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY... (9 total)\n",
      "✅ HuggingFace Hub: Logged in\n",
      "✅ Loaded pricing for 330 OpenRouter models\n",
      "✅ Cost tracking initialized ($9.92 credit remaining)\n",
      "📦 introdl v1.6.21 ready\n",
      "\n",
      "OpenRouter Credit Status:\n",
      "  Total limit: $9.92\n",
      "  Used so far: $0.02\n",
      "  Remaining:   $9.91\n"
     ]
    }
   ],
   "source": [
    "from introdl import (\n",
    "    config_paths_keys, wrap_print_text,\n",
    "    llm_generate, llm_list_models, llm_get_credits,\n",
    "    display_markdown, show_session_spending\n",
    ")\n",
    "\n",
    "# Configure paths and load API keys (cost tracking initialized automatically)\n",
    "paths = config_paths_keys()\n",
    "\n",
    "# Wrap print to format text nicely at 120 characters\n",
    "print = wrap_print_text(print, width=120)\n",
    "\n",
    "# Check your OpenRouter credit balance\n",
    "credits = llm_get_credits()\n",
    "print(f\"OpenRouter Credit Status:\")\n",
    "print(f\"  Total limit: ${credits['limit']:.2f}\")\n",
    "print(f\"  Used so far: ${credits['usage']:.2f}\")\n",
    "print(f\"  Remaining:   ${credits['limit'] - credits['usage']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "Now let's try a simple text generation example. The new `llm_generate()` API is very straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "# Simple text generation\n",
    "response = llm_generate('gemini-flash-lite', \"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Using System Prompts\n",
    "\n",
    "System prompts help guide the model's behavior and tone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! Ye want to know some scurvy facts about the vast, dark sea of space, do ye? Well, buckle yer\n",
      "bootstraps, 'cause ol' Captain AI has a few treasures to share:\n",
      "\n",
      "1.  **There be more stars in the universe than grains of sand on all the beaches of Earth.** Aye, ye heard that right!\n",
      "Imagine every single sandy shore ye've ever trod upon, then multiply that by… well, a number so big it'd make a kraken\n",
      "weep. That's how many stars be out there, glintin' like lost doubloons in the cosmic abyss. Makes ye feel like a tiny\n",
      "barnacle on a colossal galleon, don't it?\n",
      "\n",
      "2.  **A day on Venus is longer than its year.** Now, this be a real head-scratcher, even for a seasoned navigator. Venus\n",
      "spins slower than a drunken sailor trying to swab the deck. It takes about 243 Earth days to do one full rotation\n",
      "(that's its \"day\"), but it only takes about 225 Earth days to zip around the Sun (that's its \"year\"). So, it's\n",
      "technically older *each day* than it is *each year*. Makes ye wonder if they ever get the calendar straight on that\n",
      "sweltering planet.\n",
      "\n",
      "3.  **Neutron stars be so dense, a single teaspoon of their material would weigh about as much as Mount Everest.**\n",
      "Imagine a star that's collapsed in on itself, squished down tighter than a mermaid's corset. That's a neutron star, me\n",
      "hearty. If ye could somehow scoop up a tiny bit of that stuff, it'd be heavier than the biggest mountain ye can imagine.\n",
      "Ye'd need a ship made of pure adamantium just to carry it, and even then, ye'd probably sink right to the bottom of the\n",
      "cosmic ocean. Don't be tryin' to bottle that, unless ye want yer ship to become a permanent part of Davy Jones' locker.\n",
      "\n",
      "There ye have it, ye landlubber! Three mind-boggling bits of spacey knowledge to ponder while ye gaze at the stars. Now,\n",
      "if ye'll excuse me, I've got some cosmic rum to plunder. Arrr!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful AI assistant who is also sarcastic and talks like a pirate.\"\n",
    "\n",
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Tell me three interesting facts about space.\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Displaying Markdown Output\n",
    "\n",
    "Many LLM responses use markdown formatting. You can display them nicely using `display_markdown()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a short bullet-point list of tips for learning machine learning:\n",
       "\n",
       "*   **Master the Fundamentals:** Solidify your understanding of linear algebra, calculus, probability, and statistics. These are the bedrock of ML.\n",
       "*   **Learn a Programming Language:** Python is the de facto standard due to its extensive libraries (NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch).\n",
       "*   **Start with Core Concepts:** Understand supervised vs. unsupervised learning, regression vs. classification, and common algorithms like linear regression, logistic regression, decision trees, and k-means.\n",
       "*   **Get Hands-On with Libraries:** Practice implementing algorithms and working with data using libraries like Scikit-learn.\n",
       "*   **Work with Real Datasets:** Apply your knowledge to publicly available datasets (Kaggle, UCI Machine Learning Repository) to gain practical experience.\n",
       "*   **Understand Evaluation Metrics:** Learn how to properly assess the performance of your models (accuracy, precision, recall, F1-score, RMSE, etc.).\n",
       "*   **Explore Deep Learning:** Once comfortable with traditional ML, dive into neural networks, CNNs, RNNs, and frameworks like TensorFlow and PyTorch.\n",
       "*   **Read and Understand Research Papers:** Stay updated with new developments and gain deeper insights by reading influential ML papers.\n",
       "*   **Join a Community:** Engage with other learners and practitioners online (forums, Discord, Stack Overflow) or in person.\n",
       "*   **Build Projects:** The best way to learn is by doing. Work on personal projects that interest you to solidify your understanding.\n",
       "*   **Be Patient and Persistent:** Machine learning has a steep learning curve. Don't get discouraged by challenges; keep practicing and learning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Write a short bullet-point list of tips for learning machine learning.\"\n",
    ")\n",
    "\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Tracking Costs\n",
    "\n",
    "You can see estimated costs for your API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 Cost: $0.000049 | Tokens: 13 in / 120 out | Model: google/gemini-2.5-flash-lite\n",
      "Here are five dad jokes for you:\n",
      "\n",
      "1.  Why don't scientists trust atoms?\n",
      "    Because they make up everything!\n",
      "\n",
      "2.  What do you call a fish with no eyes?\n",
      "    Fsh!\n",
      "\n",
      "3.  I'm reading a book about anti-gravity.\n",
      "    It's impossible to put down!\n",
      "\n",
      "4.  Did you hear about the restaurant on the moon?\n",
      "    I heard the food was good, but it had no atmosphere.\n",
      "\n",
      "5.  What's orange and sounds like a parrot?\n",
      "    A carrot!\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Tell me five dad jokes.\",\n",
    "    print_cost=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Controlling Output Length\n",
    "\n",
    "By default, the output of `llm_generate` is limited to 200 tokens.  You can control how much text the model generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Whiskers twitched, a low rumble vibrated in Bartholomew’s chest. The object of his intense focus was a monstrous, ebony beast that occupied a significant portion of the living room. It was the piano, a source of both fascination and mild irritation for Bartholomew. His human, Eleanor, would spend hours coaxing strange, sometimes beautiful, sometimes jarring sounds from its depths.\n",
       "\n",
       "Bartholomew, a sleek, black cat with eyes the color of polished emeralds, had always been an observer. He’d watched Eleanor’s fingers dance across the keys, the way her brow furrowed in concentration, the triumphant smile that bloomed when a particularly tricky passage finally flowed. He was particularly drawn to the high notes, the ones that shimmered and seemed to hang in the air like tantalizing dust motes.\n",
       "\n",
       "One afternoon, Eleanor left the piano lid ajar. Bartholomew, emboldened by her absence, leaped onto the bench. The keys, cool and smooth beneath his paws, beckoned. With a tentative step, he pressed down on a single key. A clear, resonant C note echoed through the room. Bartholomew’s ears perked. His tail gave an inquisitive flick. He tried again, deliberately stepping on another key. A dissonant E followed.\n",
       "\n",
       "This was… interesting.\n",
       "\n",
       "Over the next few weeks, Bartholomew’s secret piano lessons began. When Eleanor was out, he’d hop onto the bench. At first, it was pure accident – a clumsy paw landing on a cluster of keys, producing a cacophony that would have sent any self-respecting cat scurrying. But Bartholomew was a cat of discerning taste, and he quickly learned to associate certain paw placements with particular sounds.\n",
       "\n",
       "He started with the lower register, his paws too large to hit individual notes cleanly, but he discovered the satisfying thrum of chords. Then, he moved to the higher keys, his delicate toes capable of more precision. He’d sit, utterly absorbed, his emerald eyes scanning the keys, his head tilted as if deciphering an ancient code.\n",
       "\n",
       "Eleanor, meanwhile, began to notice peculiar things. Sometimes, she’d come home to find the piano lid slightly ajar when she was certain she’d closed it. More than once, she’d heard faint, almost melodic tinkling sounds from the living room, only to find Bartholomew asleep on the rug, looking angelic and utterly innocent. She’d chalked"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Longer response\n",
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Write a short story about a cat who learns to play the piano.\",\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Viewing Available Models\n",
    "\n",
    "We provide a curated list of models which are suitable for use in this class.  They're typically small to medium sized models that are good at following instructions.  You can see this curated list with details about each model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available OpenRouter Models:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Short Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Size",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Released",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "In/M",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Out/M",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "JSON Schema",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Open Weights",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0ac53e3a-330d-44ab-916f-8b6d449dff0c",
       "rows": [
        [
         "0",
         "claude-haiku",
         "~30-50B",
         "2024-10",
         "$0.80",
         "$4.00",
         "❌",
         "❌"
        ],
        [
         "1",
         "deepseek-v3.1",
         "37B×18E",
         "2025-08",
         "$0.20",
         "$0.80",
         "✅",
         "✅"
        ],
        [
         "2",
         "gemini-flash",
         "~20B",
         "2025-04",
         "$0.30",
         "$2.50",
         "✅",
         "❌"
        ],
        [
         "3",
         "gemini-flash-lite",
         "~5B",
         "2025-09",
         "$0.10",
         "$0.40",
         "✅",
         "❌"
        ],
        [
         "4",
         "gemma-3-12b",
         "12B",
         "2025-03",
         "$0.03",
         "$0.10",
         "✅",
         "✅"
        ],
        [
         "5",
         "gemma-3-27b",
         "27B",
         "2025-03",
         "$0.09",
         "$0.16",
         "✅",
         "✅"
        ],
        [
         "6",
         "gpt-4o-mini",
         "~8B",
         "2024-07",
         "$0.15",
         "$0.60",
         "✅",
         "❌"
        ],
        [
         "7",
         "gpt-oss-120b",
         "5.1B×23E",
         "2025-08",
         "$0.04",
         "$0.40",
         "❌",
         "✅"
        ],
        [
         "8",
         "gpt-oss-20b",
         "3.6B×6E",
         "2025-08",
         "$0.03",
         "$0.14",
         "❌",
         "✅"
        ],
        [
         "9",
         "llama-3.2-1b",
         "1B",
         "2024-09",
         "$0.01",
         "$0.01",
         "❌",
         "✅"
        ],
        [
         "10",
         "llama-3.2-3b",
         "3B",
         "2024-09",
         "$0.02",
         "$0.02",
         "❌",
         "✅"
        ],
        [
         "11",
         "llama-3.3-70b",
         "70B",
         "2024-12",
         "$0.13",
         "$0.39",
         "✅",
         "✅"
        ],
        [
         "12",
         "llama-4-maverick",
         "17B×128E",
         "2025-04",
         "$0.15",
         "$0.60",
         "✅",
         "✅"
        ],
        [
         "13",
         "mistral-medium",
         "~60-80B",
         "2025-05",
         "$0.40",
         "$2.00",
         "✅",
         "✅"
        ],
        [
         "14",
         "mistral-nemo",
         "12B",
         "2024-07",
         "$0.02",
         "$0.04",
         "✅",
         "✅"
        ],
        [
         "15",
         "qwen3-32b",
         "32B",
         "2025-04",
         "$0.05",
         "$0.20",
         "✅",
         "✅"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Short Name</th>\n",
       "      <th>Size</th>\n",
       "      <th>Released</th>\n",
       "      <th>In/M</th>\n",
       "      <th>Out/M</th>\n",
       "      <th>JSON Schema</th>\n",
       "      <th>Open Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-haiku</td>\n",
       "      <td>~30-50B</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>$0.80</td>\n",
       "      <td>$4.00</td>\n",
       "      <td>❌</td>\n",
       "      <td>❌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek-v3.1</td>\n",
       "      <td>37B×18E</td>\n",
       "      <td>2025-08</td>\n",
       "      <td>$0.20</td>\n",
       "      <td>$0.80</td>\n",
       "      <td>✅</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-flash</td>\n",
       "      <td>~20B</td>\n",
       "      <td>2025-04</td>\n",
       "      <td>$0.30</td>\n",
       "      <td>$2.50</td>\n",
       "      <td>✅</td>\n",
       "      <td>❌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini-flash-lite</td>\n",
       "      <td>~5B</td>\n",
       "      <td>2025-09</td>\n",
       "      <td>$0.10</td>\n",
       "      <td>$0.40</td>\n",
       "      <td>✅</td>\n",
       "      <td>❌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma-3-12b</td>\n",
       "      <td>12B</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>$0.03</td>\n",
       "      <td>$0.10</td>\n",
       "      <td>✅</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemma-3-27b</td>\n",
       "      <td>27B</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>$0.09</td>\n",
       "      <td>$0.16</td>\n",
       "      <td>✅</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>~8B</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>$0.15</td>\n",
       "      <td>$0.60</td>\n",
       "      <td>✅</td>\n",
       "      <td>❌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>5.1B×23E</td>\n",
       "      <td>2025-08</td>\n",
       "      <td>$0.04</td>\n",
       "      <td>$0.40</td>\n",
       "      <td>❌</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>3.6B×6E</td>\n",
       "      <td>2025-08</td>\n",
       "      <td>$0.03</td>\n",
       "      <td>$0.14</td>\n",
       "      <td>❌</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-3.2-1b</td>\n",
       "      <td>1B</td>\n",
       "      <td>2024-09</td>\n",
       "      <td>$0.01</td>\n",
       "      <td>$0.01</td>\n",
       "      <td>❌</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-3.2-3b</td>\n",
       "      <td>3B</td>\n",
       "      <td>2024-09</td>\n",
       "      <td>$0.02</td>\n",
       "      <td>$0.02</td>\n",
       "      <td>❌</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-3.3-70b</td>\n",
       "      <td>70B</td>\n",
       "      <td>2024-12</td>\n",
       "      <td>$0.13</td>\n",
       "      <td>$0.39</td>\n",
       "      <td>✅</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama-4-maverick</td>\n",
       "      <td>17B×128E</td>\n",
       "      <td>2025-04</td>\n",
       "      <td>$0.15</td>\n",
       "      <td>$0.60</td>\n",
       "      <td>✅</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>~60-80B</td>\n",
       "      <td>2025-05</td>\n",
       "      <td>$0.40</td>\n",
       "      <td>$2.00</td>\n",
       "      <td>✅</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mistral-nemo</td>\n",
       "      <td>12B</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>$0.02</td>\n",
       "      <td>$0.04</td>\n",
       "      <td>✅</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>32B</td>\n",
       "      <td>2025-04</td>\n",
       "      <td>$0.05</td>\n",
       "      <td>$0.20</td>\n",
       "      <td>✅</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Short Name      Size Released   In/M  Out/M JSON Schema  \\\n",
       "0        claude-haiku   ~30-50B  2024-10  $0.80  $4.00           ❌   \n",
       "1       deepseek-v3.1   37B×18E  2025-08  $0.20  $0.80           ✅   \n",
       "2        gemini-flash      ~20B  2025-04  $0.30  $2.50           ✅   \n",
       "3   gemini-flash-lite       ~5B  2025-09  $0.10  $0.40           ✅   \n",
       "4         gemma-3-12b       12B  2025-03  $0.03  $0.10           ✅   \n",
       "5         gemma-3-27b       27B  2025-03  $0.09  $0.16           ✅   \n",
       "6         gpt-4o-mini       ~8B  2024-07  $0.15  $0.60           ✅   \n",
       "7        gpt-oss-120b  5.1B×23E  2025-08  $0.04  $0.40           ❌   \n",
       "8         gpt-oss-20b   3.6B×6E  2025-08  $0.03  $0.14           ❌   \n",
       "9        llama-3.2-1b        1B  2024-09  $0.01  $0.01           ❌   \n",
       "10       llama-3.2-3b        3B  2024-09  $0.02  $0.02           ❌   \n",
       "11      llama-3.3-70b       70B  2024-12  $0.13  $0.39           ✅   \n",
       "12   llama-4-maverick  17B×128E  2025-04  $0.15  $0.60           ✅   \n",
       "13     mistral-medium   ~60-80B  2025-05  $0.40  $2.00           ✅   \n",
       "14       mistral-nemo       12B  2024-07  $0.02  $0.04           ✅   \n",
       "15          qwen3-32b       32B  2025-04  $0.05  $0.20           ✅   \n",
       "\n",
       "   Open Weights  \n",
       "0             ❌  \n",
       "1             ✅  \n",
       "2             ❌  \n",
       "3             ❌  \n",
       "4             ✅  \n",
       "5             ✅  \n",
       "6             ❌  \n",
       "7             ✅  \n",
       "8             ✅  \n",
       "9             ✅  \n",
       "10            ✅  \n",
       "11            ✅  \n",
       "12            ✅  \n",
       "13            ✅  \n",
       "14            ✅  \n",
       "15            ✅  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default model: gemini-flash-lite\n",
      "Size format: Dense models show total params (e.g., '70B'), MoE models show active×experts (e.g., '17B×128E')\n",
      "JSON Schema = User-defined JSON schemas supported\n",
      "\n",
      "You can also use any OpenRouter model by its full ID (e.g., 'openai/gpt-4o')\n"
     ]
    }
   ],
   "source": [
    "# Get model information dictionary\n",
    "models = llm_list_models()\n",
    "\n",
    "# The function displays the table and returns a dictionary with model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wfub92k94zn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Looking up a specific model\n",
      "Model: gemini-flash-lite\n",
      "  Full ID: google/gemini-2.5-flash-lite\n",
      "  Provider: google\n",
      "  Input cost: $0.10 per million tokens\n",
      "  Output cost: $0.40 per million tokens\n",
      "  JSON schema support: True\n",
      "\n",
      "Example 2: Cheapest models (< $0.15/M input)\n",
      "  Found 10 models:\n",
      "    - gemini-flash-lite ($0.10/M in)\n",
      "    - gemma-3-12b ($0.03/M in)\n",
      "    - gemma-3-27b ($0.09/M in)\n",
      "    - gpt-oss-120b ($0.04/M in)\n",
      "    - gpt-oss-20b ($0.03/M in)\n",
      "\n",
      "Example 3: Models with JSON schema support\n",
      "  Found 11 models with JSON schema support:\n",
      "  deepseek-v3.1, gemini-flash, gemini-flash-lite, gemma-3-12b, gemma-3-27b, gpt-4o-mini...\n",
      "\n",
      "Example 4: Cost comparison for generating 1M input tokens + 1M output tokens\n",
      "  gemini-flash-lite    $  0.50\n",
      "  llama-3.2-3b         $  0.04\n",
      "  claude-haiku         $  4.80\n",
      "  gpt-4o-mini          $  0.75\n"
     ]
    }
   ],
   "source": [
    "# Examples of using the models dictionary\n",
    "\n",
    "# Example 1: Look up information about a specific model\n",
    "gemini_info = models['gemini-flash-lite']\n",
    "print(\"Example 1: Looking up a specific model\")\n",
    "print(f\"Model: gemini-flash-lite\")\n",
    "print(f\"  Full ID: {gemini_info['model_id']}\")\n",
    "print(f\"  Provider: {gemini_info['provider']}\")\n",
    "print(f\"  Input cost: ${gemini_info['cost_in_per_m']:.2f} per million tokens\")\n",
    "print(f\"  Output cost: ${gemini_info['cost_out_per_m']:.2f} per million tokens\")\n",
    "print(f\"  JSON schema support: {gemini_info['json_schema']}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Find the cheapest models (input tokens < $0.15/M)\n",
    "cheap_models = {name: info for name, info in models.items() \n",
    "                if info['cost_in_per_m'] < 0.15}\n",
    "print(f\"Example 2: Cheapest models (< $0.15/M input)\")\n",
    "print(f\"  Found {len(cheap_models)} models:\")\n",
    "for name in list(cheap_models.keys())[:5]:  # Show first 5\n",
    "    cost = cheap_models[name]['cost_in_per_m']\n",
    "    print(f\"    - {name} (${cost:.2f}/M in)\")\n",
    "print()\n",
    "\n",
    "# Example 3: Find models with JSON schema support\n",
    "json_capable = [name for name, info in models.items() if info['json_schema']]\n",
    "print(f\"Example 3: Models with JSON schema support\")\n",
    "print(f\"  Found {len(json_capable)} models with JSON schema support:\")\n",
    "print(f\"  {', '.join(json_capable[:6])}...\")  # Show first 6\n",
    "print()\n",
    "\n",
    "# Example 4: Compare costs between different models\n",
    "print(\"Example 4: Cost comparison for generating 1M input tokens + 1M output tokens\")\n",
    "for model_name in ['gemini-flash-lite', 'llama-3.2-3b', 'claude-haiku', 'gpt-4o-mini']:\n",
    "    info = models[model_name]\n",
    "    total_cost = info['cost_in_per_m'] + info['cost_out_per_m']\n",
    "    print(f\"  {model_name:20} ${total_cost:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Trying Different Models\n",
    "\n",
    "It's easy to compare different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small model (llama-3.2-3b):\n",
      "Quantum computing is a new type of computing that uses the principles of quantum mechanics to perform calculations that\n",
      "are exponentially faster and more powerful than those of classical computers, exploiting the unique properties of\n",
      "quantum-mechanical phenomena such as superposition, entanglement, and interference.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Recommended model (gemini-flash-lite):\n",
      "💰 Cost: $0.000010 | Tokens: 14 in / 21 out | Model: google/gemini-2.5-flash-lite\n",
      "Quantum computing leverages quantum mechanical phenomena like superposition and entanglement to perform calculations\n",
      "that are intractable for classical computers.\n",
      "\n",
      "============================================================\n",
      "\n",
      "A larger model (mistral-medium):\n",
      "💰 Cost: $0.000122 | Tokens: 20 in / 57 out | Model: mistralai/mistral-medium-3\n",
      "Quantum computing is a type of computation that uses quantum bits, or qubits, which can exist in multiple states at once\n",
      "due to the principles of quantum superposition and entanglement, allowing for complex calculations to be performed much\n",
      "faster than classical computers for certain types of problems.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "# Try a small model\n",
    "print(\"A small model (llama-3.2-3b):\")\n",
    "response1 = llm_generate('llama-3.2-3b', prompt)\n",
    "print(response1)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Try the recommended model\n",
    "print(\"Recommended model (gemini-flash-lite):\")\n",
    "response2 = llm_generate('gemini-flash-lite', prompt, estimate_cost=True)\n",
    "print(response2)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# A larger model\n",
    "print(\"A larger model (mistral-medium):\")\n",
    "response3 = llm_generate('mistral-medium', prompt, estimate_cost=True)\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ev36qb1idx",
   "metadata": {},
   "source": [
    "### Using Models Outside the Curated List\n",
    "\n",
    "You can use any model from [OpenRouter's model list](https://openrouter.ai/models) by providing the full model ID. For example, let's try OpenAI's GPT-5-nano (which isn't in our curated list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pe90mn0t2no",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 Cost: $0.000142 | Tokens: 26 in / 161 out | Model: z-ai/glm-4.5-air\n",
      "Here are three key benefits of learning Python:\n",
      "\n",
      "1. **Versatility**: Python is a multi-purpose programming language used in various fields including web development,\n",
      "data science, artificial intelligence, automation, and more. This versatility means you can apply Python skills across\n",
      "different domains and projects.\n",
      "\n",
      "2. **Beginner-Friendly**: Python has a simple, readable syntax that resembles plain English, making it one of the\n",
      "easiest programming languages for beginners to learn. Its straightforward nature allows newcomers to focus on\n",
      "programming concepts rather than complex syntax.\n",
      "\n",
      "3. **Strong Job Market and Community Support**: Python consistently ranks among the most in-demand programming skills in\n",
      "the job market, particularly in data science and machine learning. Additionally, Python has a vast global community that\n",
      "provides extensive documentation, tutorials, and support through forums like Stack Overflow and GitHub.\n"
     ]
    }
   ],
   "source": [
    "# Use the full OpenRouter model ID\n",
    "response = llm_generate(\n",
    "    'z-ai/glm-4.5-air',  # Full model ID from openrouter.ai/models\n",
    "    \"What are three benefits of learning Python?\",\n",
    "    estimate_cost=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Processing Multiple Prompts\n",
    "\n",
    "You can process multiple prompts at once by passing a list of strings. This is useful for batch processing tasks like sentiment analysis or classification.\n",
    "\n",
    "### Simple Batch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of France?\n",
      "A: The capital of France is **Paris**.\n",
      "------------------------------------------------------------\n",
      "Q: What is the capital of Germany?\n",
      "A: The capital of Germany is **Berlin**.\n",
      "------------------------------------------------------------\n",
      "Q: What is the capital of Italy?\n",
      "A: The capital of Italy is **Rome**.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'What is the capital of Germany?',\n",
    "    'What is the capital of Italy?'\n",
    "]\n",
    "\n",
    "responses = llm_generate('gemini-flash-lite', prompts)\n",
    "\n",
    "for prompt, response in zip(prompts, responses):\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Programmatic Prompt Construction\n",
    "\n",
    "Often we want to construct prompts programmatically from data. Here's an example of sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "\n",
      "Text: I love the new design of your website!\n",
      "Sentiment: Positive\n",
      "------------------------------------------------------------\n",
      "Text: The service was terrible and I will not come back.\n",
      "Sentiment: Negative\n",
      "------------------------------------------------------------\n",
      "Text: The product is okay, but it could be better.\n",
      "Sentiment: Neutral\n",
      "------------------------------------------------------------\n",
      "Text: Absolutely fantastic experience, highly recommend!\n",
      "Sentiment: Positive\n",
      "------------------------------------------------------------\n",
      "Text: I'm not sure how I feel about this.\n",
      "Sentiment: Neutral\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for sentiment analysis\n",
    "system_prompt = \"You are a sentiment analysis AI. Classify text as Positive, Negative, or Neutral.\"\n",
    "\n",
    "# List of texts to analyze\n",
    "texts = [\n",
    "    \"I love the new design of your website!\",\n",
    "    \"The service was terrible and I will not come back.\",\n",
    "    \"The product is okay, but it could be better.\",\n",
    "    \"Absolutely fantastic experience, highly recommend!\",\n",
    "    \"I'm not sure how I feel about this.\"\n",
    "]\n",
    "\n",
    "# Construct prompts programmatically\n",
    "instruction = \"Analyze the sentiment of this text. Give only the sentiment classification (Positive, Negative, or Neutral).\\n\\nText: \"\n",
    "prompts = [instruction + text for text in texts]\n",
    "\n",
    "# Generate responses\n",
    "responses = llm_generate('gemini-flash-lite', prompts, system_prompt=system_prompt)\n",
    "\n",
    "# Display results\n",
    "print(\"Sentiment Analysis Results:\\n\")\n",
    "for text, sentiment in zip(texts, responses):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c9bc4",
   "metadata": {},
   "source": [
    "### Monitoring Spending\n",
    "\n",
    "You can run `show_session_spending` at the end of a notebook to see your total OpenRouter usage and cost.  If you use `llm_generate` in multiple notebooks durning a session, the tracking may not be perfect, but it will give you an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "v9fry92hutc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "💰 Current Session Spending Summary\n",
      "======================================================================\n",
      "Total Cost:          $0.000909\n",
      "Total API Calls:     11\n",
      "Total Tokens:        480 in / 1,779 out\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "By Model:\n",
      "  google/gemini-2.5-flash-lite\n",
      "    Cost: $0.000643 | Calls: 8 | Tokens: 403 in / 1,506 out\n",
      "  z-ai/glm-4.5-air\n",
      "    Cost: $0.000142 | Calls: 1 | Tokens: 26 in / 161 out\n",
      "  mistralai/mistral-medium-3\n",
      "    Cost: $0.000122 | Calls: 1 | Tokens: 20 in / 57 out\n",
      "  meta-llama/llama-3.2-3b-instruct\n",
      "    Cost: $0.000002 | Calls: 1 | Tokens: 31 in / 55 out\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total Spent this session: $0.000909\n",
      "Approximate Credit remaining: $9.92\n",
      "(Note: This balance may not reflect the most recent spending)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show spending for this notebook session\n",
    "\n",
    "show_session_spending()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fd9b9",
   "metadata": {},
   "source": [
    "### Using Other Providers\n",
    "\n",
    "You can use other LLM providers in this class as well, but you don't need to do so.  The last notebook shows how to set up other providers if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In the next notebook, we'll explore common NLP tasks including:\n",
    "- Text classification and sentiment analysis\n",
    "- Named Entity Recognition (NER)\n",
    "- Question answering\n",
    "- Translation\n",
    "- Summarization\n",
    "\n",
    "In Lesson 11, we'll dive deeper into how text generation works, explore the underlying APIs in detail, and learn about running LLMs locally for privacy-sensitive applications.\n",
    "\n",
    "For now, practice using `llm_generate()` with different models and prompts to get comfortable with the interface!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds776_env)",
   "language": "python",
   "name": "ds776_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
