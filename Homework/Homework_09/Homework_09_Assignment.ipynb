{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assignment Prompt: Comparative Analysis of BERT, GPT, and BART**  \n",
    "\n",
    "#### **Objective**  \n",
    "In this report, you will analyze and compare three major Transformer-based models: **BERT, GPT, and BART**. Your goal is to explore their **architectures, pretraining approaches, fine-tuning strategies, attention mechanisms, and real-world applications** relevant to your field of interest.  \n",
    "These three architectures are examples of the main transformer models used in NLP today and you should get familiar with them.\n",
    "\n",
    "\n",
    "This assignment requires **critical thinking and personalized analysis**, so a chatbot alone **will not be able to generate a complete, high-quality response**. You must integrate:  \n",
    "\u2705 **Original explanations** of key concepts,  \n",
    "\u2705 **Personal insights** based on your professional or academic background, and  \n",
    "\u2705 **Real-world examples** that demonstrate practical applications.  \n",
    "\n",
    "\ud83d\udccc **Using generative AI for help with definitions is allowed, but additional sources must be provided for verification and expansion.**  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Sections to Include in Your Report**  \n",
    "\n",
    "#### **1. Introduction**  \n",
    "- Introduce **encoder, decoder, and encoder-decoder architectures** and explain their roles in modern AI.  \n",
    "- Explain why each of these architectures is important for different NLP tasks.  \n",
    "- You may focus specifically on BERT (encoder-only), GPT (decoder-only), and BART (encoder-decoder) if you prefer.  \n",
    "\n",
    "#### **2. Model Architectures & Attention Mechanisms**  \n",
    "- Compare the **encoder-decoder structures** of the three models.  \n",
    "- Explain **how BERT\u2019s bidirectional encoding** differs from **GPT\u2019s autoregressive approach** and **BART\u2019s hybrid nature**.  \n",
    "- **Describe the three types of attention mechanisms** used in these models and where they are applied:  \n",
    "  - **Self-Attention (Encoder-Side, as in BERT and BART's Encoder)** \u2013 Each token attends to all other tokens in the input to capture bidirectional context.  \n",
    "  - **Causal Self-Attention (Decoder-Side, as in GPT and BART's Decoder)** \u2013 Each token can only attend to previous tokens, enabling autoregressive text generation.  \n",
    "  - **Cross-Attention (Encoder-to-Decoder, as in BART)** \u2013 The decoder attends to all encoder outputs, allowing for contextualized sequence-to-sequence learning.  \n",
    "- Include **hand-drawn diagrams** illustrating **how attention works differently in each model**. These must be included as cropped scans or photos in your submission.  To include pictures in your markdown, save them in the same directory as your notebook and use HTML to include them in the markdown like this: &lt;img src=\"diagram1.png\" alt=\"Attention Mechanism Diagram\" width=\"600\" /&gt;\n",
    "\n",
    "#### **3. Pretraining Objectives & Key Terminology**  \n",
    "For each model, explain its **pretraining strategy** and define the following terms **in your own words**:  \n",
    "- **Masked Language Modeling (MLM)** (BERT)  \n",
    "- **Causal Language Modeling (CLM)** (GPT)  \n",
    "- **Autoregression**  \n",
    "- **Denoising Autoencoder** (BART)  \n",
    "- **Self-Attention vs. Causal Attention**  \n",
    "- **Fine-tuning** (How each model is adapted for specific tasks)  \n",
    "- **Transfer Learning** (Why these models can be used for many NLP applications)  \n",
    "\n",
    "To ensure originality, you must include **one example per term** that relates to a unique dataset, real-world challenge, or personal experience in your field.  \n",
    "\n",
    "#### **4. Fine-Tuning Approaches**  \n",
    "- Discuss how each model can be fine-tuned for tasks like **text classification, summarization, or question-answering**.   \n",
    "- Describe how you would **fine-tune one of these models for your own work or in your field of interest**.  \n",
    "\n",
    "#### **5. Real-World Applications & Your Professional Interests**  \n",
    "- Identify **three NLP applications** that interest you.  \n",
    "- Discuss which model (**BERT, GPT, or BART**) would be most effective for each and **justify your choice**.  \n",
    "- Mention **challenges in your field of expertise or interest** that these models might help solve (e.g., legal document analysis, medical text summarization, automated customer support).  \n",
    "\n",
    "#### **6. Conclusion**  \n",
    "- Summarize key takeaways.  \n",
    "- Reflect on **which model you find most useful** for your own interests and why.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Formatting & Submission Guidelines**  \n",
    "\u2705 **Length:** 1500-2000 words (this would be 5-7 pages of double-spaced text).  You can run the `Count_Words.ipynb` notebook to count the words in a notebook or HTML file.  \n",
    "\u2705 **Citations:** At least 3 sources (e.g., academic papers, Hugging Face documentation, blog posts). You must provide **one source that is not from OpenAI or Hugging Face**.  \n",
    "\u2705 **Figures/Diagrams:** Include **at least three hand-drawn diagrams** of the attention mechanisms. Other hand-drawn figures are welcome. These must be scanned or photographed and cropped for clarity.  \n",
    "\u2705 **Use Clear Formatting:** Each section and defined term must have **clear headers and subtitles** for readability.  We've included a file, `Homework_09_Report.ipynb` with all the headers to get you started.\n",
    "\u2705 **Submission Formats:** You can write your report in **Markdown within a Jupyter Notebook** (exported to HTML) or use **Word or a similar program and submit as a PDF**.  \n",
    "\u2705 **Code is not required:** If you want, you can include small examples of applications in your area of interest, but this is not a requirement.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Tips to Ensure Originality (Avoiding Chatbot Reliance)**  \n",
    "\ud83d\udccc **Personalization is required** \u2013 You must relate the models to your field. A chatbot cannot generate **your professional insights or unique examples**.  \n",
    "\ud83d\udccc **Diagrams & real-world applications** \u2013 Explain concepts visually or with domain-specific examples (e.g., healthcare, finance, law).  \n",
    "\ud83d\udccc **Compare different sources** \u2013 Don't rely on a single AI-generated explanation. Use additional sources beyond chatbots.  \n",
    "\ud83d\udccc **Critical thinking** \u2013 Challenge model limitations. Chatbots often **fail at deep analysis**\u2014you should go beyond surface-level descriptions.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Evaluation Criteria**  \n",
    "\u2705 **Completeness, Figures** - 15 points. You addressed all of the prompts and included thoughtful hand-drawn figures.  \n",
    "\u2705 **Personalization, Applications** - 15 points. You added significant personalization and discussed applications in your field of interest.  \n",
    "\u2705 **Citations, Supporting Evidence** - 10 points. Well-researched with credible sources.  \n",
    "\u2705 **AI-Generated Content Penalty** - A generic chatbot-generated document will earn *at most* 20 out of the 40-point total.  \n",
    "\n",
    "---  \n",
    "\n",
    "You should begin by copying the headers for each of the numbered sections described above.\n",
    "\n",
    "I fully expect you to use the text and chatbots to understand each of the ideas and terms \n",
    "but I also expect you to think about your answers, personalize them, and provide additional supprort for them.\n",
    "Personalization and effort are key to earning full credit for this report.  \n",
    "\n",
    "I'm generally interested in applications of AI/deep learning in education and healthcare.  To give you an idea of what I'm looking for in your reports, here are some examples of applications related to my own interests that I would discuss in a report:\n",
    "* **Encoder only models**\n",
    "  * Text classification model for classifying patient messages to their doctors as requiring immediate response or not.\n",
    "  * Vision transformer models for classification are encoder only models.  I'm interested in testing those in a computer-aided diagnosis system for diagnosing cancer in breast ultrasound images and videos.\n",
    "* **Decoder only models**\n",
    "  * Develop a fine-tuned generative model for producing tailored impressions, and possibly even structured reports, as part of computer-aided diagnostic system for cancer diagnosis.  This would help reduce dictation time and fatique.\n",
    "  * Develop a fine-tuned reasoning LLM for math tutoring that drives a collaborative tutoring system for helping student learn mathematics.\n",
    "* **Encoder-Decoder models**\n",
    "  * Question answering models that can be fine-tuned or used in conjuction with a retrieval system that can ingest a corpus of scientific papers and answer questions about them.  (LLMs can already do this to some extent.)\n",
    "  * Vision tranformer models for segmentation of lesions in breast ultrasound images and videos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n\nUncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_09_Models/` | `Homework_09_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` \u2014 never hardcode paths like `'Homework_09_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}