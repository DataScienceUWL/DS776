{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "# Homework 10: Named Entity Recognition\n\n**Name:** [Your Name Here]  \n**Total Points: 40**\n\n## Submission Checklist\n- [ ] All code cells executed with output saved\n- [ ] All questions answered in markdown cells\n- [ ] Used `DATA_PATH` and `MODELS_PATH` variables (no hardcoded paths)\n- [ ] Per-entity metrics visualization created\n- [ ] Comparison of fine-tuned models vs LLM completed\n- [ ] Reflection questions answered\n- [ ] Notebook exported to HTML\n- [ ] Canvas filename includes `_GRADE_THIS_ONE`\n- [ ] Files uploaded to Canvas\n\n---\n\n**Point Breakdown:**\n- Part 1 (Named Entities for Analysis): 6 points\n- Part 2 (Fine-tune BERT Models): 12 points\n- Part 3 (LLM for NER): 12 points\n- Part 4 (Comparison): 6 points\n- Part 5 (Reflection): 2 points"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 1 - Using Named Entities for Analysis (6 points)\n",
    "\n",
    "NER is often used to look for trends or to do other analysis on text data. Once you have the NER tags you can use them to extract the entities from the text to do analysis.\n",
    "\n",
    "Here we'll use a dataset of made-up movie reviews. The idea is to use the entity tags to extract the actors and directors from the reviews, then to figure out which actors and directors are most likely to be involved with positive sentiment movies and negative sentiment movies. We'll load the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hobbes99/fake_movie_reviews_ner_sentiment\"\n",
    ")\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aqsq06cidrl",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_10_Models/` | `Homework_10_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` ‚Äî never hardcode paths like `'Homework_10_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "Here's an entry in the training set to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "Notice that NER tags are stored as integers corresponding to their indices in `label_list`. You'll need to use those tags to extract the actor and director names. You can also extract the sentiment.\n",
    "\n",
    "For the training split, find and display in order:\n",
    "* The three actors most likely to appear in positive films.\n",
    "* The three actors most likely to appear in negative films.\n",
    "* The three directors most likely to have directed positive films.\n",
    "* The three directors most likely to have directed negative films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract entities from training set\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create empty lists or dictionaries to store:\n",
    "#    - Actors with positive sentiment\n",
    "#    - Actors with negative sentiment\n",
    "#    - Directors with positive sentiment\n",
    "#    - Directors with negative sentiment\n",
    "# 2. Loop through the training dataset\n",
    "# 3. For each example, extract:\n",
    "#    - The sentiment (0 or 1)\n",
    "#    - Tokens tagged as B-ACTOR or I-ACTOR\n",
    "#    - Tokens tagged as B-DIRECTOR or I-DIRECTOR\n",
    "# 4. Keep track of counts for each actor/director by sentiment\n",
    "\n",
    "# Hint: You'll need to:\n",
    "# - Convert token lists to strings (join consecutive I- tags with B- tag)\n",
    "# - Track how many times each actor appears in positive vs negative reviews\n",
    "# - Use label_list to convert numeric tags to string labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate proportions and find top 3\n",
    "\n",
    "# Your code here:\n",
    "# 1. For each actor/director, calculate:\n",
    "#    total_appearances = positive_count + negative_count\n",
    "#    positive_proportion = positive_count / total_appearances\n",
    "# 2. Sort by positive proportion (descending for positive, ascending for negative)\n",
    "# 3. Select top 3 for each category\n",
    "# 4. Display results with counts and proportions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 2 - Fine Tuning Two BERT NER Models (12 points)\n",
    "\n",
    "The MIT Movie Corpus is designed for movie-related NER tasks and includes the following entity types in BIO format:\n",
    "- **Actor**: Names of actors or actresses (e.g., \"Leonardo DiCaprio\").\n",
    "- **Character**: Names of characters in movies (e.g., \"Jack Dawson\").\n",
    "- **Director**: Names of movie directors (e.g., \"Christopher Nolan\").\n",
    "- **Genre**: Movie genres (e.g., \"Action\", \"Drama\").\n",
    "- **Title**: Titles of movies (e.g., \"Inception\").\n",
    "- **Year**: Year the movie was made.\n",
    "\n",
    "The original movie corpus includes more entity types, but we've produced a simplified version for this assignment.\n",
    "\n",
    "In this part of the assignment you should fine-tune \"distilbert-base-uncased\" and \"bert-base-uncased\" for NER on the dataset \"hobbes99/mit-movie-ner-simplified\". The dataset has \"train\" and \"valid\" splits. Use the \"train\" split for fine-tuning and evaluate the metrics using seqeval as shown in the lesson.\n",
    "* Figure out a way to plot precision, recall, and F1 by entity type.\n",
    "* Find two movie reviews on the internet and run inference on them to extract the named entities.\n",
    "* Write a brief summary of the results. Include answers to:\n",
    "    * Which entity types does the model struggle with?\n",
    "    * Which does it do well on?\n",
    "* The \"distilbert-base-uncased\" model is a distilled version of the \"bert-base-uncased\" model (distillation means a smaller model that was trained using the larger trained model as a \"teacher\"). The \"bert-base-uncased\" model should lead to better results here. Does it? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load the \"hobbes99/mit-movie-ner-simplified\" dataset\n",
    "# 2. Examine the 'train' and 'valid' splits\n",
    "# 3. Print an example to see the structure\n",
    "# 4. Get the label names from the dataset features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare for fine-tuning - Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load two tokenizers:\n",
    "#    - \"distilbert-base-uncased\"\n",
    "#    - \"bert-base-uncased\"\n",
    "# 2. Create a tokenization function that:\n",
    "#    - Tokenizes the tokens (use is_split_into_words=True)\n",
    "#    - Aligns the NER labels with tokenized output\n",
    "#    - Handles subword tokens (use -100 for ignored tokens)\n",
    "# 3. Apply tokenization to both train and valid splits\n",
    "\n",
    "# Hint: word_ids() method helps align labels with subword tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up metrics for evaluation\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load the seqeval metric\n",
    "# 2. Create a compute_metrics function that:\n",
    "#    - Extracts predictions and labels\n",
    "#    - Removes ignored tokens (-100)\n",
    "#    - Converts numeric labels to string labels\n",
    "#    - Computes precision, recall, F1 overall and per-entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fine-tune distilbert-base-uncased\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load the model with num_labels matching your dataset\n",
    "# 2. Create TrainingArguments (output_dir, num_epochs, batch_size, etc.)\n",
    "# 3. Create a DataCollator for token classification\n",
    "# 4. Create Trainer with model, args, datasets, tokenizer, data_collator, compute_metrics\n",
    "# 5. Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate distilbert and get per-entity metrics\n",
    "\n",
    "# Your code here:\n",
    "# 1. Run evaluation on the valid set\n",
    "# 2. Get predictions for detailed analysis\n",
    "# 3. Calculate precision, recall, F1 for each entity type\n",
    "# 4. Create a visualization (bar chart) comparing metrics by entity\n",
    "\n",
    "# Hint: Use trainer.predict() and seqeval.compute() for detailed results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create visualization of per-entity performance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Your code here:\n",
    "# 1. Extract precision, recall, F1 for each entity type\n",
    "# 2. Create a DataFrame with entity types and metrics\n",
    "# 3. Plot a grouped bar chart comparing precision, recall, F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Repeat for bert-base-uncased\n",
    "\n",
    "# Your code here:\n",
    "# 1. Tokenize the dataset with bert-base-uncased tokenizer\n",
    "# 2. Load bert-base-uncased model\n",
    "# 3. Create new Trainer with BERT model\n",
    "# 4. Train the BERT model\n",
    "# 5. Evaluate and compare with DistilBERT\n",
    "\n",
    "# (Follow same steps as DistilBERT - cells above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Run inference on movie reviews from internet\n",
    "from transformers import pipeline\n",
    "\n",
    "# Your code here:\n",
    "# 1. Find 2 movie reviews from the internet (IMDB, Rotten Tomatoes, etc.)\n",
    "# 2. Create a NER pipeline with your fine-tuned model\n",
    "# 3. Run inference on the reviews\n",
    "# 4. Display the extracted entities with their types\n",
    "\n",
    "# Hint: Use pipeline(\"ner\", model=your_model, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Compare DistilBERT vs BERT\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create a comparison table with:\n",
    "#    - Model name\n",
    "#    - Overall F1 score\n",
    "#    - Training time (approximate)\n",
    "#    - Model size (parameters)\n",
    "#    - Best/worst entity types\n",
    "# 2. Discuss which model performed better\n",
    "# 3. Analyze whether BERT's larger size justified better performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Part 3 - Using an LLM for NER (12 points)\n",
    "\n",
    "For the first 100 texts in the \"valid\" split, mimic what we did in the lesson to extract the \"Actor\", \"Character\", \"Director\", \"Genre\", \"Title\" and \"Year\" entities using an LLM. Start with just a few examples to refine your prompt and instructions, then ramp up to 100 or more examples. Get the final evaluation metrics as shown in the lesson.\n",
    "\n",
    "**Hint:** You can import the `llm_ner_extractor` function from `Lesson_10_Helpers` to streamline your LLM-based extraction, similar to how we used `llm_classifier` in Lesson 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare validation subset\n",
    "from Lesson_10_Helpers import llm_ner_extractor\n",
    "\n",
    "# Your code here:\n",
    "# 1. Get first 100 examples from valid split\n",
    "# 2. Extract texts and true labels\n",
    "# 3. Start with 2-3 examples to test your prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Design prompt for LLM-based NER\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create system_prompt explaining the task\n",
    "# 2. Create prompt_template with:\n",
    "#    - Instructions to extract entities\n",
    "#    - Entity types to look for: Actor, Character, Director, Genre, Title, Year\n",
    "#    - Request structured output (JSON format recommended)\n",
    "#    - Include the {text} placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Test prompt on small subset with llm_ner_extractor\n",
    "\n",
    "# Your code here:\n",
    "# 1. Use llm_ner_extractor function (similar to llm_classifier from Lesson 8)\n",
    "# 2. Start with a small API model or local model\n",
    "# 3. Test on 3-5 examples\n",
    "# 4. Refine your prompt based on results\n",
    "# 5. Parse JSON output and convert to BIO format\n",
    "\n",
    "# Hint: Use llm_generate() to test prompt formatting before scaling up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert LLM outputs to BIO format\n",
    "\n",
    "# Your code here:\n",
    "# 1. Parse JSON from LLM response\n",
    "# 2. Match entity spans to token positions\n",
    "# 3. Convert to BIO format (B-ACTOR, I-ACTOR, etc.)\n",
    "# 4. Handle errors/malformed JSON gracefully\n",
    "\n",
    "# Hint: llm_ner_extractor from Lesson_10_Helpers handles conversion for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Scale up to 100 examples and evaluate\n",
    "\n",
    "# Your code here:\n",
    "# 1. Once prompt is refined, run on all 100 validation examples\n",
    "# 2. Convert predictions to BIO format\n",
    "# 3. Calculate metrics using seqeval (like Part 2)\n",
    "# 4. Generate classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "## Part 4 - Comparison (6 points)\n\n* Compare the results of the two entity recognition techniques (fine-tuned BERT models vs LLM zero-shot) both quantitatively and qualitatively.\n* Consider the difficulty of obtaining labeled data in your comparison. It's time-consuming and/or costly to get tagged text, but that's not necessary for the LLM approach which may be less accurate.\n* Which approach would you choose for a production system and why? Consider accuracy, speed, cost, and maintenance requirements.\n* Give a brief summary of what you learned in this assignment."
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "üìù **YOUR COMPARISON AND SUMMARY HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Part 5 - Reflection (2 points)\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for the lesson? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}