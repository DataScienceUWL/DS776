{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Homework 04 Assignment\n**Name:** [Student Name Here]  \n**Total Points:** 40\n\n## Submission Checklist\n- [ ] All code cells executed with output saved\n- [ ] All questions answered\n- [ ] Notebook converted to HTML (use the Homework_04_Utilities notebook)\n- [ ] Canvas notebook filename includes `_GRADE_THIS_ONE`\n- [ ] Files uploaded to Canvas\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "0b439c",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": "# Deeper Networks\n\nIn this assignment you'll explore using a deeper, fully connected model to classify FashionMNIST images again.  The point isn't really to get the best classifier possible, rather we want you to get some experience seeing how deep networks can be difficult to train and how using batch normalization and residual connections can make a network easier to train while also increasing the performance.  Along the way you'll practice building a deep model using loops instead of typing out each individual layer.\n\n\n## Part 1 - Deep Fully Connected Network (8 pts)\n\nOur network will consist of:\n* input layer which maps the flattened 784 pixels to 64 neurons followed by a ReLU function.\n* 10 blocks, each block has linear + ReLU + linear + ReLU, the number of neurons stays at 64\n* a linear output layer that maps the 64 neurons to 10 output neurons (for 10 classes) \n\nUse this template (cut and paste into a code cell and complete the model)\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleBlock(nn.Module):\n    def __init__(self, hidden_dim=64):\n        super(SimpleBlock, self).__init__()\n        # fill in the repeating elements here linear(hidden_dim, hidden_dim) + ReLU + linear + ReLU\n        \n    def forward(self, x):\n        # fill the forward methods to call linear + ReLU + linear + ReLU and return result\n        return # complete\n\nclass Deep_MNIST_FC(nn.Module):\n    def __init__(self, num_blocks=10):\n        super(Deep_MNIST_FC, self).__init__()\n        \n        # Input layer\n        self.input_layer = ### complete the input layer with linear and ReLU, will handle flatten in the forward below\n    \n        \n        # Repeating simple blocks\n        self.blocks = nn.Sequential(*[SimpleBlock(64) for _ in range(num_blocks)])\n        \n        # Output layer\n        self.output_layer = ### add output layer\n        \n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten input\n        ## call input layer\n        ## call blocks\n        ## call output layer\n        return x\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Complete the SimpleBlock class\n# - Add two linear layers (hidden_dim -> hidden_dim) \n# - Add ReLU activations between and after the layers\n# - Remember to define layers in __init__ and use them in forward()\n\nclass SimpleBlock(nn.Module):\n    def __init__(self, hidden_dim=64):\n        super(SimpleBlock, self).__init__()\n        # Fill in the repeating elements here: linear(hidden_dim, hidden_dim) + ReLU + linear + ReLU\n        \n    def forward(self, x):\n        # Fill the forward method to call linear + ReLU + linear + ReLU and return result\n        return  # Complete this"
  },
  {
   "cell_type": "markdown",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_04_Models/` | `Homework_04_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` ‚Äî never hardcode paths like `'Homework_04_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Setup the downsampled (10% of training data) FashionMNIST DataSet and DataLoaders here.  Include the data augmentation from last week.  Use batchsize = 64.  If you're not doing so already, instead of test_dataset and test_loader make the second dataset (with train = False) the valid_dataset and the loader should be valid_loader."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Complete the Deep_MNIST_FC class\n# - Fill in the input layer (784 -> 64 neurons + ReLU)\n# - The blocks are already defined for you\n# - Fill in the output layer (64 -> 10 classes)\n# - Complete the forward method calls\n\nclass Deep_MNIST_FC(nn.Module):\n    def __init__(self, num_blocks=10):\n        super(Deep_MNIST_FC, self).__init__()\n        \n        # Input layer\n        self.input_layer = ### Complete the input layer with linear and ReLU, will handle flatten in forward below\n    \n        # Repeating simple blocks (already provided)\n        self.blocks = nn.Sequential(*[SimpleBlock(64) for _ in range(num_blocks)])\n        \n        # Output layer\n        self.output_layer = ### Add output layer\n        \n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten input\n        ## Call input layer\n        ## Call blocks\n        ## Call output layer\n        return x\n\n# Create model and show summary\n# model = Deep_MNIST_FC()\n# summary(model, input_size=(32, 1, 28, 28))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now train the model with AdamW (lr = 0.001) for 40 epochs.  Track the accuracy.  Be sure to save a checkpoint file."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0803",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Train the Deep_MNIST_FC model with AdamW optimizer for 40 epochs\n# - Initialize the model with 10 blocks\n# - Use AdamW optimizer with lr=0.001\n# - Track training and validation accuracy\n# - Save checkpoint after training\n# - Use the training loop from previous assignments"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now use plot_training_metrics to plot the loss and accuracy for the training and validation sets.  You don't have to display the results dataframe, just the plots."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Plot training metrics using plot_training_metrics function\n# - Load the training results from the checkpoint\n# - Use plot_training_metrics to display loss and accuracy curves\n# - Show both training and validation metrics\n# - No need to display the results dataframe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2 - Batch Normalization (8 pts)\n\nCreate a new model class called Deep_MNIST_FN based on your code from Part 1.  You should now use BatchNorm1d after every linear layer.  Create an instance of your model and train it for 40 epochs.  Produce the same graphs.  Comment on how this model trained compared to the model in Part 1."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Setup FashionMNIST data with downsampling and augmentation\n# - Load FashionMNIST with appropriate transforms (normalization: mean=0.2860, std=0.3530)\n# - Downsample training set to 10% using provided code\n# - Create DataLoaders with batch_size=64\n\n# Use this code for downsampling (copy and uncomment):\n# from torch.utils.data import Subset\n# import numpy as np\n# np.random.seed(42)  # use this seed for reproducibility\n# subset_indices = np.random.choice(len(train_dataset), size=int(0.1 * len(train_dataset)), replace=False)\n# train_dataset = Subset(train_dataset, subset_indices)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create Deep_MNIST_FN model class with batch normalization\n# - Base on Deep_MNIST_FC from Part 1\n# - Add BatchNorm1d after every linear layer\n# - Train for 40 epochs with same parameters\n# - Create plots comparing to Part 1 model\n# - Comment on training differences"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3 - Residual Connections (8 pts)\n\nNow create a new model class called Deep_MNIST_FC_Res by modifying the model from Part 1 so that each block (with two linear layers) has a residual connection around that block.  This model should not include batch normalization.  Train for 40 epochs.  Make plots."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create Deep_MNIST_FC_Res model class with residual connections\n# - Modify the SimpleBlock to include residual connections around each block\n# - Add the input to the output of each block (x + block_output)\n# - No batch normalization in this version\n# - Make sure dimensions match for the residual connection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Train the Deep_MNIST_FC_Res model for 40 epochs\n# - Use same training parameters as previous models\n# - AdamW optimizer with lr=0.001\n# - Track training and validation metrics\n# - Make plots to compare with previous models"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4 - Combined: BatchNorm + Residuals (8 pts)\n\nNow create a new model class called Deep_MNIST_FC_Res_BN by modifying one of the models from above so that each block (with two linear layers) has a residual connection around that block and all linear layers are followed by batch normalization.  Train for 40 epochs.  Make plots."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create Deep_MNIST_FC_Res_BN model with both residual connections and batch normalization\n# - Combine residual connections from Part 3 with batch normalization from Part 2\n# - Add BatchNorm1d after every linear layer\n# - Include residual connections around each block\n# - This should be the most advanced model combining both techniques"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Train the Deep_MNIST_FC_Res_BN model for 40 epochs\n# - Use same training parameters as all previous models\n# - AdamW optimizer with lr=0.001\n# - Track training and validation metrics for comparison\n# - Make plots to compare with all previous models"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 5 - Analysis and Comparison (6 pts)\n\nCreate two plot showing the validation loss and validation accuracy for each of the four models above.  Write a comparison of the four models.  If you could only choose batch normalization or residual connections, which would it be?  Does the model with both residual connections and batch normalization perform better than the others.  Which approach yeilds the fastest training? Do you find that both residual connections and batch normalization are necessary?  Address ALL of these questions in your analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create comparison plots for all four models\n# - Plot validation loss for all models on one figure\n# - Plot validation accuracy for all models on another figure\n# - Use different colors/styles for each model\n# - Add legends to identify each model clearly\n# - Include models: Deep_MNIST_FC, Deep_MNIST_FN, Deep_MNIST_FC_Res, Deep_MNIST_FC_Res_BN"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "üìù **YOUR ANALYSIS HERE:**\n\nWrite a comprehensive comparison of the four models addressing all the questions above:\n- Compare performance of all four models\n- Choose between batch normalization OR residual connections if you could only pick one\n- Evaluate whether the combined model (res + BN) performs best\n- Analyze which approach yields fastest training\n- Discuss necessity of both techniques\n\nUse your training results and plots to support your analysis."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 6 - Reflection (2 pts)\n\n1. What, if anything, did you find difficult to understand for this lesson? Why?\n\nüìù **YOUR ANSWER HERE:**\n\n2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n\nüìù **YOUR ANSWER HERE:**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n\nUncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}