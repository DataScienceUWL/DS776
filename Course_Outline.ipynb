{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Course Syllabus\n",
    "\n",
    "## Course Objectives: ##\n",
    "- Understand the conceptual framework of deep learning models/applications but maybe not all the details.\n",
    "- Be able to utilize pretrained and foundational models, perhaps with finetuning.\n",
    "- Be able to manage a simple deep learning workflow:  build and split dataset, train or fine tune model, save model, load model, inference, warm restarts, possibly monitoring experiments, possibly deployments\n",
    "- Assessing Model Performance and Limitations (Data Bias)\n",
    "- Computer Vision Applications.  NLP Applications.\n",
    "\n",
    "## Textbooks and Other Resources\n",
    "\n",
    "### Required Books\n",
    "- \"Inside Deep Learning: Math, Algorithms, Models\" by Edward Raff (2022)\n",
    "- \"Natural Language Processing with Transformers, Revised Edition\" by Tunstall, et al (2022)\n",
    "\n",
    "###  Free Online Books\n",
    "- (Dive Into Deep Learning)[https://d2l.ai/]  Shows how to code in multiple frameworks, but you'll need to use their packages to get things up and running easily.  \n",
    "- (Understanding Deep Learning)[https://udlbook.github.io/udlbook/]  Explains a lot of the mathematical details about various deep learning models.  Great pictures.  It isn't light reading.\n",
    "- (Deep Learning)[https://www.deeplearningbook.org/]  From MIT.  Good book for the first half of the class.\n",
    "\n",
    "### Other Comprehensive Resources:\n",
    "- **Hugging Face Documentation:** [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "- **PyTorch Documentation:** [PyTorch](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Week 1: Introduction to Deep Learning and Fully Connected Networks\n",
    "\n",
    "## Objectives\n",
    "1. **Implement Basic Deep Learning Workflow in PyTorch**\n",
    "   - Prepare Data (Datasets, DataLoaders, batching)\n",
    "   - Configure Model (Classes and OOP, PyTorch building blocks)\n",
    "   - Train Model (train_simple_network)\n",
    "   - Evaluate Model (metrics)\n",
    "   - Make Predictions on New Data (write a predict function)\n",
    "\n",
    "2.  **Regression and Classification Basics**\n",
    "   - Similarities and Differences in NN's and Loss Functions\n",
    "\n",
    "## Readings and Videos\n",
    "* Read Chapters 1-2 of IDL\n",
    "* Review first half of DS740 NN Storybook\n",
    "* (NOT DONE YET) How to use course notebooks.\n",
    "* Notebooks\n",
    "   * Nonlinear_Regression_1D\n",
    "   * Classification_2D\n",
    "\n",
    "## Assessments\n",
    "1. Reading Quiz in Canvas\n",
    "2. Submit notebook for classifying spiral points.\n",
    "\n",
    "## Auxiliary Resources\n",
    "1.  Some Deep Learning Big Picture Video - [Deep Learning Basics: Introduction and Overview](https://www.youtube.com/watch?v=O5xeyoRL95U)\n",
    "2.  Linear Algebra Resources\n",
    "3.  [Introduction to PyTorch Tensors - PyTorch Documentation](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) \n",
    "\n",
    "\n",
    "- [MIT 6.S191: Introduction to Deep Learning](https://www.youtube.com/watch?v=5tvmMX8r_OM) - Lecture series from MIT's introductory deep learning course.  Low level, begins with deep fake of Obama introducing class (from 2020). Newer video from 2021.  Look to see what the newest version of this intro video.  Great intro level to NN, Loss Functions, Training, and Overfitting.\n",
    "- [Deep Learning Crash Course for Beginners](https://www.youtube.com/watch?v=VyWAvY2CF9c) - A comprehensive introduction to deep learning basics.  This 90 minute video is a good summary of most of weeks 1-7.  I could copy the style of this video or just assign bits of it.  DON'T USE THIS ONE, but consider integrating or copying it's style.\n",
    "\n",
    "- [Introduction to Deep Learning - MIT OpenCourseWare](https://ocw.mit.edu/courses/6-s191-introduction-to-deep-learning-january-iap-2020/) - Free course material including lecture videos and notes.\n",
    "\n",
    "- [Visual Guide to NN](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)By the Illustrated Transformer guy. (All his stuff is great)\n",
    "- [DeepLearningAI - NNs and DL playlist](https://www.youtube.com/watch?v=CS4cs9xVecg&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&ab_channel=DeepLearningAI) Andrew Ng videos - use some of these\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## Objectives / Topics\n",
    "\n",
    "1. **Image Data Representation**: Grayscale, RGB, channels, Numpy vs PyTorch.\n",
    "2. **Convolutional Kernels**: fixed kernels (color and edge detectors) vs learnable kernels, receptive fields, Conv2D in PyTorch, kernel size, padding, stride\n",
    "3. **Feature Maps and Size Calculation**:  formula for output size, different levels of features\n",
    "4. **Pooling Layers:** max vs average, dimensionality reduction, improved translational invariance, pooling vs stride > 1\n",
    "\n",
    "## Readings and Videos\n",
    "\n",
    "* Notebook for Image Representation\n",
    "* MNIST_FC notebook\n",
    "* Read Chapter 3 in IDL\n",
    "* Output Sizes Notebook\n",
    "* Andrew Ng video(s) on convolutions\n",
    "* MNIST_CNN Notebook (include brief CNN Explainer Video)\n",
    "\n",
    "## Assessments\n",
    "\n",
    "* Canvas Quiz for reading and output size calculations\n",
    "* Build and test a couple of networks for FashionMNIST.  Try LeNet5.  Try \"SmallCNN\" as follows.\n",
    "    * What are the most common misclassifications?  Do they make sense?\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Optimization and Regularization Techniques\n",
    "\n",
    "## Topics:\n",
    "* Overfitting\n",
    "* Optimizers\n",
    "* Regularization\n",
    "* Splitting Data\n",
    "* Data Augmentation\n",
    "* Learning Rate Schedulers\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Overfitting:** What is it?  What are common approaches to preventing it?  Why is a common problem in deep learning?\n",
    "2. **Optimizers:** Learn about alternatives to SGD.  Adaptive learning rates.  Momentum.\n",
    "3. **Regularization:** L2 and L1 regulurization.  L2 is built into many optimizers.  L1 needs a term added to loss function.\n",
    "4. **Learning Rate Schedulers:** What do they do?  General advice.\n",
    "5. **Data Augmentation:** What is it?  Why is it helpful?  How to choose augmentations.\n",
    "\n",
    "\n",
    "## Readings and Videos\n",
    "* IDL 5.1-5.3\n",
    "* Resizing and Augmentation Notebook\n",
    "* CIFAR 10 Notebook (split into smaller parts, mention Optuna here)\n",
    "* Augmentation Notebook.\n",
    "\n",
    "**Assessments:**\n",
    "- Quiz on optimization and regularization\n",
    "- Homework: Experiment with different optimization, regularization, and augmentation for CNN on FashionMNIST.\n",
    "\n",
    "**Textbook Support:**\n",
    "- IDL end of Chapter 3.  IDL 5.1-5.3\n",
    "\n",
    "**Video Lectures:**\n",
    "- [Optimization Techniques in Deep Learning](https://www.youtube.com/watch?v=G_wG9tCVA3k) - Explanation of various optimization algorithms.\n",
    "- [Regularization in Deep Learning](https://www.youtube.com/watch?v=ZLUtucsaE5g) - Detailed lecture on regularization methods.\n",
    "\n",
    "**Tutorials:**\n",
    "- [PyTorch Optimization Techniques](https://www.youtube.com/watch?v=t9aUoUnwhvI) - Implementing different optimization algorithms in PyTorch.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Advanced CNNs and Transfer Learning\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "Batch Normalization, Residual Connections, Skip Connections, Transfer Learning (head only vs fine-tuning)\n",
    "\n",
    "1.  **Batch Normalization**  What's the point of normalization in general?  Experiment with batch normalization and training CNN.  Show for CIFAR10.\n",
    "\n",
    "2.  **Residual Connections**  Help forward and backward propogation of info in network.  Smooths loss surface. Allows deeper networks and improved training.\n",
    "\n",
    "3.  **Skip Connections**  Similar to residual connections, but results from parallel paths are concatenated before sending to next layers.\n",
    "\n",
    "4.  **Transfer Learning**  Work through an example showing tuning only the head vs finetuning all layers.\n",
    "\n",
    "## Readings and Videos\n",
    "- IDL 6.1-6.5, 13.1-13.2\n",
    "\n",
    "## Assessments\n",
    "- Quiz on advanced CNN architectures\n",
    "- Homework: Compare the performance of different CNN architectures on a chosen dataset\n",
    "- Quiz on transfer learning\n",
    "- Homework: Apply transfer learning to a custom dataset\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Object Detection and Segmentation\n",
    "\n",
    "1. **Fundamentals of Object Detection and Image Segmentation**:\n",
    "   - Understand the basic concepts and techniques in object detection and image segmentation.\n",
    "   - Grasp the importance and applications of object detection and segmentation in various fields.\n",
    "\n",
    "2. **Object Detection Models**:\n",
    "   - Learn the differences between one-stage object detection models (e.g., YOLO, SSD) and two-stage object detection models (e.g., R-CNN, Fast R-CNN, Faster R-CNN).\n",
    "   - Compare and contrast the performance, accuracy, and use-cases of one-stage and two-stage object detection methods.\n",
    "\n",
    "3. **Image Segmentation Techniques**:\n",
    "   - Implement basic image segmentation techniques using the U-Net architecture.\n",
    "   - Understand the process of per-pixel prediction and the applications of U-Net in medical imaging and other fields.\n",
    "\n",
    "4. **Advanced Object Detection**:\n",
    "   - Explore the applications and implementation of Faster R-CNN for object detection.\n",
    "   - Learn how to apply a pre-trained Faster R-CNN model to detect objects in images.\n",
    "\n",
    "5. **Practical Implementation and Evaluation**:\n",
    "   - Gain hands-on experience in coding and implementing U-Net for image segmentation on a provided dataset.\n",
    "   - Apply and evaluate the performance of a pre-trained Faster R-CNN model for object detection tasks.\n",
    "   - Develop skills in preparing data, training models, and interpreting results in object detection and segmentation tasks.\n",
    "\n",
    "**Lecture Topics:**\n",
    "- **Segmentation**: Introduction to image segmentation, per-pixel prediction, U-Net architecture, applications in medical imaging, and data preparation for segmentation.\n",
    "- **Object Detection**:\n",
    "  - **One-stage Detection**: YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector).\n",
    "  - **Two-stage Detection**: R-CNN (Region-based Convolutional Neural Networks), Fast R-CNN, Faster R-CNN.\n",
    "\n",
    "**Activities:**\n",
    "- **Hands-on Coding**: \n",
    "  - Implement a U-Net for image segmentation on a provided dataset.\n",
    "  - Apply a pre-trained Faster R-CNN model to detect objects in images.\n",
    "- **Group Discussion**:\n",
    "  - Compare the advantages and disadvantages of one-stage vs. two-stage object detection methods.\n",
    "  - Discuss real-world applications of segmentation and object detection.\n",
    "\n",
    "**Assessment:**\n",
    "- **Homework Assignment**: Implement a U-Net model for image segmentation and evaluate its performance on a test dataset.\n",
    "- **Quiz**: Short quiz on the concepts of image segmentation and object detection, including differences between one-stage and two-stage models.\n",
    "- **Project Proposal**: Students to submit a proposal on how they would use object detection or segmentation in a real-world application.\n",
    "\n",
    "**Textbook Support:**\n",
    "- **Chapter 8 - Object Detection and Segmentation**:\n",
    "  - Segmentation (Section 8.1).\n",
    "  - Image segmentation with U-Net.\n",
    "  - Object detection with bounding boxes (Section 8.4).\n",
    "  - Implementation of Faster R-CNN.\n",
    "\n",
    "**Video Lectures:**\n",
    "- [PyTorch Image Segmentation Tutorial with U-NET: everything from scratch baby](https://www.youtube.com/watch?v=IHq1t7NxS8k)\n",
    "- [Implement and Train U-NET From Scratch for Image Segmentation - PyTorch](https://www.youtube.com/watch?v=HS3Q_90hnDg)\n",
    "\n",
    "**Tutorials:**\n",
    "- **Image Segmentation with U-Net**:\n",
    "  - [PyImageSearch U-Net Tutorial](https://pyimagesearch.com/2019/08/12/u-net-image-segmentation-in-keras/)\n",
    "- **Object Detection with Faster R-CNN**:\n",
    "  - [PyTorch Faster R-CNN Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n",
    "These resources and activities are designed to provide a comprehensive understanding of object detection and segmentation, supported by practical implementations and real-world applications.\n",
    "\n",
    "---\n",
    "#### Week 7: Introduction to Transformers and Attention Mechanisms\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Principles of Attention Mechanisms**:\n",
    "   - Understand the theoretical foundations and principles of attention mechanisms in neural networks.\n",
    "   - Learn how attention mechanisms improve the performance of neural networks by allowing the model to focus on relevant parts of the input data.\n",
    "\n",
    "2. **Transformer Architecture**:\n",
    "   - Comprehend the architecture of transformer models, including components such as multi-head attention, positional encoding, and feed-forward networks.\n",
    "   - Explore the advantages of transformers over traditional RNNs and CNNs, particularly in handling sequential data and long-range dependencies.\n",
    "\n",
    "3. **Applications of Transformers**:\n",
    "   - Discover the various applications of transformer models in computer vision and natural language processing.\n",
    "   - Learn about specific use cases such as image classification, language translation, and text summarization using transformers.\n",
    "\n",
    "4. **Hands-on Implementation**:\n",
    "   - Implement a simple transformer model using PyTorch.\n",
    "   - Apply attention mechanisms within a convolutional neural network (CNN) and evaluate their impact on performance.\n",
    "\n",
    "5. **Sequence-to-Sequence Learning**:\n",
    "   - Understand the sequence-to-sequence (Seq2Seq) paradigm and its application in tasks like machine translation and arithmetic prediction.\n",
    "   - Implement a Seq2Seq example to predict sums of three or four-digit numbers, demonstrating the practical use of transformers in handling sequential data.\n",
    "\n",
    "These learning objectives aim to provide a thorough understanding of attention mechanisms and transformer models, along with practical skills in implementing and applying these concepts using PyTorch.\n",
    "\n",
    "**Lecture Topics:**\n",
    "- Introduction to attention mechanisms\n",
    "- Overview of transformer architecture\n",
    "- Applications of transformers in computer vision\n",
    "- Sequence to Sequence example - predicting sums of three or four digit numbers\n",
    "\n",
    "**Activities:**\n",
    "- Implementing attention mechanisms in a CNN\n",
    "- Exploring transformer-based models for image classification\n",
    "\n",
    "**Assessments:**\n",
    "- Quiz on attention mechanisms and transformers\n",
    "- Homework: Implement a transformer model for image classification\n",
    "\n",
    "**Textbook Support:**\n",
    "- Chapter 12: Transformers\n",
    "\n",
    "**Video Lectures:**\n",
    "- [Attention Mechanisms in Neural Networks](https://www.youtube.com/watch?v=Wwkvz-_6XAo) - Introduction to attention mechanisms.\n",
    "- [Transformers Explained](https://www.youtube.com/watch?v=U0s0f995w14) - Detailed video on transformer architecture.\n",
    "\n",
    "**Tutorials:**\n",
    "- [Implementing Attention Mechanisms](https://www.youtube.com/watch?v=BR9h47Jtqyw) - Coding attention mechanisms in neural networks.\n",
    "\n",
    "**Other Resources:**\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "---\n",
    "#### Week 8: Introduction to Hugging Face and Transformers\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Introduction to Hugging Face Library**:\n",
    "   - Understand the purpose and features of the Hugging Face library.\n",
    "   - Explore the different components and tools provided by Hugging Face for natural language processing (NLP) tasks.\n",
    "\n",
    "2. **Pre-trained Transformer Models**:\n",
    "   - Learn about pre-trained transformer models and their advantages in NLP applications.\n",
    "   - Understand how to leverage pre-trained models for various tasks, reducing the need for extensive training from scratch.\n",
    "\n",
    "3. **Implementing Transformer Models with Hugging Face**:\n",
    "   - Gain hands-on experience in implementing a simple transformer model using the Hugging Face library.\n",
    "   - Learn how to fine-tune a pre-trained transformer model on a specific dataset to improve its performance for a particular task.\n",
    "\n",
    "4. **Exploring Hugging Face Documentation and Tutorials**:\n",
    "   - Navigate and utilize the Hugging Face documentation to understand the usage of different functions and classes.\n",
    "   - Follow tutorials to build practical skills in working with the Hugging Face library.\n",
    "\n",
    "**Lecture Topics:**\n",
    "- Overview of Hugging Face library\n",
    "- Pre-trained transformer models\n",
    "\n",
    "**Activities:**\n",
    "- Exploring Hugging Face documentation and tutorials\n",
    "- Implementing a simple transformer model using Hugging Face\n",
    "\n",
    "**Assessments:**\n",
    "\n",
    "1. **Quiz on Hugging Face Basics**:\n",
    "   - Assess students' understanding of the Hugging Face library's components, tools, and functionalities.\n",
    "   - Include questions on the advantages of using pre-trained transformer models and the process of fine-tuning them.\n",
    "\n",
    "2. **Homework: Fine-tune a Pre-trained Transformer Model Using Hugging Face**:\n",
    "   - Provide a dataset and instructions for fine-tuning a pre-trained transformer model using the Hugging Face library.\n",
    "   - Evaluate the students' ability to apply the concepts learned in class to customize a model for a specific task and assess its performance.\n",
    "\n",
    "**Textbook Support:**\n",
    "- Chapter 1: Hello Transformers\n",
    "- Chapter 2: Text Classification\n",
    "\n",
    "**Video Lectures:**\n",
    "- [Introduction to Hugging Face Transformers](https://www.youtube.com/watch?v=kXg8iE0pBwE) - Overview of Hugging Face library.\n",
    "- [Using Pre-trained Models in Hugging Face](https://www.youtube.com/watch?v=M1CBQS9xHB8) - Tutorial on leveraging pre-trained models.\n",
    "\n",
    "**Online Courses:**\n",
    "- [Hugging Face Course](https://huggingface.co/course/chapter1) - Free course on transformers and Hugging Face library.\n",
    "\n",
    "---\n",
    "\n",
    "#### Week 9: Advanced Transformer Models for NLP\n",
    "\n",
    "**Lecture Topics:**\n",
    "- BERT, GPT, and other advanced transformer models\n",
    "- Applications of transformers in NLP\n",
    "\n",
    "**Activities:**\n",
    "- Fine-tuning BERT for text classification\n",
    "- Implementing a text generation model using GPT\n",
    "\n",
    "**Assessments:**\n",
    "- Quiz on advanced transformer models\n",
    "- Homework: Experiment with different transformer models for a specific NLP task\n",
    "\n",
    "**Textbook Support:**\n",
    "- Chapter 3: Transformer Anatomy\n",
    "- Chapter 5: Text Generation\n",
    "\n",
    "**Video Lectures:**\n",
    "- [BERT Explained](https://www.youtube.com/watch?v=8kLLg54h7fQ) - Detailed explanation of BERT model.\n",
    "- [GPT-3: Language Models are Few-Shot Learners](https://www.youtube.com/watch?v=zmv5MIlksg8) - Overview of GPT-3.\n",
    "\n",
    "**Tutorials:**\n",
    "- [Fine-Tuning BERT for Text Classification](https://www.youtube.com/watch?v=ehjewW0e1a8) - Step-by-step guide to fine-tuning BERT.\n",
    "\n",
    "\n",
    "#### Week 10: NLP Applications and Transfer Learning with Transformers\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Understanding Transfer Learning in NLP**:\n",
    "   - Comprehend the concept of transfer learning and its significance in natural language processing (NLP).\n",
    "   - Learn how pre-trained transformer models can be fine-tuned for specific NLP tasks to improve performance and reduce training time.\n",
    "\n",
    "2. **Applications of Transformers in NLP**:\n",
    "   - Explore various NLP tasks where transformers are applied, such as sentiment analysis, named entity recognition (NER), text classification, and more.\n",
    "   - Understand the benefits and challenges of using transformers in different NLP applications.\n",
    "\n",
    "3. **Sentiment Analysis with Transformers**:\n",
    "   - Implement a sentiment analysis model using pre-trained transformers and the Hugging Face library.\n",
    "   - Learn how to preprocess data, fine-tune the model, and evaluate its performance on a sentiment analysis task.\n",
    "\n",
    "4. **Named Entity Recognition (NER) Using Hugging Face Models**:\n",
    "   - Understand the principles of named entity recognition and its importance in extracting structured information from text.\n",
    "   - Implement an NER model using Hugging Face's pre-trained transformers and fine-tune it for a specific dataset.\n",
    "\n",
    "**Lecture Topics:**\n",
    "- Transfer learning in NLP\n",
    "- Applications of transformers in various NLP tasks\n",
    "\n",
    "**Activities:**\n",
    "- Sentiment analysis with transformers\n",
    "- Named entity recognition using Hugging Face models\n",
    "\n",
    "**Assessments:**\n",
    "1. **Quiz on NLP Applications**:\n",
    "   - Assess students' understanding of the concepts and applications of transformers in various NLP tasks.\n",
    "   - Include questions on transfer learning, sentiment analysis, named entity recognition, and other NLP applications discussed in the lectures.\n",
    "\n",
    "2. **Homework: Develop an NLP Application Using Hugging Face**:\n",
    "   - **Assignment Details**:\n",
    "     - Provide students with a dataset for an NLP task such as sentiment analysis, named entity recognition, or text classification.\n",
    "     - Instruct students to choose an appropriate pre-trained transformer model from the Hugging Face library.\n",
    "     - Guide students through the process of fine-tuning the chosen model on the provided dataset, including data preprocessing, model training, and evaluation.\n",
    "     - Require students to submit a report detailing their approach, the model's performance metrics, and any challenges encountered during the implementation.\n",
    "\n",
    "   - **Evaluation Criteria**:\n",
    "     - **Correctness**: Ensure the students have correctly implemented the fine-tuning process and the model functions as expected.\n",
    "     - **Performance**: Evaluate the performance of the model based on metrics appropriate for the task (e.g., accuracy, F1 score, precision, recall).\n",
    "     - **Clarity**: Assess the clarity and comprehensiveness of the report, including explanations of the chosen approach, data preprocessing steps, and evaluation results.\n",
    "     - **Innovation**: Give additional credit for innovative approaches or optimizations that improve the model's performance or efficiency.\n",
    "\n",
    "**Textbook Support:**\n",
    "- Chapter 13: Transfer Learning\n",
    "- Chapter 4: Multilingual Named Entity Recognition\n",
    "\n",
    "**Video Lectures:**\n",
    "- [Applications of Transformers in NLP](https://www.youtube.com/watch?v=UnxcYRNBMIw) - Use cases of transformers in NLP.\n",
    "- [Transfer Learning in NLP](https://www.youtube.com/watch?v=ViDLgkHh0Pw) - Practical guide to transfer learning in NLP.\n",
    "\n",
    "**Tutorials:**\n",
    "- [Sentiment Analysis with Transformers](https://www.youtube.com/watch?v=nXZp759E2-Y) - Implementing sentiment analysis using Hugging Face models.\n",
    "\n",
    "---\n",
    "### Week 11: Advanced Techniques in NLP with Transformers\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Fine-Tuning NLP Models**:\n",
    "   - Understand the principles and techniques of fine-tuning pre-trained transformer models for specific NLP tasks.\n",
    "   - Learn how to adjust model parameters and hyperparameters to optimize performance for a custom dataset.\n",
    "\n",
    "2. **Optimization Strategies**:\n",
    "   - Explore advanced optimization strategies to enhance the performance and efficiency of NLP models.\n",
    "   - Understand techniques such as learning rate scheduling, gradient clipping, and mixed precision training.\n",
    "\n",
    "3. **Advanced NLP Techniques**:\n",
    "   - Implement advanced NLP techniques using transformers, such as text summarization, question answering, and sequence-to-sequence learning.\n",
    "   - Learn how to apply transfer learning to solve complex NLP tasks.\n",
    "\n",
    "4. **Practical Implementation Skills**:\n",
    "   - Gain hands-on experience in fine-tuning a transformer model on a custom NLP dataset using the Hugging Face library.\n",
    "   - Develop skills in debugging and troubleshooting issues that arise during the fine-tuning process.\n",
    "\n",
    "**Assessments:**\n",
    "\n",
    "1. **Quiz on Advanced NLP Techniques**:\n",
    "   - **Quiz Content**:\n",
    "     - Assess students' understanding of fine-tuning techniques and optimization strategies for transformer models.\n",
    "     - Include questions on advanced NLP applications such as text summarization, question answering, and sequence-to-sequence tasks.\n",
    "     - Test knowledge of specific methods for improving model efficiency and performance.\n",
    "\n",
    "   - **Evaluation Criteria**:\n",
    "     - Correctness of answers regarding key concepts and techniques.\n",
    "     - Ability to apply theoretical knowledge to practical scenarios.\n",
    "\n",
    "2. **Homework: Fine-Tune an NLP Model for a Custom Task**:\n",
    "   - **Assignment Details**:\n",
    "     - Provide students with a custom NLP dataset related to tasks such as text summarization, question answering, or another advanced NLP application.\n",
    "     - Instruct students to select an appropriate pre-trained transformer model from the Hugging Face library.\n",
    "     - Guide students through the process of fine-tuning the model on the provided dataset, including data preprocessing, model training, and performance evaluation.\n",
    "     - Require students to submit a report detailing their approach, the fine-tuning process, the model's performance metrics, and any challenges encountered during implementation.\n",
    "\n",
    "   - **Evaluation Criteria**:\n",
    "     - **Correctness**: Ensure the students have correctly implemented the fine-tuning process and the model functions as expected.\n",
    "     - **Performance**: Evaluate the performance of the model based on metrics appropriate for the task (e.g., ROUGE score for summarization, accuracy for question answering).\n",
    "     - **Clarity**: Assess the clarity and comprehensiveness of the report, including explanations of the chosen approach, data preprocessing steps, fine-tuning process, and evaluation results.\n",
    "     - **Innovation**: Give additional credit for innovative approaches or optimizations that improve the model's performance or efficiency.\n",
    "\n",
    "**Textbook Support:**\n",
    "- Chapter 6: Summarization\n",
    "- Chapter 8: Making Transformers Efficient in Production\n",
    "\n",
    "**Video Lectures:**\n",
    "- [Fine-Tuning and Optimizing NLP Models](https://www.youtube.com/watch?v=iwjyOzLWB6Q) - Advanced techniques for fine-tuning NLP models.\n",
    "- [Advanced NLP with Transformers](https://www.youtube.com/watch?v=UnxcYRNBMIw) - Further insights into advanced NLP techniques.\n",
    "\n",
    "**Tutorials:**\n",
    "- [Fine-Tuning a Transformer for a Custom NLP Dataset](https://www.youtube.com/watch?v=viTL3ghJfZQ) - Practical guide to fine-tuning transformers for custom tasks.\n",
    "\n",
    "---\n",
    "### Week 12: Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs)\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Understanding Retrieval-Augmented Generation (RAG)**:\n",
    "   - Comprehend the concept and significance of Retrieval-Augmented Generation in enhancing the capabilities of large language models (LLMs).\n",
    "   - Learn about the key components of RAG: the retriever, the generator, and their integration.\n",
    "\n",
    "2. **Key Components and Workflow**:\n",
    "   - Understand the role of the retriever in retrieving relevant documents or passages based on a query.\n",
    "   - Understand the role of the generator in generating contextually accurate and relevant responses.\n",
    "   - Learn how the retriever and generator work together to form a RAG pipeline.\n",
    "\n",
    "3. **Applications of RAG**:\n",
    "   - Explore various applications of RAG in NLP tasks such as question answering, text summarization, and more.\n",
    "   - Analyze real-world use cases where RAG has been effectively implemented.\n",
    "\n",
    "4. **Implementing RAG Models**:\n",
    "   - Gain hands-on experience in implementing a basic RAG model using Hugging Face Transformers.\n",
    "   - Learn how to set up a retriever and generator and integrate them to form a complete RAG pipeline.\n",
    "   - Experiment with applying RAG to specific tasks like question answering.\n",
    "\n",
    "**Assessments:**\n",
    "\n",
    "1. **Quiz on RAG Concepts and Implementation**:\n",
    "   - **Quiz Content**:\n",
    "     - Assess students' understanding of the principles and components of RAG.\n",
    "     - Include questions on the roles of the retriever and generator, their integration, and the workflow of a RAG pipeline.\n",
    "     - Test knowledge on applications of RAG in various NLP tasks.\n",
    "\n",
    "   - **Evaluation Criteria**:\n",
    "     - Correctness of answers regarding the concepts and components of RAG.\n",
    "     - Ability to explain the workflow and applications of RAG.\n",
    "\n",
    "2. **Homework: Develop a Simple RAG-Based Application Using Hugging Face**:\n",
    "   - **Assignment Details**:\n",
    "     - Provide students with a specific NLP task, such as question answering or text summarization.\n",
    "     - Instruct students to choose appropriate datasets and pre-trained models from the Hugging Face library.\n",
    "     - Guide students through setting up a retriever to fetch relevant documents or passages based on the given task.\n",
    "     - Instruct students to configure a generator to produce responses or summaries based on the retrieved information.\n",
    "     - Require students to integrate the retriever and generator to form a complete RAG pipeline.\n",
    "     - Ask students to submit a report detailing their approach, the integration process, performance metrics, and any challenges encountered.\n",
    "\n",
    "   - **Evaluation Criteria**:\n",
    "     - **Correctness**: Ensure students have correctly implemented the RAG pipeline and the components work as expected.\n",
    "     - **Performance**: Evaluate the performance of the RAG model based on metrics appropriate for the task (e.g., accuracy for question answering, ROUGE score for summarization).\n",
    "     - **Clarity**: Assess the clarity and comprehensiveness of the report, including explanations of the chosen approach, the integration process, and evaluation results.\n",
    "     - **Innovation**: Give additional credit for innovative approaches or optimizations that improve the model's performance or efficiency.\n",
    "\n",
    "**Textbook Support:**\n",
    "\n",
    "**Video Lectures:**\n",
    "- [LangChain: Chat with Your Data](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/) Use LangChain and RAG to build a chatbot that understands the context from your documents.\n",
    "\n",
    "---\n",
    "#### Weeks 13-14: Project Weeks\n",
    "\n",
    "**Lecture Topics:**\n",
    "- Project planning and development\n",
    "- Final project presentations\n",
    "\n",
    "**Activities:**\n",
    "- Work on final projects\n",
    "- Present and critique final projects\n",
    "\n",
    "**Assessments:**\n",
    "- Final project presentations and reports\n",
    "\n",
    "**Project Options:**\n",
    "- **Computer Vision Project:**\n",
    "  - Task: Build and fine-tune a CNN or Vision Transformer (ViT) for image classification.\n",
    "  - Dataset: CIFAR-10, MNIST, or any accessible image dataset.\n",
    "  - Resources: Google Colab, free datasets, PyTorch, Hugging Face Transformers.\n",
    "- **NLP Project:**\n",
    "  - Task: Develop an NLP application using transformers (e.g., text classification, named entity recognition, sentiment analysis).\n",
    "  - Dataset: IMDb reviews, AG News, or any accessible text dataset.\n",
    "  - Resources: Google Colab, free datasets, Hugging Face Transformers.\n",
    "\n",
    "**Project Planning and Development Resources:**\n",
    "- [Guidelines for Planning Machine Learning Projects](https://www.youtube.com/watch?v=9SZ2tQkQDww) - Tips on planning and executing ML projects.\n",
    "- [Google Colab Tutorials](https://www.youtube.com/watch?v=inN8seMm7UI) - Getting started with Google Colab for your projects.\n",
    "\n",
    "**Computer Vision Project:**\n",
    "- [Building and Fine-Tuning a CNN](https://www.youtube.com/watch?v=oK7Fi8gL3GA) - Tutorial on building and fine-tuning a CNN.\n",
    "- [Vision Transformers (ViT) Tutorial](https://www.youtube.com/watch?v=1q3LEadIk3I) - Implementing ViT for image classification.\n",
    "\n",
    "**NLP Project:**\n",
    "- [Developing an NLP Application with Transformers](https://www.youtube.com/watch?v=ehjewW0e1a8) - Tutorial on building an NLP application using Hugging Face transformers.\n",
    "- [Named Entity Recognition with Transformers](https://www.youtube.com/watch?v=3wvFL6D53AY) - Step-by-step guide to implementing NER.\n",
    "\n",
    "**Miscellaneous Resources for Part 2**\n",
    " \n",
    "https://youtu.be/SZorAJ4I-sA?si=WSvlE5EDSM0lcSO9\n",
    "\n",
    "https://youtu.be/bCz4OMemCcA?si=O0JG_SIfLDCIAZ6I\n",
    "https://youtu.be/90mGPxR2GgY?si=zVvGBy3yPWm6VJ2N\n",
    "(These videos by Umar Jamil are very good)\n",
    "\n",
    "https://bbycroft.net/llm\n",
    "https://huggingface.co/spaces/exbert-project/exbert\n",
    "https://arxiv.org/pdf/1910.05276\n",
    "\n",
    "https://youtu.be/t45S_MwAcOw?si=jHgwRAEKZDsOW3xi\n",
    " \n",
    "Hugging Face encoder/decode videos\n",
    "\n",
    "https://youtu.be/MUqNwgPjJvQ?si=e2lFDk1gjgOluStP\n",
    "https://youtu.be/d_ixlCubqQw?si=x88NFT7Ce0MgKu0M\n",
    "\n",
    "This one by Jeremy Howard has lots of good ideas. I often show the video from the 17:21 mark for weaker audiences:\n",
    "https://youtu.be/jkrNMKz9pWU?si=-gdapXOPSY5S0Ikz\n",
    "\n",
    "And of course the 3Blue1Brown videos are superb\n",
    "https://youtu.be/wjZofJX0v4M?si=-hwfraw0EONYBuiC\n",
    "https://youtu.be/eMlx5fFNoYc?si=w5zr5VKyI70k_wcW\n",
    "\n",
    "Stuff on word embeddings like word2vec\n",
    "https://youtu.be/gQddtTdmG_8?si=4_L6tXrFiO5y7QXG\n",
    "https://youtu.be/f7o8aDNxf7k?si=rmfwG4GrM_FSM1zi\n",
    "\n",
    "This one is long, but Manning is a genius\n",
    "https://youtu.be/ERibwqs9p38?si=YbYUWjUf1Rf0U7Qz\n",
    "\n",
    "A Turing Lecture about generative AI for general audiences:\n",
    "https://youtu.be/fwaDtRbfioU?si=W_UxNiB5\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def embed_youtube_video(video_id):\n",
    "    \"\"\"\n",
    "    Embed a YouTube video in a Jupyter Notebook and provide a link to open it in a new tab.\n",
    "    \n",
    "    Parameters:\n",
    "    video_id (str): The ID of the YouTube video.\n",
    "    \"\"\"\n",
    "    html_code = f\"\"\"\n",
    "    You can watch the video below or <a href=\"https://www.youtube.com/watch?v={video_id}\" target=\"_blank\">click here to open in a new tab</a>.<br>\n",
    "    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/{video_id}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "    \"\"\"\n",
    "    display(HTML(html_code))\n",
    "\n",
    "# Example usage:\n",
    "embed_youtube_video(\"dQw4w9WgXcQ\")\n",
    "\n",
    "# Embedding a YouTube Video with an External Link\n",
    "\n",
    "You can watch the video below or [click here to open in a new tab](https://www.youtube.com/watch?v=dQw4w9WgXcQ){target=\"_blank\"}.\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
