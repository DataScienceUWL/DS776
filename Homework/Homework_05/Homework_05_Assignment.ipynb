{
 "cells": [
  {
   "cell_type": "code",
   "source": "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n%run ../../Lessons/Course_Tools/auto_update_introdl.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Homework 05 Assignment\n**Name:** [Student Name Here]  \n**Total Points:** 40\n\n## Submission Checklist\n- [ ] All code cells executed with output saved\n- [ ] All questions answered\n- [ ] Notebook converted to HTML (use the Homework_05_Utilities notebook)\n- [ ] Canvas notebook filename includes `_GRADE_THIS_ONE`\n- [ ] Files uploaded to Canvas\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Transfer Learning\n\nIn this homework you'll experiment with applying transfer learning for fine-grained classification using the Flowers102 dataset in torchvision.datasets. Fine-grained classification is when you have many categories or classes that are similar like related species of flowers. Or, for example, trying to distinguish breeds of dogs as opposed to cats, dogs, and foxes.\n\nNote: we were able to train all the models described in this homework in about 40 minutes on the T4 Compute Server. The ConvNeXt model was the biggest and took the most time."
  },
  {
   "cell_type": "code",
   "source": "# === YOUR IMPORTS HERE ===\n# Add any additional imports you need below this line\n\nfrom introdl.utils import config_paths_keys\n\n# Configure paths\npaths = config_paths_keys()\nDATA_PATH = paths['DATA_PATH']\nMODELS_PATH = paths['MODELS_PATH']\n# === END YOUR IMPORTS ===",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Storage Guidance\n\n**Your files MUST be saved in the standard locations** or cleanup utilities won't find them:\n\n| What | Where | Variable |\n|------|-------|----------|\n| Trained models | `Homework_05_Models/` (in this folder) | `MODELS_PATH` |\n| Datasets | `~/home_workspace/data/` | `DATA_PATH` |\n| Pretrained models | `~/home_workspace/downloads/` | `CACHE_PATH` |\n\n**Why this matters:**\n- CoCalc has a **10GB storage limit** for synced files\n- The **Storage_Cleanup.ipynb** notebook (in this folder) helps you free space\n- Files saved elsewhere won't be found by cleanup utilities\n\n**Tip:** Always use `MODELS_PATH / 'filename.pt'` instead of hardcoded paths.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## [5 pts] Data Exploration\n\nFirst, let's explore the Flowers102 dataset to understand what we're working with. Load the dataset, examine the number of classes, display some sample images with their labels, and analyze the dataset size and structure.\n"
  },
  {
   "cell_type": "code",
   "source": "# === YOUR CODE HERE ===\n# TODO: Load the Flowers102 dataset and explore its structure\n# - Load training, validation, and test splits\n# - Print dataset sizes and number of classes\n# - Display sample images with their class labels\n# - Analyze the class distribution\n\n\n# === END YOUR CODE ===",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## [5 pts] Augmentation and DataLoaders\n\nBuild your transforms for training. Remember that for testing and validation the transforms shouldn't add any augmentation. The images should be $224 \\times 224$ when transformed since our pretrained models were trained on Imagenet with the same size images. We used `batch_size = 32` on the T4 Compute Servers. For normalization use the statistics from Imagenet since the pretrained models we are using expect that normalization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === YOUR CODE HERE ===\n# TODO: Create data transforms and DataLoaders\n# - Create training transforms with augmentation (appropriate for fine-grained classification)\n# - Create validation/test transforms without augmentation\n# - Use ImageNet normalization statistics\n# - Create DataLoaders with batch_size=32\n\n\n# === END YOUR CODE ===",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## [5 pts] ResNet50\n\nThe ResNet models establish good baselines for results.\n\nBuild a custom model class for ResNet50 (AI may be helpful here) with an adjustable number of output classes. It should have methods to freeze and unfreeze the backbone. Apply transfer learning instantiating your model with the default Imagenet weights and training with for 5 epochs followed by training for a suitable number of epochs (you may need to experiment). Include graphics or display dataframes to show how the model is converging (at least for the unfrozen training).\n\nUse the training and validation sets here. The test set will be reserved for your final best model.\n\nWhat kind of validation accuracy are you able to achieve? Is the model overfitting?\n\nNote: the training dataset is already pretty small so downsampling it to expedite experimentation isn't a good idea, but you could temporarily reduce the size of the images to say 128x128 in your transforms to get things working, then go back to 224x224 to train your models. All final results should be done with 224x224.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === YOUR CODE HERE ===\n# TODO: Create a custom ResNet50 model class\n# - Load pretrained ResNet50 with ImageNet weights\n# - Replace final classifier layer for 102 flower classes\n# - Add methods to freeze/unfreeze backbone weights\n# - Train with frozen backbone for 5 epochs, then unfreeze and continue training\n\n\n# === END YOUR CODE ===",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "üìù **What validation accuracy did you achieve? Is the model overfitting?**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## [5 pts] EfficientNet V2 Small\n\nEfficientNet models are a modern upgrade to traditional convolutional neural networks, offering improved performance and efficiency. Repeat what you did for ResNet50 for EfficientNet V2 Small. Use AI to search for how to load it in torchvision and how to adapt in your custom model class."
  },
  {
   "cell_type": "code",
   "source": "# === YOUR CODE HERE ===\n# TODO: Create EfficientNet V2 Small model\n# - Load pretrained EfficientNet V2 Small with ImageNet weights\n# - Adapt the model for 102 flower classes\n# - Apply the same two-phase training approach as ResNet50\n\n\n# === END YOUR CODE ===",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## [5 pts] ConvNeXt Small\n\nConvNeXt models are a family of convolutional neural networks that aim to modernize the design of traditional CNNs by incorporating elements from vision transformers. They provide a strong performance baseline for various computer vision tasks. Use transfer learning to train a ConvNeXT Small (not Tiny) model on Flowers102."
  },
  {
   "cell_type": "code",
   "source": "# === YOUR CODE HERE ===\n# TODO: Create ConvNeXt Small model\n# - Load pretrained ConvNeXt Small with ImageNet weights\n# - Adapt the model for 102 flower classes\n# - Apply the same two-phase training approach as previous models\n\n\n# === END YOUR CODE ===",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## [5 pts] ViT Small\n\nVision Transformers (ViTs) are a type of neural network architecture that leverages the transformer model, originally designed for natural language processing, to process image data. Unlike Convolutional Neural Networks (CNNs), which use convolutional layers to capture spatial hierarchies, ViTs divide images into patches and process them as sequences, allowing for global context understanding. ViTs typically require more data to train from scratch compared to CNNs, but they can be effectively used for transfer learning on smaller datasets if the images are similar to those in the Imagenet dataset. We'll learn more about transformer models in the second half of the course.\n\nWe'll use the timm library which doesn't seem to be installed in CoCalc.\nTo use ViT Small from the timm library, you can install timm with the following command:\n```python\n!pip install timm\n```\nThen, load the pre-trained ViT Small model with:\n```python\nimport timm\nmodel = timm.create_model('vit_small_patch16_224', pretrained=True)\n```\n\n(Note: you'll need to copy this code from this markdown cell to a regular code cell for the installation to work correctly.)\n\nThe ViT Small model is pretrained on Imagenet and expects the same size images and same normalization as other models. Typically we fine tune the whole model and don't train with a frozen backbone. The learning rates used are usually smaller, too. Do the same kind of fine tuning as you've done above using OneCycleLR with max_lr = 0.0005. We found that the number of epochs needed was similar to the total number of epochs used in the two-phase training used by our other models."
  },
  {
   "cell_type": "code",
   "source": "# === YOUR CODE HERE ===\n# TODO: Install and use ViT Small from timm library\n# - Install timm library\n# - Create ViT Small model with ImageNet pretrained weights\n# - Fine-tune the whole model (don't use frozen backbone approach)\n# - Use OneCycleLR scheduler with max_lr=0.0005\n\n\n# === END YOUR CODE ===",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## [8 pts] Apply Best Model to Test Data and Evaluate\n\nWrite a brief summary of your investigations above. Include a graph comparing the training metrics from the fine-tuning phases on the validation data from above.\n\nGenerate a classification report comparing the predictions of your best model to the ground truth labels on the test dataset. Summarize the highlights of the report. A confusion matrix display probably isn't helpful because there are so many classes (set `display_confusion=False` if use `evaluate_classifier` from `introdl.utils`.) But you can look at slices of the confusion matrix. Try to identify at least two classes which are being confused by your model and display examples, with proper labels, from those classes."
  },
  {
   "cell_type": "code",
   "source": "# === YOUR CODE HERE ===\n# TODO: Compare all models and evaluate best one on test data\n# - Create comparison plots of validation metrics from all models\n# - Select the best performing model based on validation results\n# - Evaluate the best model on the test dataset\n# - Generate classification report (set display_confusion=False)\n# - Identify and display examples of confused classes\n\n\n# === END YOUR CODE ===",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "üìù **YOUR SUMMARY OF MODEL COMPARISONS:**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## [2 pts] Reflection\n\n1. What, if anything, did you find difficult to understand for this lesson? Why?\n\nüìù **YOUR ANSWER HERE:**\n\n2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n\nüìù **YOUR ANSWER HERE:**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n\nUncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}