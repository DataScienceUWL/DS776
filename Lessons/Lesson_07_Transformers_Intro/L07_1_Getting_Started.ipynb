{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ introdl v1.6.15 already up to date\n"
     ]
    }
   ],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "### DO THIS FIRST\n",
    "\n",
    "Change `force_update=True` in the last line and run the next cell to install an updated course package.  Once it's done restart your kernel and change back to `force_update=False`.  You only need to do this once per server (not once per notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "#### L07_1_Getting_Started_with_NLP Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/oDi5d1FbYBx\" target=\"_blank\">Open Descript version of video in new tab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## A Tiny History of Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) has evolved significantly over the past few decades. Initially, NLP relied heavily on rule-based systems and statistical methods to understand and generate human language. These early approaches, prominent in the 1980s and 1990s, focused on the syntactic structure of text, using techniques such as n-grams and Hidden Markov Models (HMMs) to model language. However, these methods struggled with capturing the semantic meaning and context of words.\n",
    "\n",
    "The introduction of word embeddings in the early 2010s, such as Word2Vec and GloVe, marked a significant advancement in NLP. These embeddings allowed for the representation of words in continuous vector space, capturing semantic relationships between words. This shift enabled more sophisticated models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, to process sequences of text and maintain context over longer passages. RNNs, in particular, played a crucial role in tasks like language translation and sentiment analysis.\n",
    "\n",
    "The advent of transformers in 2017 revolutionized NLP by addressing the limitations of RNNs. Transformers, introduced with the Attention is All You Need paper, utilize self-attention mechanisms to process entire sequences of text simultaneously, allowing for better handling of long-range dependencies and parallelization. This led to the development of powerful models like BERT, GPT, and T5, which have set new benchmarks in various NLP tasks by providing a deeper semantic understanding of text.\n",
    "\n",
    "Transformers have almost entirely supplanted previous approaches to NLP because:\n",
    "\n",
    "1. **Superior Performance:** Models like BERT, GPT, T5, and their successors dominate leaderboards on tasks such as text classification, translation, summarization, and question answering.\n",
    "2. **Pretraining and Transfer Learning:** Unlike traditional methods that required training separate models from scratch for different tasks, transformers leverage large-scale pretraining on vast text corpora and fine-tune efficiently on specific tasks.\n",
    "3. **Self-Attention and Contextual Representations:** Transformers provide rich, context-dependent word representations, whereas earlier models like Word2Vec and GloVe generated static embeddings.\n",
    "4. **Scalability and Adaptability:** With advancements in scaling laws, models can achieve better performance just by increasing their size and training data, an advantage that RNNs and classical machine learning approaches lacked.\n",
    "\n",
    "There are a few areas where older approaches still exist:\n",
    "\n",
    "1. **Small Datasets & Low Compute Environments:** Logistic regression, SVMs, and Lasso-penalized models often remain competitive when data is limited or when computational efficiency is a concern.\n",
    "2. **Domain-Specific Applications:** Some applications, like biomedical text mining, may still rely on domain-specific feature engineering approaches alongside transformers.\n",
    "3. **Traditional ML for Interpretability:** Some NLP applications in finance, healthcare, and legal fields still favor older methods due to the need for interpretability and robustness.\n",
    "\n",
    "However, since transformer models for NLP are now so dominant we will focus exclusively on them in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## NLP Tasks Instead of Transformer Details\n",
    "\n",
    "Transformers are more complicated than the CNNs we saw for computer vision so we're not going to dive as deeply into the details. We will, in Lesson 9 - Transformer Details, learn about some of the nuts and bolts especially the self-attention mechanism that allows transformers to figure out relationships between words and to understand context. Mostly, though, we will focus on the applications of transformers. To this end we'll dive into the open source HuggingFace ecosystem which hosts thousands of NLP models and datasets and makes it quite simple to dive into NLP applications without having to master too much code. All of the newest, biggest open source transformer models are hosted there including those from Meta, Mistral, and Deepseek. The only thing keeping us from running the biggest state-of-the-art models will be lack of compute, but we can run their smaller cousins on the GPU in CoCalc's compute server, a decent gaming GPU, or even a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## API-based LLMs versus Fine-tuning Specialized Models\n",
    "\n",
    "As large language models (LLMs) continue to improve, their use as general NLP task solvers via prompting is increasingly popular, especially when we don't have access to large amounts of training data. In this course, we'll focus on two main approaches to solving NLP tasks:\n",
    "\n",
    "1. **Using LLMs via APIs** (like GPT-4o, Claude, or Gemini through OpenRouter)\n",
    "2. **Fine-tuning specialized transformer models** for specific tasks\n",
    "\n",
    "*(We'll also explore running LLMs locally in Lesson 11, but for Lessons 7-10 and 12 we'll use API-based models and task-specific fine-tuned models.)*\n",
    "\n",
    "### Example: Text Classification\n",
    "\n",
    "For a text-classification task, you could choose:\n",
    "\n",
    "**LLM via API (GPT-4o, Claude, Gemini, etc.)**\n",
    "- When you need **a quick, general-purpose classifier** without training a model\n",
    "- When **zero-shot or few-shot classification** (via prompting) is sufficient\n",
    "- When categories may evolve frequently, making retraining impractical\n",
    "- When you don't have a large labeled dataset\n",
    "- Example: Categorizing support tickets by topic\n",
    "\n",
    "**Fine-tune BERT / RoBERTa / DistilBERT**\n",
    "- When you have a **moderate to large labeled dataset** and need **high accuracy**\n",
    "- When you need **fast inference at scale**, as fine-tuned models are more efficient than large LLMs\n",
    "- When your classification task requires **domain-specific adaptation**\n",
    "- When you need **very low latency** or **predictable costs**\n",
    "- Example: Sentiment analysis on customer feedback in a specific industry\n",
    "\n",
    "**Note on terminology:** Zero-shot classification means classifying text without seeing any examples - the LLM just gets a prompt with the possible categories. Few-shot classification means providing a small number of examples in the LLM prompt to guide the model's behavior.\n",
    "\n",
    "### Choosing the Right Approach\n",
    "\n",
    "**Use API-based LLMs when:**\n",
    "- You need **quick, adaptable solutions** without training infrastructure\n",
    "- You **don't have much labeled data** for fine-tuning\n",
    "- You want to **experiment rapidly** with different task formulations\n",
    "- Task requirements may change frequently\n",
    "- You're prototyping or building proof-of-concepts\n",
    "\n",
    "**Fine-tune a specialized model when:**\n",
    "- You have **domain-specific labeled data** and need **high accuracy**\n",
    "- You need **very fast inference** or processing at large scale\n",
    "- You need **predictable costs** (no per-token API charges)\n",
    "- You require **consistent, structured outputs**\n",
    "- Latency is critical (milliseconds matter)\n",
    "\n",
    "### Understanding Data Privacy with API-based LLMs\n",
    "\n",
    "A common concern with API-based LLMs is: **\"Will my data be used to train the model?\"** or **\"Is my sensitive data secure?\"** The answer depends on the provider and the agreements in place.\n",
    "\n",
    "**Privacy Protections Available:**\n",
    "\n",
    "Most major LLM providers now offer enterprise-grade privacy protections:\n",
    "- **Zero Data Retention (ZDR):** Your API requests are not stored or logged after processing\n",
    "- **Data Processing Agreements (DPAs):** Legal contracts preventing use of your data for model training\n",
    "- **HIPAA and SOC 2 Compliance:** Meeting healthcare and security standards for regulated industries\n",
    "- **Private Deployments:** Dedicated instances in your own cloud environment (e.g., Azure OpenAI, AWS Bedrock)\n",
    "- **Regional Data Residency:** Keep data within specific geographic boundaries (e.g., EU-only processing)\n",
    "\n",
    "**Examples:**\n",
    "- **OpenAI API:** Has a default policy not to use API data for training. Enterprise customers can enable additional protections.\n",
    "- **Azure OpenAI Service:** Fully isolated deployments in your Azure subscription with complete data control\n",
    "- **Google Vertex AI:** Private endpoints with data residency controls and enterprise security\n",
    "- **Anthropic Claude:** API data not used for training; enterprise options for additional controls\n",
    "\n",
    "**When API Privacy May Not Be Enough:**\n",
    "\n",
    "Even with these protections, there are situations where API-based solutions may not be acceptable:\n",
    "- **Air-gapped environments:** Systems physically isolated from external networks (e.g., classified government systems)\n",
    "- **Extreme regulatory restrictions:** Some industries may prohibit any external data transmission regardless of agreements\n",
    "- **Zero-trust requirements:** Organizations that cannot accept any third-party processing, even contractually protected\n",
    "- **Competitive intelligence:** Proprietary algorithms or trade secrets that cannot be exposed, even with DPAs\n",
    "\n",
    "In these cases, running models locally (Lesson 11) or fine-tuning your own specialized models on internal infrastructure becomes necessary.\n",
    "\n",
    "**Bottom Line:** For most educational, research, and business applications, modern API providers offer sufficient privacy protections through contractual agreements and technical controls. Understanding your specific regulatory requirements and risk tolerance will guide your choice.\n",
    "\n",
    "### Course Approach (with changes for Fall 2025)\n",
    "\n",
    "For each NLP task in Lessons 7-10 and 12, we'll explore both API-based LLM approaches and fine-tuned specialized models. In Lesson 11, we'll dive deeper into text generation and demonstrate running LLMs locally for complete control and privacy.\n",
    "\n",
    "*You may notice some slight differences between the lesson notebooks and some of the recorded videos in the NLP portion of the class.  When the course was developed we ran very small LLMs locally on the compute servers in CoCalc.  It was slow and we were limited to very slow models.  This semester we're providing some credits on OpenRouter to allow you to use various models (even the biggest, newest ones ... but you may need to pay for more credits).  We'll explain more about OpenRouter below.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## OpenRouter API and Your Course API Keys\n",
    "\n",
    "For this course, we've set up access to Large Language Models through **OpenRouter**, a unified API that provides access to all major commercial models (GPT-4o, Claude, Gemini) and most open-weight models (Llama, Mistral, DeepSeek, Qwen, and many more). This means you can experiment with different models using a single API interface.\n",
    "\n",
    "### Your API Credit\n",
    "\n",
    "**Each student has been provided with $15 in OpenRouter API credit.** This should be more than sufficient to complete all coursework if you use small and medium-sized models appropriately. For reference:\n",
    "\n",
    "- **Small models** (like `gemini-flash-lite`, `llama-3.2-3b`, `gpt-4o-mini`): Very inexpensive, typically $0.075-0.15 per million input tokens\n",
    "- **Medium models** (like `gemini-flash`, `claude-haiku`): Moderate cost, good quality\n",
    "- **Premium models** (like `gpt-4o`, `claude-sonnet`, `o3-mini`): Higher cost, best quality\n",
    "\n",
    "We recommend using **`gemini-flash-lite`** as your default model for coursework - it's fast, inexpensive, and produces good results for learning tasks.\n",
    "\n",
    "If you want to experiment beyond the course assignments or try premium models, you can always purchase your own OpenRouter API key and load it with whatever credit you choose.\n",
    "\n",
    "### Checking Your Remaining Credit\n",
    "\n",
    "You can check your remaining OpenRouter credit using the `llm_get_credits()` function from the course package. This will show you how much of your $15 credit remains:\n",
    "\n",
    "```python\n",
    "from introdl.nlp import llm_get_credits\n",
    "\n",
    "credits = llm_get_credits()\n",
    "print(f\"Remaining credit: ${credits['usage']:.2f} of ${credits['limit']:.2f}\")\n",
    "print(f\"Credit remaining: ${credits['limit'] - credits['usage']:.2f}\")\n",
    "```\n",
    "\n",
    "### Your API Keys Are Already Configured\n",
    "\n",
    "Your OpenRouter API key has already been distributed to your CoCalc project and is stored in:\n",
    "```\n",
    "~/home_workspace/api_keys.env\n",
    "```\n",
    "\n",
    "When you run `config_paths_keys()` in your import cell (as shown below), this API key will be automatically loaded and available for use with `llm_generate()`. You don't need to do anything else!\n",
    "\n",
    "**Security Note:** Never commit your `api_keys.env` file to git or share it publicly. The file is stored in `home_workspace` which should not be tracked by version control.\n",
    "\n",
    "### Exploring Available Models\n",
    "\n",
    "The course package includes 16 carefully curated models covering a range of capabilities and price points. You can see them all with `llm_list_models()`, which we'll demonstrate shortly.\n",
    "\n",
    "**Want to try models beyond our curated list?** OpenRouter provides access to hundreds of models! You can:\n",
    "\n",
    "1. **Browse all available models** at: https://openrouter.ai/models\n",
    "2. **Use any model** by providing its full OpenRouter model ID\n",
    "\n",
    "For example, to use OpenAI's new GPT-5-nano model (not in our curated list), you would use:\n",
    "\n",
    "```python\n",
    "response = llm_generate('openai/gpt-5-nano', \"Your prompt here\")\n",
    "```\n",
    "\n",
    "The full model ID format is typically `provider/model-name` (e.g., `openai/gpt-5-nano`, `anthropic/claude-opus-4.1`, `google/gemini-2.5-pro`).\n",
    "\n",
    "**Note:** Models outside our curated list won't show pricing or metadata with `llm_list_models()`, but they'll work fine if you provide the correct model ID from the OpenRouter website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Using `llm_generate` with OpenRouter\n",
    "\n",
    "The course package provides a simple, unified interface for working with LLMs through the `llm_generate()` function. This function handles all the complexity of API calls, cost tracking, and response formatting.\n",
    "\n",
    "### Setting Up and Checking Your Credit\n",
    "\n",
    "First, let's import the necessary functions, configure our environment, and check your OpenRouter credit balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "from introdl.utils import config_paths_keys, wrap_print_text\nfrom introdl.nlp import (\n    llm_generate, llm_list_models, llm_get_credits,\n    init_cost_tracking, display_markdown, show_pricing_table\n)\n\n# Configure paths and load API keys\npaths = config_paths_keys()\n\n# Initialize LLM cost tracking system\ninit_cost_tracking()\n\n# Wrap print to format text nicely at 120 characters\nprint = wrap_print_text(print, width=120)\n\n# Check your OpenRouter credit balance\ncredits = llm_get_credits()\nprint(f\"OpenRouter Credit Status:\")\nprint(f\"  Total limit: ${credits['limit']:.2f}\")\nprint(f\"  Used so far: ${credits['usage']:.2f}\")\nprint(f\"  Remaining:   ${credits['limit'] - credits['usage']:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "Now let's try a simple text generation example. The new `llm_generate()` API is very straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "# Simple text generation\n",
    "response = llm_generate('gemini-flash-lite', \"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Using System Prompts\n",
    "\n",
    "System prompts help guide the model's behavior and tone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, ye landlubber! Ye want to know about the vast, dark abyss I call\n",
      "home? Well, gather 'round, and I'll spin ye a yarn of three cosmic curiosities\n",
      "that'll shiver yer timbers:\n",
      "\n",
      "1.  **There be more stars in the universe than grains o' sand on all the beaches\n",
      "o' Earth!** Aye, ye heard me right. If ye could count every single speck o' sand\n",
      "on every sandy shore, from the Caribbean to the farthest reaches o' the Orient,\n",
      "ye'd still be short. The universe be a truly massive place, full o' glittering\n",
      "celestial bodies, each one a sun in its own right, likely with planets o' its\n",
      "own. Makes ye feel like a tiny barnacle on a giant galleon, don't it?\n",
      "\n",
      "2.  **Venus spins the wrong way and has a day longer than its year!** Imagine\n",
      "this, matey: While most planets be spinnin' like a well-oiled compass, Venus\n",
      "decides to be a contrarian and spins backward, or retrograde, as the fancy folk\n",
      "call it. And as if that weren't peculiar enough, a single day on Venus ‚Äì from\n",
      "sunrise to sunrise ‚Äì takes a whopping 243 Earth days. But a Venusian year, the\n",
      "time it takes to orbit the sun, is a mere 225 Earth days. So, it's always a bit\n",
      "of a long, slow day on that sweltering hot planet. Truly a cosmic oddity!\n",
      "\n",
      "3.  **There's a giant cloud o' alcohol in space!** Now, before ye start gettin'\n",
      "any wild ideas, it ain't the kind ye can bottle and drink. 'Tis a massive cloud\n",
      "called Sagittarius B2, located near the center o' our Milky Way galaxy, and it's\n",
      "absolutely chock-full o' ethyl alcohol, the same stuff ye find in yer grog! It's\n",
      "not enough to get a whole galaxy tipsy, mind ye, but it's a hefty amount of the\n",
      "good stuff floatin' around in the void. Makes ye wonder what other peculiar\n",
      "brews are out there, eh?\n",
      "\n",
      "So there ye have it, three tidbits from the cosmos to tickle yer fancy. Now, if\n",
      "ye'll excuse me, I've got some nebulae to explore and some black holes to avoid.\n",
      "Arrr!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful AI assistant who is also sarcastic and talks like a pirate.\"\n",
    "\n",
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Tell me three interesting facts about space.\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Displaying Markdown Output\n",
    "\n",
    "Many LLM responses use markdown formatting. You can display them nicely using `display_markdown()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a short bullet-point list of tips for learning machine learning:\n",
       "\n",
       "*   **Build a strong foundation in math:** Focus on linear algebra, calculus, probability, and statistics.\n",
       "*   **Master programming fundamentals:** Python is the industry standard, so get comfortable with its syntax and libraries like NumPy and Pandas.\n",
       "*   **Understand core ML concepts:** Learn about supervised, unsupervised, and reinforcement learning, as well as common algorithms (e.g., linear regression, logistic regression, decision trees, SVMs, k-means).\n",
       "*   **Start with practical projects:** Apply what you learn to real-world datasets. Kaggle is a great resource for this.\n",
       "*   **Learn by doing, not just reading:** Experiment with different models, tune hyperparameters, and analyze results.\n",
       "*   **Understand the evaluation metrics:** Know how to assess the performance of your models (e.g., accuracy, precision, recall, F1-score, MSE).\n",
       "*   **Explore deep learning:** Once you have a grasp of traditional ML, dive into neural networks and frameworks like TensorFlow or PyTorch.\n",
       "*   **Stay updated:** The field evolves rapidly. Follow reputable blogs, research papers, and online courses.\n",
       "*   **Join a community:** Engage with other learners and practitioners for support and insights.\n",
       "*   **Be patient and persistent:** Machine learning can be challenging, so don't get discouraged by setbacks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Write a short bullet-point list of tips for learning machine learning.\"\n",
    ")\n",
    "\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Tracking Costs\n",
    "\n",
    "You can see estimated costs for your API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Cost: $0.000054 | Tokens: 13 in / 132 out | Model: google/gemini-2.5-flash-lite\n",
      "Here are five dad jokes for you:\n",
      "\n",
      "1.  **Why don't scientists trust atoms?**\n",
      "    Because they make up everything!\n",
      "\n",
      "2.  **What do you call a fish with no eyes?**\n",
      "    Fsh!\n",
      "\n",
      "3.  **I'm reading a book about anti-gravity. It's impossible to put down!**\n",
      "\n",
      "4.  **Did you hear about the restaurant on the moon?**\n",
      "    I heard the food was good, but it had no atmosphere.\n",
      "\n",
      "5.  **Why did the scarecrow win an award?**\n",
      "    Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Tell me five dad jokes.\",\n",
    "    print_cost=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Controlling Output Length\n",
    "\n",
    "By default, the output of `llm_generate` is limited to 200 tokens.  You can control how much text the model generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Mittens was no ordinary feline. While other cats chased laser pointers and napped in sunbeams, Mittens had a more refined curiosity. Her gaze often drifted to the grand piano in the living room, its polished surface reflecting the world like a dark, silent lake. She‚Äôd perch on the edge of the piano bench, her tail twitching with an unspoken longing.\n",
       "\n",
       "Her humans, a kindly couple named Arthur and Eleanor, were amateur musicians. Their evenings were often filled with the warm, resonant embrace of music. Mittens would listen, her emerald eyes half-closed, captivated by the cascade of notes. Sometimes, when Arthur played a particularly melancholic piece, she‚Äôd let out a soft, questioning meow, as if trying to harmonize with the sorrow.\n",
       "\n",
       "One afternoon, Arthur left the piano lid open. Mittens, bolder than usual, hopped onto the bench. She sniffed at the keys, a strange, foreign scent. Then, tentatively, she extended a paw. A single, clear note, a startling B-flat, echoed in the quiet room. Mittens flinched, her ears swiveling. But the sound, though unexpected, didn't scare her. Instead, it sparked something within.\n",
       "\n",
       "Over the next few weeks, Mittens‚Äô secret practice sessions became a nightly ritual. She‚Äôd wait until Arthur and Eleanor were asleep, then pad silently into the living room. Her initial attempts were clumsy. Paws would land haphazardly, producing jarring chords. She‚Äôd bat at the keys, a flurry of random sounds. But with each attempt, she grew more deliberate. She discovered that gently pressing a key produced a softer tone, and a firmer press a louder one. Her tail would swish rhythmically, as if conducting her own silent orchestra.\n",
       "\n",
       "She began to associate certain keys with specific feelings. A low rumble from the bass clef felt like a contented purr. Higher, tinkling notes reminded her of the jingle of her toy mice. She even started to mimic the melodies she‚Äôd heard her humans play, albeit in a simplified, feline interpretation. Her ‚ÄúF√ºr Elise‚Äù was a series of hesitant, yet recognizable, arpeggios. Her ‚ÄúTwinkle, Twinkle Little Star‚Äù was a whimsical, slightly off-key rendition.\n",
       "\n",
       "Arthur and Eleanor began to notice the strange occurrences. A key would be slightly depressed when they entered the room. A faint, almost imperceptible melody seemed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Longer response\n",
    "response = llm_generate(\n",
    "    'gemini-flash-lite',\n",
    "    \"Write a short story about a cat who learns to play the piano.\",\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Viewing Available Models\n",
    "\n",
    "You can see all available models and their details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available OpenRouter Models:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Short Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Model ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "In/M",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Out/M",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Schema",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "8f89e508-3aba-4acc-80e5-5967bb6c64d1",
       "rows": [
        [
         "0",
         "claude-haiku",
         "anthropic/claude-3.5-haiku",
         "$0.80",
         "$4.00",
         "‚ùå"
        ],
        [
         "1",
         "deepseek-v3.1",
         "deepseek/deepseek-chat-v3.1",
         "$0.20",
         "$0.80",
         "‚úÖ"
        ],
        [
         "2",
         "gemini-flash",
         "google/gemini-2.5-flash",
         "$0.30",
         "$2.50",
         "‚úÖ"
        ],
        [
         "3",
         "gemini-flash-lite",
         "google/gemini-2.5-flash-lite",
         "$0.10",
         "$0.40",
         "‚úÖ"
        ],
        [
         "4",
         "gpt-4o",
         "openai/gpt-4o-2024-11-20",
         "$2.50",
         "$10.00",
         "‚úÖ"
        ],
        [
         "5",
         "gpt-4o-mini",
         "openai/gpt-4o-mini-2024-07-18",
         "$0.15",
         "$0.60",
         "‚úÖ"
        ],
        [
         "6",
         "gpt-oss-120b",
         "openai/gpt-oss-120b",
         "$0.04",
         "$0.40",
         "‚ùå"
        ],
        [
         "7",
         "gpt-oss-20b",
         "openai/gpt-oss-20b",
         "$0.03",
         "$0.14",
         "‚úÖ"
        ],
        [
         "8",
         "llama-3.2-1b",
         "meta-llama/llama-3.2-1b-instruct",
         "$0.01",
         "$0.01",
         "‚ùå"
        ],
        [
         "9",
         "llama-3.2-3b",
         "meta-llama/llama-3.2-3b-instruct",
         "$0.02",
         "$0.02",
         "‚ùå"
        ],
        [
         "10",
         "llama-3.3-70b",
         "meta-llama/llama-3.3-70b-instruct",
         "$0.13",
         "$0.39",
         "‚úÖ"
        ],
        [
         "11",
         "mistral-medium",
         "mistralai/mistral-medium-3",
         "$0.40",
         "$2.00",
         "‚úÖ"
        ],
        [
         "12",
         "mistral-nemo",
         "mistralai/mistral-nemo",
         "$0.02",
         "$0.04",
         "‚úÖ"
        ],
        [
         "13",
         "qwen3-32b",
         "qwen/qwen3-32b",
         "$0.05",
         "$0.20",
         "‚úÖ"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 14
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Short Name</th>\n",
       "      <th>Model ID</th>\n",
       "      <th>In/M</th>\n",
       "      <th>Out/M</th>\n",
       "      <th>Schema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-haiku</td>\n",
       "      <td>anthropic/claude-3.5-haiku</td>\n",
       "      <td>$0.80</td>\n",
       "      <td>$4.00</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek-v3.1</td>\n",
       "      <td>deepseek/deepseek-chat-v3.1</td>\n",
       "      <td>$0.20</td>\n",
       "      <td>$0.80</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-flash</td>\n",
       "      <td>google/gemini-2.5-flash</td>\n",
       "      <td>$0.30</td>\n",
       "      <td>$2.50</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini-flash-lite</td>\n",
       "      <td>google/gemini-2.5-flash-lite</td>\n",
       "      <td>$0.10</td>\n",
       "      <td>$0.40</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>openai/gpt-4o-2024-11-20</td>\n",
       "      <td>$2.50</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>openai/gpt-4o-mini-2024-07-18</td>\n",
       "      <td>$0.15</td>\n",
       "      <td>$0.60</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-oss-120b</td>\n",
       "      <td>openai/gpt-oss-120b</td>\n",
       "      <td>$0.04</td>\n",
       "      <td>$0.40</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-oss-20b</td>\n",
       "      <td>openai/gpt-oss-20b</td>\n",
       "      <td>$0.03</td>\n",
       "      <td>$0.14</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-3.2-1b</td>\n",
       "      <td>meta-llama/llama-3.2-1b-instruct</td>\n",
       "      <td>$0.01</td>\n",
       "      <td>$0.01</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-3.2-3b</td>\n",
       "      <td>meta-llama/llama-3.2-3b-instruct</td>\n",
       "      <td>$0.02</td>\n",
       "      <td>$0.02</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-3.3-70b</td>\n",
       "      <td>meta-llama/llama-3.3-70b-instruct</td>\n",
       "      <td>$0.13</td>\n",
       "      <td>$0.39</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>mistralai/mistral-medium-3</td>\n",
       "      <td>$0.40</td>\n",
       "      <td>$2.00</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mistral-nemo</td>\n",
       "      <td>mistralai/mistral-nemo</td>\n",
       "      <td>$0.02</td>\n",
       "      <td>$0.04</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>qwen/qwen3-32b</td>\n",
       "      <td>$0.05</td>\n",
       "      <td>$0.20</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Short Name                           Model ID   In/M   Out/M Schema\n",
       "0        claude-haiku         anthropic/claude-3.5-haiku  $0.80   $4.00      ‚ùå\n",
       "1       deepseek-v3.1        deepseek/deepseek-chat-v3.1  $0.20   $0.80      ‚úÖ\n",
       "2        gemini-flash            google/gemini-2.5-flash  $0.30   $2.50      ‚úÖ\n",
       "3   gemini-flash-lite       google/gemini-2.5-flash-lite  $0.10   $0.40      ‚úÖ\n",
       "4              gpt-4o           openai/gpt-4o-2024-11-20  $2.50  $10.00      ‚úÖ\n",
       "5         gpt-4o-mini      openai/gpt-4o-mini-2024-07-18  $0.15   $0.60      ‚úÖ\n",
       "6        gpt-oss-120b                openai/gpt-oss-120b  $0.04   $0.40      ‚ùå\n",
       "7         gpt-oss-20b                 openai/gpt-oss-20b  $0.03   $0.14      ‚úÖ\n",
       "8        llama-3.2-1b   meta-llama/llama-3.2-1b-instruct  $0.01   $0.01      ‚ùå\n",
       "9        llama-3.2-3b   meta-llama/llama-3.2-3b-instruct  $0.02   $0.02      ‚ùå\n",
       "10      llama-3.3-70b  meta-llama/llama-3.3-70b-instruct  $0.13   $0.39      ‚úÖ\n",
       "11     mistral-medium         mistralai/mistral-medium-3  $0.40   $2.00      ‚úÖ\n",
       "12       mistral-nemo             mistralai/mistral-nemo  $0.02   $0.04      ‚úÖ\n",
       "13          qwen3-32b                     qwen/qwen3-32b  $0.05   $0.20      ‚úÖ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default model: gemini-flash-lite\n",
      "Schema = User-defined JSON schemas supported\n",
      "\n",
      "You can also use any OpenRouter model by its full ID (e.g., 'openai/gpt-4o')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'claude-haiku'),\n",
       " (2, 'deepseek-v3.1'),\n",
       " (3, 'gemini-flash'),\n",
       " (4, 'gemini-flash-lite'),\n",
       " (5, 'gpt-4o'),\n",
       " (6, 'gpt-4o-mini'),\n",
       " (7, 'gpt-oss-120b'),\n",
       " (8, 'gpt-oss-20b'),\n",
       " (9, 'llama-3.2-1b'),\n",
       " (10, 'llama-3.2-3b'),\n",
       " (11, 'llama-3.3-70b'),\n",
       " (12, 'mistral-medium'),\n",
       " (13, 'mistral-nemo'),\n",
       " (14, 'qwen3-32b')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Trying Different Models\n",
    "\n",
    "It's easy to compare different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small model (llama-3.2-3b):\n",
      "Quantum computing is a revolutionary technology that uses the principles of\n",
      "quantum mechanics to perform calculations that are exponentially faster and more\n",
      "complex than those possible with classical computers, leveraging the unique\n",
      "properties of subatomic particles to solve problems that are intractable with\n",
      "traditional computing methods.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Recommended model (gemini-flash-lite):\n",
      "Quantum computing harnesses the principles of quantum mechanics, like\n",
      "superposition and entanglement, to perform calculations exponentially faster\n",
      "than classical computers for certain complex problems.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "# Try a small model\n",
    "print(\"A small model (llama-3.2-3b):\")\n",
    "response1 = llm_generate('llama-3.2-3b', prompt)\n",
    "print(response1)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Try the recommended model\n",
    "print(\"Recommended model (gemini-flash-lite):\")\n",
    "response2 = llm_generate('gemini-flash-lite', prompt, estimate_cost=True)\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ev36qb1idx",
   "metadata": {},
   "source": [
    "### Using Models Outside the Curated List\n",
    "\n",
    "You can use any model from [OpenRouter's model list](https://openrouter.ai/models) by providing the full model ID. For example, let's try OpenAI's GPT-5-nano (which isn't in our curated list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pe90mn0t2no",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three key benefits of learning Python:\n",
      "\n",
      "1. **Versatility and Wide Application**: Python is a general-purpose programming\n",
      "language that can be used for web development, data analysis, artificial\n",
      "intelligence, machine learning, automation, and more. This versatility makes it\n",
      "valuable across many industries and career paths.\n",
      "\n",
      "2. **Beginner-Friendly Syntax**: Python has a clean, readable syntax that\n",
      "resembles plain English, making it one of the easiest programming languages for\n",
      "beginners to learn. Its simplicity allows newcomers to focus on programming\n",
      "concepts rather than complex syntax rules.\n",
      "\n",
      "3. **Strong Community and Ecosystem**: Python has a vast community of developers\n",
      "who contribute to extensive libraries and frameworks (like Django, Flask, NumPy,\n",
      "and Pandas). This rich ecosystem provides pre-built solutions for various tasks,\n",
      "saving development time and offering abundant learning resources.\n"
     ]
    }
   ],
   "source": [
    "# Use the full OpenRouter model ID\n",
    "response = llm_generate(\n",
    "    'z-ai/glm-4.5-air',  # Full model ID from openrouter.ai/models\n",
    "    \"What are three benefits of learning Python?\",\n",
    "    estimate_cost=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Processing Multiple Prompts\n",
    "\n",
    "You can process multiple prompts at once by passing a list of strings. This is useful for batch processing tasks like sentiment analysis or classification.\n",
    "\n",
    "### Simple Batch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of France?\n",
      "A: The capital of France is **Paris**.\n",
      "------------------------------------------------------------\n",
      "Q: What is the capital of Germany?\n",
      "A: The capital of Germany is **Berlin**.\n",
      "------------------------------------------------------------\n",
      "Q: What is the capital of Italy?\n",
      "A: The capital of Italy is **Rome**.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'What is the capital of Germany?',\n",
    "    'What is the capital of Italy?'\n",
    "]\n",
    "\n",
    "responses = llm_generate('gemini-flash-lite', prompts)\n",
    "\n",
    "for prompt, response in zip(prompts, responses):\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Programmatic Prompt Construction\n",
    "\n",
    "Often we want to construct prompts programmatically from data. Here's an example of sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "\n",
      "Text: I love the new design of your website!\n",
      "Sentiment: Positive\n",
      "------------------------------------------------------------\n",
      "Text: The service was terrible and I will not come back.\n",
      "Sentiment: Negative\n",
      "------------------------------------------------------------\n",
      "Text: The product is okay, but it could be better.\n",
      "Sentiment: Neutral\n",
      "------------------------------------------------------------\n",
      "Text: Absolutely fantastic experience, highly recommend!\n",
      "Sentiment: Positive\n",
      "------------------------------------------------------------\n",
      "Text: I'm not sure how I feel about this.\n",
      "Sentiment: Neutral\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for sentiment analysis\n",
    "system_prompt = \"You are a sentiment analysis AI. Classify text as Positive, Negative, or Neutral.\"\n",
    "\n",
    "# List of texts to analyze\n",
    "texts = [\n",
    "    \"I love the new design of your website!\",\n",
    "    \"The service was terrible and I will not come back.\",\n",
    "    \"The product is okay, but it could be better.\",\n",
    "    \"Absolutely fantastic experience, highly recommend!\",\n",
    "    \"I'm not sure how I feel about this.\"\n",
    "]\n",
    "\n",
    "# Construct prompts programmatically\n",
    "instruction = \"Analyze the sentiment of this text. Give only the sentiment classification (Positive, Negative, or Neutral).\\n\\nText: \"\n",
    "prompts = [instruction + text for text in texts]\n",
    "\n",
    "# Generate responses\n",
    "responses = llm_generate('gemini-flash-lite', prompts, system_prompt=system_prompt)\n",
    "\n",
    "# Display results\n",
    "print(\"Sentiment Analysis Results:\\n\")\n",
    "for text, sentiment in zip(texts, responses):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322d9e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e49fd9b9",
   "metadata": {},
   "source": [
    "### Using Other Providers\n",
    "\n",
    "You can use other LLM providers in this class as well, but you don't need to do so.  The last notebook shows how to set up other providers if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In the next notebook, we'll explore common NLP tasks including:\n",
    "- Text classification and sentiment analysis\n",
    "- Named Entity Recognition (NER)\n",
    "- Question answering\n",
    "- Translation\n",
    "- Summarization\n",
    "\n",
    "In Lesson 11, we'll dive deeper into how text generation works, explore the underlying APIs in detail, and learn about running LLMs locally for privacy-sensitive applications.\n",
    "\n",
    "For now, practice using `llm_generate()` with different models and prompts to get comfortable with the interface!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}