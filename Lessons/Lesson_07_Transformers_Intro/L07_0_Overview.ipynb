{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7: Introduction to Transformers, Hugging Face, and Using LLMs Effectively\n",
    "\n",
    "### Topics\n",
    "* Transformer architecture overview\n",
    "* Self-attention mechanism\n",
    "* Hugging Face library introduction\n",
    "* Effective use of large language models (LLMs) and prompt engineering basics\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Describe Transformer Architecture**: Identify the core components of transformers, including the encoder-decoder structure and the role of self-attention.\n",
    "   \n",
    "2. **Explain Self-Attention Basics**: Outline how self-attention works, including the function of queries, keys, and values, and the advantages it provides for handling context.\n",
    "\n",
    "3. **Use Pre-trained Models**: Load and use a pre-trained Hugging Face model for a simple NLP task to gain familiarity with the Hugging Face transformers library.\n",
    "\n",
    "4. **Improve Prompting Techniques**: Experiment with prompt engineering to enhance response quality when interacting with LLMs, learning to refine prompts for clarity and relevance.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 1: Hello Transformers* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each of the notebooks included in the Lesson_07 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "### Homework Ideas\n",
    "\n",
    "1. **Prompt Experimentation with an LLM**: In a Jupyter notebook, have students interact with an LLM (e.g., OpenAI’s GPT-3 or Hugging Face’s T5 model) and test different prompt styles (e.g., rephrasing questions, adding context) to improve response accuracy. Students can document prompt adjustments, evaluate outputs, and identify the most effective prompt styles for specific types of queries.\n",
    "\n",
    "2. **Sentiment Classification Using Hugging Face**: Guide students to load a pre-trained sentiment classification model from Hugging Face (such as DistilBERT) and apply it to a sample dataset. Students can explore how the model’s predictions vary with different sentence structures or tone (e.g., sarcasm, informal language) and document observations about the model’s strengths and limitations.\n",
    "\n",
    "3. **Mini Research Task with LLM Outputs**: Students can use an LLM to research simple questions on a chosen topic. Have them experiment with prompt variations and summarize findings, comparing the quality of structured prompts versus open-ended questions. This activity will help them observe how prompt structure affects the quality and relevance of LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
