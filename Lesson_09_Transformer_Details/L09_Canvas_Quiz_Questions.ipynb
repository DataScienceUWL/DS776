{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10-Question Multiple-Choice Reading Quiz for Chapter 3: \"Transformer Anatomy\"**  \n",
    "\n",
    "---\n",
    "\n",
    "#### **1. What is the primary function of the self-attention mechanism in transformers?**  \n",
    "A. To encode positional information of tokens in a sequence.  \n",
    "B. To predict the next token in a sequence.  \n",
    "C. To calculate relationships between all tokens in the input sequence.  \n",
    "D. To feed contextual information through recurrent layers.  \n",
    "**Answer**: C  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. What is the purpose of multi-headed attention in a transformer?**  \n",
    "A. To eliminate the need for positional encodings.  \n",
    "B. To process input sequences in a single pass.  \n",
    "C. To add randomness during training for better generalization.  \n",
    "D. To focus on multiple aspects of the input sequence simultaneously.  \n",
    "**Answer**: D  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Why are positional encodings important in transformers?**  \n",
    "A. They add sequence order information that transformers lack due to parallel processing.  \n",
    "B. They improve token embeddings for downstream tasks.  \n",
    "C. They replace attention mechanisms in encoders.  \n",
    "D. They eliminate the need for feed-forward layers.  \n",
    "**Answer**: A  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4. What mathematical functions are typically used for positional encodings?**  \n",
    "A. Polynomial and quadratic functions  \n",
    "B. Square root and division functions  \n",
    "C. Sine and cosine functions  \n",
    "D. Exponential and logarithmic functions  \n",
    "**Answer**: C  \n",
    "\n",
    "---\n",
    "\n",
    "#### **5. How do residual connections improve transformers?**  \n",
    "A. They stabilize gradients and prevent vanishing gradient issues during training.  \n",
    "B. They allow parallel training by dividing sequences into segments.  \n",
    "C. They simplify tokenization by bypassing subword representations.  \n",
    "D. They reduce the model's complexity by skipping certain computations.  \n",
    "**Answer**: A  \n",
    "\n",
    "---\n",
    "\n",
    "#### **6. What is the role of feed-forward layers in a transformer?**  \n",
    "A. To encode the positional information in input sequences.  \n",
    "B. To compute the attention scores for input tokens.  \n",
    "C. To aggregate information from multiple attention heads.  \n",
    "D. To process information independently for each token.  \n",
    "**Answer**: D  \n",
    "\n",
    "---\n",
    "\n",
    "#### **7. What distinguishes transformers from traditional RNNs in handling sequences?**  \n",
    "A. Transformers require less memory than RNNs for sequence processing.  \n",
    "B. Transformers process sequences in parallel, while RNNs process them sequentially.  \n",
    "C. Transformers rely on convolutional layers, while RNNs use attention mechanisms.  \n",
    "D. Transformers only handle short sequences, while RNNs can handle long sequences.  \n",
    "**Answer**: B  \n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Which of the following is a challenge addressed by layer normalization in transformers?**  \n",
    "A. Overfitting in smaller datasets  \n",
    "B. Vanishing and exploding gradients  \n",
    "C. Excessive parameter usage  \n",
    "D. Long training times  \n",
    "**Answer**: B  \n",
    "\n",
    "---\n",
    "\n",
    "#### **9. How do positional encodings differ from token embeddings in transformers?**  \n",
    "A. Positional encodings capture sequence order, while token embeddings represent word meanings.  \n",
    "B. Positional encodings are learned during training, while token embeddings are static.  \n",
    "C. Token embeddings use sine functions, while positional encodings use cosine functions.  \n",
    "D. Token embeddings are applied at the output layer, while positional encodings are applied at the input layer.  \n",
    "**Answer**: A  \n",
    "\n",
    "---\n",
    "\n",
    "#### **10. What is a key limitation of the vanilla transformer architecture?**  \n",
    "A. It has high computational and memory requirements for long sequences.  \n",
    "B. It struggles with understanding token-level classification tasks.  \n",
    "C. It does not support pretraining on large datasets.  \n",
    "D. It cannot handle non-textual data.  \n",
    "**Answer**: A  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
