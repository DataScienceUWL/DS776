{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1\n",
    "\n",
    "### Message (include in course intro video)\n",
    "\n",
    "Much of your learning in this class will come from readings and videos from a variety of sources.  In addition, I've prepared a few videos of examples and concepts to supplement the readings and videos.  Our main textbook for the first 6 weeks or so is the book \"Inside Deep Learning - Math, Algorithms, Models\" by Edward Raff (IDL).  Depending on your background you may sometimes find the reading difficult or that you're missing certain concepts.  \n",
    "\n",
    "For example, in this first week's readings you should notice that the author uses matrix multiplication and vector addition to summarize what happens in a fully connected linear layer in a neural network.  You don't have to know what matrix multiplication is to succeed in this class or even to understand deep learning, just know that $x^T W + b$ is mathematical shorthand multiplying by weights and adding biases in a linear layer.  When there are things like this that pop up in the reading or other materials I'll try to identify auxillary materials that you can use to get the basic ideas, but skipping those auxillary materials likely won't prevent you from learning the overall deep learning concepts.  This week there are a couple of videos listed in the auxillary about matrix and vector operations.  I'm also delighted to answer questions about anything related to the readings and videos in a our course forum.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand deep learning fundamentals**, including the structure of neural networks and their relationship to broader machine learning concepts.\n",
    "2. **Utilize PyTorch datasets and data loaders** to efficiently handle and preprocess data for training deep learning models.\n",
    "3. **Develop and train basic fully connected neural networks** in PyTorch, applying optimization techniques and appropriate loss functions.\n",
    "4. **Optimize model performance** by adjusting network architecture and layers, while evaluating outcomes.\n",
    "\n",
    "### Primary Materials\n",
    "\n",
    "* **Review Neural Networks from DS740**\n",
    "\n",
    "    * You might want to review the first 14 slides of the [Lesson on Neural networks in DS740](https://media.uwex.edu/content/ds/ds740_r23/ds740_artificial-neural-networks.sbproj/).  We're covering similar material this week.  Don't review the material about neural networks in R since we'll be using Python.\n",
    "\n",
    "* **Readings from Inside Deep Learning (IDL)**\n",
    "\n",
    "    * **Chapter 1: The Mechanics of Learning**\n",
    "        - **Read Sections 1.2, 1.4, and 1.5**. Skim the other sections. No need to understand the detailed code or the backpropagation algorithm, but ensure you understand how the gradient is used in training.\n",
    "\n",
    "    * **Chapter 2: Fully Connected Networks*\n",
    "        - **Section 2.1**: Focus on understanding the training loop structure and process. Skip the code details but grasp the concept.\n",
    "            - Don’t worry about the math notation at the bottom of page 40. It's shorthand for a fully connected linear layer. If you want to learn more about matrix multiplication see the videos listed under Auxiliary Materials below.\n",
    "        - **Section 2.2**: Understand how activation functions introduce nonlinearity into networks.\n",
    "        - **Section 2.3**: Grasp the basics of softmax and cross-entropy, especially how the loss function changes for classification tasks. An example will be explained in a video.\n",
    "        - **Section 2.4**: Note the key concepts; they will be reinforced in video lectures.\n",
    "        - **Section 2.5**: Understand the importance of batch training, particularly for large datasets that won’t fit in memory.\n",
    "\n",
    "* **Course Videos**\n",
    "    * There are 3 videos and an accompanying notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxillary Materials\n",
    "\n",
    "#### Background Mathematics\n",
    "\n",
    "If you want to understand the theory of building and training neural networks you likely need some background in multivariate calculus and linear algebra.  To work with and apply neural networks you can by with just a little math.  \n",
    "\n",
    "* **Matrices, Vectors, and Gradients.** Having some idea about matrices, vectors and gradients is a good idea.  Dr. Anne Hsu has recorded some helpful videos on many topics related to introductory deep learning:\n",
    "    * [Matrices and Vectors](https://www.youtube.com/watch?v=sM2Mm6aT_HI)\n",
    "    * [Derivatives and Gradients 1](https://www.youtube.com/watch?v=Fiw0_w4AykA)\n",
    "    * [Derivatives and Gradients 2](https://www.youtube.com/watch?v=qORZmKCB0g8)\n",
    "    * [Playlist for entire Intro to Deep Learning](https://www.youtube.com/@drannehsu/playlists)\n",
    "\n",
    "* **Mathematics for Machine Learning and Data Science.** This specialization includes linear algebra, calculus, and probability and statistics.  It's too much to pick up for a quick introduction but would be terrific for a deeper dive:\n",
    "    * [Coursera three course specialization](https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science?)\n",
    "\n",
    "#### Other Deep Learning Resources:\n",
    "\n",
    "* **Deep Learning Basics: Introduction and Overview**  Lex Fridman is a well known podcast host for AI and Data Science.  Here he gives an [introductory lecture](https://youtu.be/O5xeyoRL95U?si=SrM7RLWB_iBPMiK4) for MIT's public deep learning class.  Since it was recorded in 2019 it doesn't include the latest on transformer architectures that are driving the current boom in AI (ChatGPT, etc.), it's still a really nice introduction that discusses many applications of deep learning.  Watch this if you want a good overview.  I also recommend his podcast.\n",
    "\n",
    "* **Neural Networks and Deep Learning.** Andrew Ng is a big name in AI and I love listening to him.  This [playlist](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0) is all the videos from the first course of his Deep Learning Specialization on Coursera.  This is a really great resource for a deep learning introduction.\n",
    "\n",
    "* **Introduction to Deep Learning from MIT**  [Free public course](http://introtodeeplearning.com/) updated annually.  The beginning lecture is very accessible.\n",
    "\n",
    "* **Activation Functions** There are more than the three activation functions we discussed in our video.  [All the Activation Functions](https://dublog.net/blog/all-the-activations/) is a quick summary of most of activation functions used in deep learning.\n",
    "\n",
    "\n",
    "#### Programming and PyTorch Resources\n",
    "\n",
    "* **Object-Oriented Programming** It helps to have a basic familiarity with classes, inheritance, and methods for finding your way around in PyTorch.  [Real Python has a great tutorial](https://realpython.com/python3-object-oriented-programming/) on the basics of OOP with many code examples that you can either read or watch (40 minutes).\n",
    "\n",
    "* **PyTorch Tutorials** The [official PyTorch tutorials](https://pytorch.org/tutorials/) are quite good.  For Lesson 1 the tutorials on tensors (both [written](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) and on [YouTube](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)) and on [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) might be of interest.  The [Build the Neural Network tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) also breaks down the ingredients in a fully connected network. \n",
    "\n",
    "\n",
    "* **PyTorch Documentation** This often isn't as useful as the tutorials, but you can find information about [everything PyTorch here](https://pytorch.org/docs/stable/index.html).  One nugget that might be helpful for Lesson 1 is the the [documentation for torch.Tensor](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) which shows the available data types.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UWL-DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
