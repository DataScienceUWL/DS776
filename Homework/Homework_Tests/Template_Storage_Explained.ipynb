{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup-auto-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DS776 REQUIRED SETUP - Run this cell FIRST, before any other code!\n",
    "# =============================================================================\n",
    "%run ../Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for this notebook\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from introdl import config_paths_keys\n",
    "\n",
    "paths = config_paths_keys()\n",
    "DATA_PATH = paths['DATA_PATH']\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "CACHE_PATH = paths['CACHE_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Understanding Storage in DS776\n",
    "\n",
    "This notebook explains how storage works in this course, where different types of files go, and how to manage your storage effectively.\n",
    "\n",
    "**Why this matters:** Deep learning involves downloading large pretrained models and saving training checkpoints. Without understanding where these files go, you can easily fill up your storage quota.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-overview",
   "metadata": {},
   "source": [
    "## Storage Overview\n",
    "\n",
    "In this course, you'll work with several types of files:\n",
    "\n",
    "| File Type | Examples | Size | Persistence |\n",
    "|-----------|----------|------|-------------|\n",
    "| **Datasets** | CIFAR-10, IMDB, CoNLL | 10MB - 5GB | Downloaded once, reused |\n",
    "| **Pretrained Models** | BERT, ResNet, GPT-2 | 100MB - 5GB | Cached automatically |\n",
    "| **Your Checkpoints** | `model.pt`, `checkpoint-1000/` | 50MB - 2GB each | You control these |\n",
    "| **Training Logs** | TensorBoard, wandb | 1MB - 100MB | Usually temporary |\n",
    "\n",
    "The course setup system organizes these into specific folders so you can:\n",
    "1. Find your files easily\n",
    "2. Clean up when needed\n",
    "3. Know what's safe to delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-path-vars",
   "metadata": {},
   "source": [
    "## The Path Variables\n",
    "\n",
    "Every notebook uses three path variables from `config_paths_keys()`:\n",
    "\n",
    "### `DATA_PATH` - Datasets\n",
    "Where datasets are downloaded and stored.\n",
    "\n",
    "```python\n",
    "# Loading CIFAR-10\n",
    "dataset = CIFAR10(root=DATA_PATH, download=True)\n",
    "\n",
    "# Loading a HuggingFace dataset\n",
    "dataset = load_dataset('imdb', cache_dir=DATA_PATH)\n",
    "```\n",
    "\n",
    "### `MODELS_PATH` - Your Trained Models\n",
    "Where YOU save your trained model checkpoints. This is unique per notebook.\n",
    "\n",
    "```python\n",
    "# Saving a PyTorch model\n",
    "torch.save(model.state_dict(), MODELS_PATH / 'my_model.pt')\n",
    "\n",
    "# HuggingFace Trainer output\n",
    "TrainingArguments(output_dir=str(MODELS_PATH / 'bert_classifier'), ...)\n",
    "```\n",
    "\n",
    "### `CACHE_PATH` - Downloaded Pretrained Models\n",
    "Where pretrained models (BERT, ResNet, etc.) are automatically cached.\n",
    "\n",
    "```python\n",
    "# This downloads and caches automatically to CACHE_PATH\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-show-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see where your paths point to\n",
    "print(\"Your storage locations:\")\n",
    "print(f\"  DATA_PATH:   {DATA_PATH}\")\n",
    "print(f\"  MODELS_PATH: {MODELS_PATH}\")\n",
    "print(f\"  CACHE_PATH:  {CACHE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cocalc-storage",
   "metadata": {},
   "source": [
    "## CoCalc Storage Architecture\n",
    "\n",
    "CoCalc has two types of storage:\n",
    "\n",
    "### Home Server (Synced Storage)\n",
    "- **Location:** `~/home_workspace/`\n",
    "- **Limit:** ~10GB total\n",
    "- **Syncs:** Yes - available everywhere, backed up\n",
    "- **Use for:** API keys, important models you want to keep\n",
    "\n",
    "### Compute Server (Local Storage)\n",
    "- **Location:** `~/cs_workspace/`\n",
    "- **Limit:** ~50GB\n",
    "- **Syncs:** No - only on compute server\n",
    "- **Use for:** Large datasets, cached models, temporary files\n",
    "\n",
    "**The course setup automatically uses the right storage:**\n",
    "- On compute server: Data and cache go to `cs_workspace/` (fast, lots of space)\n",
    "- Your models always go to `MODELS_PATH` which syncs back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-what-gets-cached",
   "metadata": {},
   "source": [
    "## What Gets Cached Automatically?\n",
    "\n",
    "### PyTorch/TorchVision Models\n",
    "When you use pretrained models:\n",
    "```python\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')  # ~100MB download\n",
    "```\n",
    "PyTorch caches these in `TORCH_HOME` (set by our setup to `CACHE_PATH`).\n",
    "\n",
    "### HuggingFace Models\n",
    "Transformers downloads are LARGE:\n",
    "```python\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')  # ~440MB\n",
    "model = AutoModel.from_pretrained('bert-large-uncased')  # ~1.3GB\n",
    "model = AutoModel.from_pretrained('gpt2-medium')  # ~1.5GB\n",
    "```\n",
    "These cache in `HF_HOME` (set by our setup to `CACHE_PATH/huggingface`).\n",
    "\n",
    "### HuggingFace Datasets\n",
    "```python\n",
    "dataset = load_dataset('imdb')  # ~80MB\n",
    "dataset = load_dataset('xsum')  # ~250MB\n",
    "```\n",
    "These cache in `HF_DATASETS_CACHE` (set to `DATA_PATH`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-check-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's in your cache\n",
    "def get_folder_size(path):\n",
    "    \"\"\"Get total size of a folder in bytes.\"\"\"\n",
    "    total = 0\n",
    "    path = Path(path)\n",
    "    if path.exists():\n",
    "        for f in path.rglob('*'):\n",
    "            if f.is_file():\n",
    "                total += f.stat().st_size\n",
    "    return total\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    \"\"\"Format bytes to human readable.\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.1f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "    return f\"{size_bytes:.1f} TB\"\n",
    "\n",
    "print(\"Current storage usage:\")\n",
    "print(f\"  DATA_PATH:   {format_size(get_folder_size(DATA_PATH))}\")\n",
    "print(f\"  CACHE_PATH:  {format_size(get_folder_size(CACHE_PATH))}\")\n",
    "print(f\"  MODELS_PATH: {format_size(get_folder_size(MODELS_PATH))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-hf-trainer-problem",
   "metadata": {},
   "source": [
    "## The HuggingFace Trainer Storage Problem\n",
    "\n",
    "**This is the #1 cause of storage issues in this course!**\n",
    "\n",
    "By default, HuggingFace Trainer saves a checkpoint after EVERY epoch:\n",
    "\n",
    "```\n",
    "output_dir/\n",
    "├── checkpoint-500/     # 500MB\n",
    "├── checkpoint-1000/    # 500MB\n",
    "├── checkpoint-1500/    # 500MB\n",
    "├── checkpoint-2000/    # 500MB\n",
    "└── checkpoint-2500/    # 500MB\n",
    "                        # = 2.5GB for 5 epochs!\n",
    "```\n",
    "\n",
    "For a BERT model trained for 10 epochs, this can use **5-10GB of storage**.\n",
    "\n",
    "### The Solution: `save_total_limit=1`\n",
    "\n",
    "**Always include these settings when using HuggingFace Trainer:**\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODELS_PATH / 'my_model'),\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,           # CRITICAL: Only keep 1 checkpoint!\n",
    "    load_best_model_at_end=True,  # Load the best one when done\n",
    "    metric_for_best_model=\"eval_loss\",  # Or eval_accuracy, eval_f1\n",
    "    # ... other settings ...\n",
    ")\n",
    "```\n",
    "\n",
    "With these settings:\n",
    "```\n",
    "output_dir/\n",
    "└── checkpoint-best/    # 500MB (only the best model!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-what-safe-delete",
   "metadata": {},
   "source": [
    "## What's Safe to Delete?\n",
    "\n",
    "### Safe to Delete (will re-download automatically):\n",
    "- ✅ `DATA_PATH` contents - datasets re-download\n",
    "- ✅ `CACHE_PATH` contents - pretrained models re-download\n",
    "- ✅ `~/.cache/huggingface/` - HuggingFace cache\n",
    "- ✅ Old `Lesson_XX_Models/` folders\n",
    "- ✅ Old `Homework_XX_Models/` folders (after grades posted)\n",
    "\n",
    "### Keep Until Grades Posted:\n",
    "- ⚠️ Current homework `MODELS_PATH` - your submitted work\n",
    "\n",
    "### Never Delete:\n",
    "- ❌ `home_workspace/api_keys.env` - your API keys\n",
    "- ❌ Your notebook files (`.ipynb`)\n",
    "\n",
    "### Using Storage_Cleanup.ipynb\n",
    "\n",
    "The easiest way to clean up is to run:\n",
    "\n",
    "**`Lessons/Course_Tools/Storage_Cleanup.ipynb`**\n",
    "\n",
    "This notebook:\n",
    "1. Shows you exactly what's using storage\n",
    "2. Lets you delete it with one click\n",
    "3. Protects your API keys and current work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-tips",
   "metadata": {},
   "source": [
    "## Storage Tips\n",
    "\n",
    "### 1. Always Use Path Variables\n",
    "```python\n",
    "# Good - works everywhere\n",
    "torch.save(model, MODELS_PATH / 'model.pt')\n",
    "\n",
    "# Bad - breaks on different environments\n",
    "torch.save(model, './models/model.pt')\n",
    "```\n",
    "\n",
    "### 2. Set `save_total_limit=1` for HF Trainer\n",
    "Every. Single. Time.\n",
    "\n",
    "### 3. Clean Up After Grades Are Posted\n",
    "Old homework models can be deleted once you have your grade.\n",
    "\n",
    "### 4. Don't Download the Same Model Twice\n",
    "If a notebook asks you to load a pretrained model, it's cached after first download. Running the cell again uses the cache.\n",
    "\n",
    "### 5. Use Compute Server for Training\n",
    "The compute server has more local storage and faster GPUs. Use it for training, then your models sync back.\n",
    "\n",
    "### 6. Check Storage Before Long Training Runs\n",
    "Run Storage_Cleanup.ipynb before starting a multi-hour training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Path Variable | What Goes There | Safe to Delete? |\n",
    "|--------------|-----------------|------------------|\n",
    "| `DATA_PATH` | Datasets | ✅ Yes - re-downloads |\n",
    "| `CACHE_PATH` | Pretrained models | ✅ Yes - re-downloads |\n",
    "| `MODELS_PATH` | YOUR checkpoints | ⚠️ After grades posted |\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. Use `DATA_PATH`, `MODELS_PATH`, `CACHE_PATH` - never hardcode paths\n",
    "2. Always use `save_total_limit=1` with HuggingFace Trainer\n",
    "3. Run `Storage_Cleanup.ipynb` when you need space\n",
    "4. Cached models re-download automatically - don't worry about deleting them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
