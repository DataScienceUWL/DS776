{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TOKEN = os.environ.get('HF_TOKEN')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Specify the model name or path\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\" # 1B model\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\" # 3B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import TextWrapper\n",
    "\n",
    "def wrap_print_text(print):\n",
    "    \"\"\"Adapted from: https://stackoverflow.com/questions/27621655/how-to-overload-print-function-to-expand-its-functionality/27621927\"\"\"\n",
    "\n",
    "    def wrapped_func(text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        wrapper = TextWrapper(\n",
    "            width=80,\n",
    "            break_long_words=True,\n",
    "            break_on_hyphens=False,\n",
    "            replace_whitespace=False,\n",
    "        )\n",
    "        return print(\"\\n\".join(wrapper.fill(line) for line in text.split(\"\\n\")))\n",
    "\n",
    "    return wrapped_func\n",
    "\n",
    "# Wrap the print function\n",
    "print = wrap_print_text(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_model(model_name=MODEL_NAME, device='cuda', token=TOKEN):\n",
    "    \"\"\"\n",
    "    Load the LLaMA model and tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,token=token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",  # Automatically selects FP16 if GPU is used\n",
    "        device_map=\"auto\",    # Automatically maps the model to GPU\n",
    "        token=token\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_llama_response(prompt, model, tokenizer, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate a response to a given prompt using the LLaMA model.\n",
    "    \"\"\"\n",
    "    # Ensure the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,  # Ensures padding is applied if needed\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "def unload_model(model):\n",
    "    \"\"\"\n",
    "    Unload the model and clear GPU memory.\n",
    "    \"\"\"\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b9fbeb545e4f72bf2e62ed578d91e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "response = generate_llama_response(\"How many moons does Mars have?\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many moons does Mars have? Two, Phobos and Deimos.\n",
      "Mars has two small moons, Phobos and Deimos. While the planet Earth has one\n",
      "large moon and several smaller natural satellites, Mars has only two of these\n",
      "smaller bodies orbiting it. Both moons are thought to be captured asteroids and\n",
      "are quite small, with diameters of only about 22 kilometers and 12 kilometers,\n",
      "respectively.\n",
      "Phobos, the larger moon, orbits Mars at a very close distance of about 6,000\n",
      "kilometers, which is much closer than the distance between the Earth and the\n",
      "Moon. Deimos, the smaller moon, orbits Mars at a distance of about 20,000\n",
      "kilometers.\n",
      "Phobos is a captured asteroid that is thought to have originated from the\n",
      "asteroid belt between the orbits of Mars and Jupiter. Its orbit is slowly\n",
      "decaying, and it is expected to crash into Mars within the next 50 million\n",
      "years.\n",
      "Deimos, on the other hand, is\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
