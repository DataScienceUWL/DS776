{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Homework 10: Named Entity Recognition\n",
    "\n",
    "**Total Points: 50**\n",
    "- Reading Questions: 8 points\n",
    "- Part 1 (Named Entities for Analysis): 6 points\n",
    "- Part 2 (Fine-tune BERT Models): 12 points\n",
    "- Part 3 (LLM for NER): 12 points\n",
    "- Part 4 (Comparison): 8 points\n",
    "- Part 5 (Reflection): 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Reading Questions (8 points)\n",
    "\n",
    "Answer the following questions based on Chapter 4: Multilingual Named Entity Recognition from *Natural Language Processing with Transformers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "**Question 1 (2 points):** Explain the BIO tagging scheme used in Named Entity Recognition. What do the B-, I-, and O tags represent, and why is this tagging scheme necessary for NER tasks instead of simply labeling entity types?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "**Question 2 (2 points):** Describe the tokenization pipeline used in transformer models for NER. What are the four main steps in this pipeline, and what is the purpose of each step? How does this process prepare text for NER tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "**Question 3 (2 points):** Explain the difference between token-level and entity-level evaluation for NER. Why is entity-level F1 score (using metrics like seqeval) generally preferred over token-level accuracy for evaluating NER models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "**Question 4 (2 points):** What are nested entities in NER, and why do they pose a challenge for traditional sequence labeling approaches? Provide an example of nested entities and explain how they complicate the BIO tagging scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 1 - Using Named Entities for Analysis (6 points)\n",
    "\n",
    "NER is often used to look for trends or to do other analysis on text data. Once you have the NER tags you can use them to extract the entities from the text to do analysis.\n",
    "\n",
    "Here we'll use a dataset of made-up movie reviews. The idea is to use the entity tags to extract the actors and directors from the reviews, then to figure out which actors and directors are most likely to be involved with positive sentiment movies and negative sentiment movies. We'll load the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hobbes99/fake_movie_reviews_ner_sentiment\"\n",
    ")\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "Here's an entry in the training set to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "Notice that NER tags are stored as integers corresponding to their indices in `label_list`. You'll need to use those tags to extract the actor and director names. You can also extract the sentiment.\n",
    "\n",
    "For the training split, find and display in order:\n",
    "* The three actors most likely to appear in positive films.\n",
    "* The three actors most likely to appear in negative films.\n",
    "* The three directors most likely to have directed positive films.\n",
    "* The three directors most likely to have directed negative films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract entities from training set\n\n# Your code here:\n# 1. Create empty lists or dictionaries to store:\n#    - Actors with positive sentiment\n#    - Actors with negative sentiment\n#    - Directors with positive sentiment\n#    - Directors with negative sentiment\n# 2. Loop through the training dataset\n# 3. For each example, extract:\n#    - The sentiment (0 or 1)\n#    - Tokens tagged as B-ACTOR or I-ACTOR\n#    - Tokens tagged as B-DIRECTOR or I-DIRECTOR\n# 4. Keep track of counts for each actor/director by sentiment\n\n# Hint: You'll need to:\n# - Convert token lists to strings (join consecutive I- tags with B- tag)\n# - Track how many times each actor appears in positive vs negative reviews\n# - Use label_list to convert numeric tags to string labels\n\n# Example structure:\n# from collections import defaultdict\n# \n# actor_positive = defaultdict(int)\n# actor_negative = defaultdict(int)\n# director_positive = defaultdict(int)\n# director_negative = defaultdict(int)\n#\n# for example in dataset[\"train\"]:\n#     tokens = example[\"tokens\"]\n#     ner_tags = example[\"ner_tags\"]\n#     sentiment = example[\"sentiment\"]\n#     \n#     # Your extraction logic here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate proportions and find top 3\n\n# Your code here:\n# 1. For each actor/director, calculate:\n#    total_appearances = positive_count + negative_count\n#    positive_proportion = positive_count / total_appearances\n# 2. Sort by positive proportion (descending for positive, ascending for negative)\n# 3. Select top 3 for each category\n# 4. Display results with counts and proportions\n\n# Example:\n# import pandas as pd\n# \n# # Create DataFrame for analysis\n# actor_stats = []\n# for actor in set(list(actor_positive.keys()) + list(actor_negative.keys())):\n#     pos_count = actor_positive[actor]\n#     neg_count = actor_negative[actor]\n#     total = pos_count + neg_count\n#     if total >= 2:  # Filter out rare appearances\n#         actor_stats.append({\n#             'name': actor,\n#             'positive': pos_count,\n#             'negative': neg_count,\n#             'total': total,\n#             'positive_pct': pos_count / total\n#         })\n# \n# df_actors = pd.DataFrame(actor_stats)\n# df_actors_sorted = df_actors.sort_values('positive_pct', ascending=False)\n# \n# print(\"Top 3 actors in positive films:\")\n# display(df_actors_sorted.head(3))\n# \n# print(\"\\nTop 3 actors in negative films:\")\n# display(df_actors_sorted.tail(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 2 - Fine Tuning Two BERT NER Models (12 points)\n",
    "\n",
    "The MIT Movie Corpus is designed for movie-related NER tasks and includes the following entity types in BIO format:\n",
    "- **Actor**: Names of actors or actresses (e.g., \"Leonardo DiCaprio\").\n",
    "- **Character**: Names of characters in movies (e.g., \"Jack Dawson\").\n",
    "- **Director**: Names of movie directors (e.g., \"Christopher Nolan\").\n",
    "- **Genre**: Movie genres (e.g., \"Action\", \"Drama\").\n",
    "- **Title**: Titles of movies (e.g., \"Inception\").\n",
    "- **Year**: Year the movie was made.\n",
    "\n",
    "The original movie corpus includes more entity types, but we've produced a simplified version for this assignment.\n",
    "\n",
    "In this part of the assignment you should fine-tune \"distilbert-base-uncased\" and \"bert-base-uncased\" for NER on the dataset \"hobbes99/mit-movie-ner-simplified\". The dataset has \"train\" and \"valid\" splits. Use the \"train\" split for fine-tuning and evaluate the metrics using seqeval as shown in the lesson.\n",
    "* Figure out a way to plot precision, recall, and F1 by entity type.\n",
    "* Find two movie reviews on the internet and run inference on them to extract the named entities.\n",
    "* Write a brief summary of the results. Include answers to:\n",
    "    * Which entity types does the model struggle with?\n",
    "    * Which does it do well on?\n",
    "* The \"distilbert-base-uncased\" model is a distilled version of the \"bert-base-uncased\" model (distillation means a smaller model that was trained using the larger trained model as a \"teacher\"). The \"bert-base-uncased\" model should lead to better results here. Does it? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\nfrom datasets import load_dataset\n\n# Your code here:\n# 1. Load the \"hobbes99/mit-movie-ner-simplified\" dataset\n# 2. Examine the 'train' and 'valid' splits\n# 3. Print an example to see the structure\n# 4. Get the label names from the dataset features\n\n# Example:\n# dataset = load_dataset(\"hobbes99/mit-movie-ner-simplified\")\n# print(\"Train examples:\", len(dataset[\"train\"]))\n# print(\"Valid examples:\", len(dataset[\"valid\"]))\n# print(\"\\nFirst training example:\")\n# print(dataset[\"train\"][0])\n# \n# label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n# print(\"\\nNER labels:\", label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare for fine-tuning - Tokenization\nfrom transformers import AutoTokenizer\n\n# Your code here:\n# 1. Load two tokenizers:\n#    - \"distilbert-base-uncased\"\n#    - \"bert-base-uncased\"\n# 2. Create a tokenization function that:\n#    - Tokenizes the tokens (use is_split_into_words=True)\n#    - Aligns the NER labels with tokenized output\n#    - Handles subword tokens (use -100 for ignored tokens)\n# 3. Apply tokenization to both train and valid splits\n\n# Example structure:\n# tokenizer_distilbert = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n# tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n# \n# def tokenize_and_align_labels(examples, tokenizer):\n#     tokenized_inputs = tokenizer(\n#         examples[\"tokens\"],\n#         truncation=True,\n#         is_split_into_words=True,\n#         padding=\"max_length\",\n#         max_length=128\n#     )\n#     \n#     labels = []\n#     for i, label in enumerate(examples[\"ner_tags\"]):\n#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n#         label_ids = []\n#         previous_word_idx = None\n#         \n#         for word_idx in word_ids:\n#             if word_idx is None:\n#                 label_ids.append(-100)\n#             elif word_idx != previous_word_idx:\n#                 label_ids.append(label[word_idx])\n#             else:\n#                 label_ids.append(-100)  # Set subword tokens to -100\n#             previous_word_idx = word_idx\n#         \n#         labels.append(label_ids)\n#     \n#     tokenized_inputs[\"labels\"] = labels\n#     return tokenized_inputs\n# \n# # Apply to datasets\n# tokenized_train_distilbert = dataset[\"train\"].map(\n#     lambda x: tokenize_and_align_labels(x, tokenizer_distilbert),\n#     batched=True\n# )\n# tokenized_valid_distilbert = dataset[\"valid\"].map(\n#     lambda x: tokenize_and_align_labels(x, tokenizer_distilbert),\n#     batched=True\n# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up metrics for evaluation\nimport evaluate\nimport numpy as np\n\n# Your code here:\n# 1. Load the seqeval metric\n# 2. Create a compute_metrics function that:\n#    - Extracts predictions and labels\n#    - Removes ignored tokens (-100)\n#    - Converts numeric labels to string labels\n#    - Computes precision, recall, F1 overall and per-entity\n\n# Example:\n# seqeval = evaluate.load(\"seqeval\")\n# \n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     predictions = np.argmax(predictions, axis=2)\n#     \n#     # Remove ignored index (special tokens) and convert to labels\n#     true_predictions = [\n#         [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n#         for prediction, label in zip(predictions, labels)\n#     ]\n#     true_labels = [\n#         [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n#         for prediction, label in zip(predictions, labels)\n#     ]\n#     \n#     results = seqeval.compute(predictions=true_predictions, references=true_labels)\n#     return {\n#         \"precision\": results[\"overall_precision\"],\n#         \"recall\": results[\"overall_recall\"],\n#         \"f1\": results[\"overall_f1\"],\n#         \"accuracy\": results[\"overall_accuracy\"],\n#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fine-tune distilbert-base-uncased\nfrom transformers import (\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForTokenClassification\n)\n\n# Your code here:\n# 1. Load the model with num_labels matching your dataset\n# 2. Create TrainingArguments (output_dir, num_epochs, batch_size, etc.)\n# 3. Create a DataCollator for token classification\n# 4. Create Trainer with model, args, datasets, tokenizer, data_collator, compute_metrics\n# 5. Train the model\n\n# Example:\n# num_labels = len(label_names)\n# \n# model_distilbert = AutoModelForTokenClassification.from_pretrained(\n#     \"distilbert-base-uncased\",\n#     num_labels=num_labels,\n#     id2label={i: label for i, label in enumerate(label_names)},\n#     label2id={label: i for i, label in enumerate(label_names)}\n# )\n# \n# training_args = TrainingArguments(\n#     output_dir=\"./results_distilbert\",\n#     evaluation_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     learning_rate=2e-5,\n#     per_device_train_batch_size=16,\n#     per_device_eval_batch_size=16,\n#     num_train_epochs=3,\n#     weight_decay=0.01,\n#     load_best_model_at_end=True,\n# )\n# \n# data_collator = DataCollatorForTokenClassification(tokenizer_distilbert)\n# \n# trainer_distilbert = Trainer(\n#     model=model_distilbert,\n#     args=training_args,\n#     train_dataset=tokenized_train_distilbert,\n#     eval_dataset=tokenized_valid_distilbert,\n#     tokenizer=tokenizer_distilbert,\n#     data_collator=data_collator,\n#     compute_metrics=compute_metrics,\n# )\n# \n# trainer_distilbert.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate distilbert and get per-entity metrics\n\n# Your code here:\n# 1. Run evaluation on the valid set\n# 2. Get predictions for detailed analysis\n# 3. Calculate precision, recall, F1 for each entity type\n# 4. Create a visualization (bar chart) comparing metrics by entity\n\n# Example:\n# # Get predictions\n# predictions_distilbert = trainer_distilbert.predict(tokenized_valid_distilbert)\n# preds_distilbert = np.argmax(predictions_distilbert.predictions, axis=2)\n# \n# # Convert to label strings\n# true_predictions_distilbert = [\n#     [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n#     for prediction, label in zip(preds_distilbert, predictions_distilbert.label_ids)\n# ]\n# true_labels_distilbert = [\n#     [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n#     for prediction, label in zip(preds_distilbert, predictions_distilbert.label_ids)\n# ]\n# \n# # Get detailed results\n# results_detailed_distilbert = seqeval.compute(\n#     predictions=true_predictions_distilbert,\n#     references=true_labels_distilbert\n# )\n# \n# print(\"DistilBERT Results:\")\n# print(f\"Overall F1: {results_detailed_distilbert['overall_f1']:.3f}\")\n# print(\"\\nPer-entity results:\")\n# for entity_type in results_detailed_distilbert.keys():\n#     if entity_type not in ['overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']:\n#         print(f\"{entity_type}: {results_detailed_distilbert[entity_type]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create visualization of per-entity performance\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Your code here:\n# 1. Extract precision, recall, F1 for each entity type\n# 2. Create a DataFrame with entity types and metrics\n# 3. Plot a grouped bar chart comparing precision, recall, F1\n\n# Example:\n# # Extract entity metrics (skip 'overall_*' keys)\n# entity_metrics = []\n# for key, value in results_detailed_distilbert.items():\n#     if not key.startswith('overall_'):\n#         entity_metrics.append({\n#             'Entity': key,\n#             'Precision': value.get('precision', 0),\n#             'Recall': value.get('recall', 0),\n#             'F1': value.get('f1', 0)\n#         })\n# \n# df_metrics = pd.DataFrame(entity_metrics)\n# \n# # Plot\n# df_metrics.plot(x='Entity', y=['Precision', 'Recall', 'F1'], kind='bar', figsize=(10, 6))\n# plt.title('DistilBERT NER Performance by Entity Type')\n# plt.ylabel('Score')\n# plt.xlabel('Entity Type')\n# plt.legend(loc='lower right')\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Repeat for bert-base-uncased\n\n# Your code here:\n# 1. Tokenize the dataset with bert-base-uncased tokenizer\n# 2. Load bert-base-uncased model\n# 3. Create new Trainer with BERT model\n# 4. Train the BERT model\n# 5. Evaluate and compare with DistilBERT\n\n# (Follow same steps as DistilBERT - cells above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Run inference on movie reviews from internet\nfrom transformers import pipeline\n\n# Your code here:\n# 1. Find 2 movie reviews from the internet (IMDB, Rotten Tomatoes, etc.)\n# 2. Create a NER pipeline with your fine-tuned model\n# 3. Run inference on the reviews\n# 4. Display the extracted entities with their types\n\n# Example:\n# # Create NER pipeline\n# ner_pipeline_distilbert = pipeline(\n#     \"ner\",\n#     model=model_distilbert,\n#     tokenizer=tokenizer_distilbert,\n#     aggregation_strategy=\"simple\"\n# )\n# \n# # Example review\n# review_1 = '''\n# The new Christopher Nolan film starring Leonardo DiCaprio is a masterpiece.\n# Set in 2010, this Science Fiction thriller takes viewers on a mind-bending journey.\n# '''\n# \n# review_2 = '''\n# I watched The Dark Knight last night. Heath Ledger's performance as the Joker\n# was incredible. This 2008 Action film directed by Christopher Nolan is a must-see.\n# '''\n# \n# # Run NER\n# entities_1 = ner_pipeline_distilbert(review_1)\n# entities_2 = ner_pipeline_distilbert(review_2)\n# \n# print(\"Review 1 entities:\")\n# for entity in entities_1:\n#     print(f\"  {entity['word']}: {entity['entity_group']} (score: {entity['score']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Compare DistilBERT vs BERT\n\n# Your code here:\n# 1. Create a comparison table with:\n#    - Model name\n#    - Overall F1 score\n#    - Training time (approximate)\n#    - Model size (parameters)\n#    - Best/worst entity types\n# 2. Discuss which model performed better\n# 3. Analyze whether BERT's larger size justified better performance\n\n# Example:\n# comparison_data = {\n#     'Model': ['DistilBERT', 'BERT'],\n#     'Parameters': ['66M', '110M'],\n#     'Overall F1': [0.XX, 0.XX],  # Fill with your results\n#     'Training Time': ['X min', 'Y min'],\n#     'Best Entity': ['...', '...'],\n#     'Worst Entity': ['...', '...']\n# }\n# \n# df_comparison = pd.DataFrame(comparison_data)\n# display(df_comparison)\n# \n# print(\"\\nAnalysis:\")\n# print(\"BERT performed [better/worse] than DistilBERT by [X]% F1 score.\")\n# print(\"The improvement [was/was not] worth the extra training time because...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Part 3 - Using an LLM for NER (12 points)\n",
    "\n",
    "For the first 100 texts in the \"valid\" split, mimic what we did in the lesson to extract the \"Actor\", \"Character\", \"Director\", \"Genre\", \"Title\" and \"Year\" entities using an LLM. Start with just a few examples to refine your prompt and instructions, then ramp up to 100 or more examples. Get the final evaluation metrics as shown in the lesson.\n",
    "\n",
    "**Hint:** You can import the `llm_ner_extractor` function from `Lesson_10_Helpers` to streamline your LLM-based extraction, similar to how we used `llm_classifier` in Lesson 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare validation subset\nfrom Lesson_10_Helpers import llm_ner_extractor\n\n# Your code here:\n# 1. Get first 100 examples from valid split\n# 2. Extract texts and true labels\n# 3. Start with 2-3 examples to test your prompt\n\n# Example:\n# valid_subset = dataset[\"valid\"].select(range(100))\n# valid_texts = [\" \".join(example[\"tokens\"]) for example in valid_subset]\n# valid_labels = valid_subset[\"ner_tags\"]\n# \n# # Test with just 3 examples first\n# test_texts = valid_texts[:3]\n# print(\"Testing with 3 examples:\")\n# for i, text in enumerate(test_texts):\n#     print(f\"\\nExample {i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Design prompt for LLM-based NER\n\n# Your code here:\n# 1. Create system_prompt explaining the task\n# 2. Create prompt_template with:\n#    - Instructions to extract entities\n#    - Entity types to look for: Actor, Character, Director, Genre, Title, Year\n#    - Request structured output (JSON format recommended)\n#    - Include the {text} placeholder\n\n# Example:\n# system_prompt = '''You are an expert at identifying named entities in movie-related text.\n# Your task is to extract entities and classify them into the following types:\n# - Actor: Names of actors or actresses\n# - Character: Names of characters in movies\n# - Director: Names of movie directors\n# - Genre: Movie genres (Action, Drama, Comedy, etc.)\n# - Title: Titles of movies\n# - Year: Years when movies were made\n# '''\n# \n# prompt_template = '''Extract all named entities from the following text and classify them by type.\n# \n# Return ONLY a JSON object with this structure:\n# {{\n#   \"Actor\": [\"name1\", \"name2\"],\n#   \"Character\": [\"name1\"],\n#   \"Director\": [\"name1\"],\n#   \"Genre\": [\"genre1\"],\n#   \"Title\": [\"title1\"],\n#   \"Year\": [\"year1\"]\n# }}\n# \n# If no entities of a type are found, use an empty list [].\n# \n# Text: {text}\n# \n# JSON output:'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Test prompt on small subset with llm_ner_extractor\n\n# Your code here:\n# 1. Use llm_ner_extractor function (similar to llm_classifier from Lesson 8)\n# 2. Start with a small API model or local model\n# 3. Test on 3-5 examples\n# 4. Refine your prompt based on results\n# 5. Parse JSON output and convert to BIO format\n\n# Example:\n# from introdl import llm_generate\n# import json\n# \n# # Test with one model first\n# model_name = \"gemini-flash-lite\"  # or try a local model\n# \n# # Generate predictions for test examples\n# predictions_raw = []\n# for text in test_texts:\n#     prompt = prompt_template.format(text=text)\n#     response = llm_generate(\n#         prompt=prompt,\n#         system_prompt=system_prompt,\n#         model=model_name,\n#         temperature=0.1,  # Low temperature for more consistent outputs\n#         max_tokens=500\n#     )\n#     predictions_raw.append(response)\n#     print(f\"\\nText: {text}\")\n#     print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert LLM outputs to BIO format\n\n# Your code here:\n# 1. Parse JSON from LLM response\n# 2. Match entity spans to token positions\n# 3. Convert to BIO format (B-ACTOR, I-ACTOR, etc.)\n# 4. Handle errors/malformed JSON gracefully\n\n# This is complex! You may want to use llm_ner_extractor from Lesson_10_Helpers\n# which handles the conversion for you.\n\n# Example using llm_ner_extractor:\n# predictions_bio = llm_ner_extractor(\n#     model_name=model_name,\n#     texts=test_texts,\n#     system_prompt=system_prompt,\n#     prompt_template=prompt_template,\n#     label_names=label_names,\n#     estimate_cost=True\n# )\n# \n# print(\"\\nBIO predictions:\", predictions_bio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Scale up to 100 examples and evaluate\n\n# Your code here:\n# 1. Once prompt is refined, run on all 100 validation examples\n# 2. Convert predictions to BIO format\n# 3. Calculate metrics using seqeval (like Part 2)\n# 4. Generate classification report\n\n# Example:\n# # Run on all 100 examples\n# predictions_llm = llm_ner_extractor(\n#     model_name=\"gemini-flash-lite\",\n#     texts=valid_texts,  # All 100 texts\n#     system_prompt=system_prompt,\n#     prompt_template=prompt_template,\n#     label_names=label_names,\n#     estimate_cost=True\n# )\n# \n# # Evaluate\n# results_llm = seqeval.compute(\n#     predictions=predictions_llm,\n#     references=[[label_names[l] for l in labels] for labels in valid_labels]\n# )\n# \n# print(\"\\nLLM NER Results:\")\n# print(f\"Overall F1: {results_llm['overall_f1']:.3f}\")\n# print(f\"Precision: {results_llm['overall_precision']:.3f}\")\n# print(f\"Recall: {results_llm['overall_recall']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Part 4 - Comparison (8 points)\n",
    "\n",
    "* Compare the results of the two entity recognition techniques (fine-tuned BERT models vs LLM zero-shot) both quantitatively and qualitatively.\n",
    "* Consider the difficulty of obtaining labeled data in your comparison. It's time-consuming and/or costly to get tagged text, but that's not necessary for the LLM approach which may be less accurate.\n",
    "* Which approach would you choose for a production system and why? Consider accuracy, speed, cost, and maintenance requirements.\n",
    "* Give a brief summary of what you learned in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR COMPARISON AND SUMMARY HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Part 5 - Reflection (2 points)\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for the lesson? Why?\n",
    "\n",
    "\ud83d\udcdd **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}