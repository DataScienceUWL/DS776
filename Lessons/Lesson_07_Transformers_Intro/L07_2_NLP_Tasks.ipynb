{
 "cells": [
  {
   "cell_type": "code",
   "source": "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n%run ../Course_Tools/auto_update_introdl.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:18.532720Z",
     "iopub.status.busy": "2025-08-26T00:43:18.532532Z",
     "iopub.status.idle": "2025-08-26T00:43:33.211450Z",
     "shell.execute_reply": "2025-08-26T00:43:33.210391Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from introdl.utils import get_device, wrap_print_text, config_paths_keys\n",
    "from introdl.nlp import llm_configure, llm_generate, clear_pipeline, print_pipeline_info, display_markdown, llm_list_models\n",
    "\n",
    "# overload print to wrap text\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "paths = config_paths_keys()\n",
    "\n",
    "# we use the Mistral 7B model for this notebook, but you can any supported model, e.g. llama-3p1-8B, llama-3p2-3B, gemini-flash-lite, etc.\n",
    "# use llm_list_models() to see all available models\n",
    "LLM_MODEL = 'mistral-7B'\n",
    "llm_config = llm_configure(LLM_MODEL) # load model configuration for use throughout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L07_2_NLP_Tasks Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_2_nlp_tasks\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_2_nlp_tasks\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/omj2ldze713\" target=\"_blank\">Open Descript version of video in new tab</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Tasks with Transformer Models\n",
    "\n",
    "In this notebook we'll demonstrate solutions to some common Natural Language Processing (NLP) tasks that use transformer models.  We expand on the material in our NLP textbook Chapter 1 - Hello Transformers.  We'll add a little background about the underlying models.  We'll also demonstrate how these same tasks come be done using a large language model with either \"zero-shot prompting\" or \"few-shot prompting\".\n",
    "\n",
    "Over the next five lessons we'll go into some of these NLP tasks in detail and a learn a bit about the transformer neural network architecture.  For each of the NLP tasks that follows we'll demonstrate how to do the task two ways.  The first is by using a pre-trained transformer-based model downloaded from HuggingFace.  In the second approach we'll use a a large language model and prompting.\n",
    "\n",
    "Using LLMs for various NLP tasks is common when there isn't much labeled data available.  Zero-shot prompting means that no examples are provided to the LLM.  Few-shot prompting means that a small number of examples are provided to the LLM.  In this notebook we'll demonstrate zero-shot prompting, but in the lessons to come we'll include few-shot prompting examples.\n",
    "\n",
    "Throughout this notebook we'll use the following customer feedback message that as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:33.214197Z",
     "iopub.status.busy": "2025-08-26T00:43:33.213694Z",
     "iopub.status.idle": "2025-08-26T00:43:33.218096Z",
     "shell.execute_reply": "2025-08-26T00:43:33.217310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day\n",
      "delivery, but after three days, I hadn’t even received a shipping update. After\n",
      "waiting 45 minutes on hold, customer service told me there was a stock issue—yet\n",
      "no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead. The support rep was apologetic but said an exchange would take another\n",
      "two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer. To add insult to injury, the customer service\n",
      "representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake.\n",
      "The entire experience has been incredibly disappointing and has left me\n",
      "questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and\n",
      "efficiency. I hope this feedback reaches someone who can make a difference, as\n",
      "no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three days, I hadn’t even received a shipping update. After waiting 45 minutes on hold, customer service told me there was a stock issue—yet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. To add insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. I had to explain my situation multiple times before they even acknowledged the mistake. The entire experience has been incredibly disappointing and has left me questioning whether I should ever shop with Tech Haven again. \n",
    "\n",
    "It's baffling how a company can operate with such a lack of transparency and efficiency. I hope this feedback reaches someone who can make a difference, as no customer should have to go through what I did. Tech Haven, you need to do better! Sincerely, Jamie.\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Classification\n",
    "\n",
    "Text classification is the process of assigning predefined categories to text. It involves analyzing the content of the text and categorizing it based on its subject, sentiment, or other criteria. One common application of text classification is sentiment analysis, which determines the sentiment expressed in a piece of text, such as positive, negative, or neutral. Sentiment analysis is widely used in customer feedback analysis, social media monitoring, and market research to gauge public opinion and customer satisfaction.\n",
    "\n",
    "### Sentiment Analysis with a Specialized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for sentiment analysis and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:33.220072Z",
     "iopub.status.busy": "2025-08-26T00:43:33.219895Z",
     "iopub.status.idle": "2025-08-26T00:43:34.243427Z",
     "shell.execute_reply": "2025-08-26T00:43:34.242537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english, Size: 66,955,010 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9989209175109863}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "print_pipeline_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a \"BERT\" model correctly classified the customer feedback as negative. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model developed by Google. It is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows BERT to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. The particular model used here is a distilled BERT model that has been fine-tuned on a sentiment dataset. A distilled model is a smaller, faster, and more efficient version of a larger model, trained using knowledge distillation, where the smaller model learns to mimic the outputs of the larger one while retaining most of its performance. In Lesson 9, we'll learn more about the family of transformer models called encoders, which include BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's never a bad idea to remove models from memory when they aren't being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:34.245423Z",
     "iopub.status.busy": "2025-08-26T00:43:34.245230Z",
     "iopub.status.idle": "2025-08-26T00:43:34.915933Z",
     "shell.execute_reply": "2025-08-26T00:43:34.915078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(sentiment_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentiment Analysis with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "A system prompt is used to give instructions to an LLM while a user prompt is the specific input you want the LLM to respond to.  Here we define a system prompt for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:34.917831Z",
     "iopub.status.busy": "2025-08-26T00:43:34.917619Z",
     "iopub.status.idle": "2025-08-26T00:43:38.262730Z",
     "shell.execute_reply": "2025-08-26T00:43:38.261791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "response_zero_shot = llm_generate(llm_config, user_prompt, system_prompt=system_prompt)\n",
    "print(response_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also handle batches of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:38.265132Z",
     "iopub.status.busy": "2025-08-26T00:43:38.264867Z",
     "iopub.status.idle": "2025-08-26T00:43:53.851512Z",
     "shell.execute_reply": "2025-08-26T00:43:53.850636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65ef90340c64f5398337dc7bec9aa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Local Generation:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fast shipping and great customer support. Highly recommend!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The item arrived damaged and the return process was a nightmare.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: I'm very satisfied with my purchase. Will buy again.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The website is user-friendly and the prices are unbeatable.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: Received the wrong item and customer service was unhelpful.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Fantastic experience from start to finish.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The product is okay, but not worth the price.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Excellent quality and quick delivery. Very happy!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The product works as expected, nothing more, nothing less.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: I have mixed feelings about the service; it was both good and bad.\n",
      "Sentiment: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customer_comments = [\n",
    "    \"Fast shipping and great customer support. Highly recommend!\",\n",
    "    \"The item arrived damaged and the return process was a nightmare.\",\n",
    "    \"I'm very satisfied with my purchase. Will buy again.\",\n",
    "    \"The website is user-friendly and the prices are unbeatable.\",\n",
    "    \"Received the wrong item and customer service was unhelpful.\",\n",
    "    \"Fantastic experience from start to finish.\",\n",
    "    \"The product is okay, but not worth the price.\",\n",
    "    \"Excellent quality and quick delivery. Very happy!\",\n",
    "    \"The product works as expected, nothing more, nothing less.\",\n",
    "    \"I have mixed feelings about the service; it was both good and bad.\"\n",
    "]\n",
    "\n",
    "user_prompts = [f\"Text: {comment}\\nSentiment:\" for comment in customer_comments]\n",
    "\n",
    "responses_zero_shot = llm_generate(llm_config, user_prompts, system_prompt=system_prompt)\n",
    "\n",
    "for comment, response_zero_shot in zip(customer_comments, responses_zero_shot):\n",
    "    print(f\"Text: {comment}\\nSentiment: {response_zero_shot}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text classification more in Lesson 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to Write Better Prompts\n",
    "\n",
    "There are many prompt engineering resources available on the internet.  I encourage you to look at those as needed.  I've also had good luck asking ChatGPT how to craft prompts.  One particularly useful resource is [ChatGPT Prompt Engineering for Developers](https://app.datacamp.com/learn/courses/chatgpt-prompt-engineering-for-developers) on DataCamp.  It's not free (though I'm trying to see if we can get free acess - I'll announce it in Piazza if we are able to), but it is cheap.  I've viewed parts of this course and found it to be a very good introduction to programatic prompt writing.  It's tailored to ChatGPT, but you can use the OpenAI API with Gemini as well.  Working through this short course (they say it takes 4 hours) is likely worthwhile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Practical examples of NER include:\n",
    "- **Business Application**: Extracting company names, dates, and monetary amounts from financial reports to automate data entry and analysis.\n",
    "- **Healthcare**: Identifying patient names, medical conditions, and treatment dates from clinical notes to improve patient record management.\n",
    "- **News Aggregation**: Categorizing and tagging entities like people, places, and events in news articles to enhance search and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for NER and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:53.853339Z",
     "iopub.status.busy": "2025-08-26T00:43:53.853145Z",
     "iopub.status.idle": "2025-08-26T00:43:56.291882Z",
     "shell.execute_reply": "2025-08-26T00:43:56.290706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Named Entity Recognition**\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889 parameters\n",
      "\n",
      "[{'entity_group': 'MISC', 'score': np.float32(0.990973), 'word': 'Samsung Galaxy\n",
      "S24 Ultra', 'start': 14, 'end': 38}, {'entity_group': 'ORG', 'score':\n",
      "np.float32(0.994846), 'word': 'Tech Haven', 'start': 44, 'end': 54},\n",
      "{'entity_group': 'MISC', 'score': np.float32(0.9928634), 'word': 'Google Pixel 8\n",
      "Pro', 'start': 323, 'end': 341}, {'entity_group': 'ORG', 'score':\n",
      "np.float32(0.9964845), 'word': 'Tech Haven', 'start': 863, 'end': 873},\n",
      "{'entity_group': 'ORG', 'score': np.float32(0.9887396), 'word': 'Tech Haven',\n",
      "'start': 1089, 'end': 1099}, {'entity_group': 'PER', 'score':\n",
      "np.float32(0.9780477), 'word': 'Jamie', 'start': 1135, 'end': 1140}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\\n\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print_pipeline_info(ner_pipeline)\n",
    "print(\"\")\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That output is hard to read, but we can easily convert it to a Pandas data frame for display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:56.294124Z",
     "iopub.status.busy": "2025-08-26T00:43:56.293807Z",
     "iopub.status.idle": "2025-08-26T00:43:56.304805Z",
     "shell.execute_reply": "2025-08-26T00:43:56.303849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990973</td>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.994846</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.992863</td>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>323</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996485</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>863</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.988740</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>1089</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.978048</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1135</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                      word  start   end\n",
       "0         MISC  0.990973  Samsung Galaxy S24 Ultra     14    38\n",
       "1          ORG  0.994846                Tech Haven     44    54\n",
       "2         MISC  0.992863        Google Pixel 8 Pro    323   341\n",
       "3          ORG  0.996485                Tech Haven    863   873\n",
       "4          ORG  0.988740                Tech Haven   1089  1099\n",
       "5          PER  0.978048                     Jamie   1135  1140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.DataFrame(ner_result)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"BERT\" model used here is the `dbmdz/bert-large-cased-finetuned-conll03-english` model, which has been fine-tuned on the CoNLL-2003 dataset for Named Entity Recognition (NER). This fine-tuning process allows the model to accurately identify and classify entities such as names of persons, organizations, locations, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:56.307086Z",
     "iopub.status.busy": "2025-08-26T00:43:56.306815Z",
     "iopub.status.idle": "2025-08-26T00:43:58.023959Z",
     "shell.execute_reply": "2025-08-26T00:43:58.023048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(ner_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for NER.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:58.025960Z",
     "iopub.status.busy": "2025-08-26T00:43:58.025755Z",
     "iopub.status.idle": "2025-08-26T00:44:45.483722Z",
     "shell.execute_reply": "2025-08-26T00:44:45.482911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"text\": \"Samsung Galaxy S24 Ultra\",\n",
      "      \"type\": \"Product\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Tech Haven\",\n",
      "      \"type\": \"Organization\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Google Pixel 8 Pro\",\n",
      "      \"type\": \"Product\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"$1,200\",\n",
      "      \"type\": \"CurrencyAmount\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Jamie\",\n",
      "      \"type\": \"Person\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "system_prompt = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "llm_config = llm_configure(LLM_MODEL)\n",
    "response_ner = llm_generate(llm_config, user_prompt, system_prompt=system_prompt)\n",
    "print(response_ner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Mistral-7B LLM returned a string containing json. Below we convert it to json (a dictionary -like format) and use a dataframe to display it.  Different LLM's will likely return different versions of the output which would need to be processed differently.  Some LLMs are capable of producing JSON with a particular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:45.485905Z",
     "iopub.status.busy": "2025-08-26T00:44:45.485611Z",
     "iopub.status.idle": "2025-08-26T00:44:45.495786Z",
     "shell.execute_reply": "2025-08-26T00:44:45.494660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1,200</td>\n",
       "      <td>CurrencyAmount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jamie</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text            type\n",
       "0  Samsung Galaxy S24 Ultra         Product\n",
       "1                Tech Haven    Organization\n",
       "2        Google Pixel 8 Pro         Product\n",
       "3                    $1,200  CurrencyAmount\n",
       "4                     Jamie          Person"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Clean the response_ner string\n",
    "response_ner = response_ner.strip('```json\\n').strip('\\n```')\n",
    "\n",
    "# Convert the cleaned response to a DataFrame for display\n",
    "ner_result = json.loads(response_ner)\n",
    "\n",
    "df = pd.DataFrame(ner_result['entities'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the LLM is similar to that of the specialized model from HuggingFace.  If we want different output from the LLM we could include instructions for that in our system prompt.\n",
    "\n",
    "In Lesson 10 we'll learn more about Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Question Answering\n",
    "\n",
    "Question Answering (QA) is a subtask of information retrieval and natural language understanding that involves automatically answering questions posed by humans in a natural language. QA systems can be designed to answer questions based on a given context or a large corpus of documents. The goal is to provide accurate and relevant answers to user queries.\n",
    "\n",
    "Practical examples of QA include:\n",
    "- **Customer Support**: Providing instant answers to customer queries based on a knowledge base or FAQ, improving response times and customer satisfaction.\n",
    "- **Education**: Assisting students by answering questions related to their coursework or providing explanations for complex topics.\n",
    "- **Healthcare**: Offering medical professionals quick access to information from medical literature or patient records to support clinical decision-making.\n",
    "- **Search Engines**: Enhancing search results by directly providing answers to user queries, rather than just a list of relevant documents.\n",
    "\n",
    "Here we will let the HuggingFace transformers library provide its default model for QA and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:45.498408Z",
     "iopub.status.busy": "2025-08-26T00:44:45.498231Z",
     "iopub.status.idle": "2025-08-26T00:44:46.507507Z",
     "shell.execute_reply": "2025-08-26T00:44:46.506695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Question Answering**\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450 parameters\n",
      "\n",
      "{'score': 0.4041374623775482, 'start': 218, 'end': 231, 'answer': 'a stock\n",
      "issue'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\\n\")\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "print_pipeline_info(qa_pipeline)\n",
    "print(\"\")\n",
    "question = \"What is the main issue?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:46.509470Z",
     "iopub.status.busy": "2025-08-26T00:44:46.509292Z",
     "iopub.status.idle": "2025-08-26T00:44:47.175036Z",
     "shell.execute_reply": "2025-08-26T00:44:47.174188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(qa_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for QA.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:47.176975Z",
     "iopub.status.busy": "2025-08-26T00:44:47.176790Z",
     "iopub.status.idle": "2025-08-26T00:45:09.158225Z",
     "shell.execute_reply": "2025-08-26T00:45:09.157329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main issues are delayed delivery, incorrect product shipment (Google Pixel 8\n",
      "Pro instead of Samsung Galaxy S24 Ultra), inadequate communication about the\n",
      "stock issue, prolonged resolution process resulting in further delay, and\n",
      "apparent indifference towards the customer's frustration by the customer service\n",
      "representative at Tech Haven.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. Be succinct.\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: What is the main issue?\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(llm_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:09.160737Z",
     "iopub.status.busy": "2025-08-26T00:45:09.160539Z",
     "iopub.status.idle": "2025-08-26T00:45:17.944279Z",
     "shell.execute_reply": "2025-08-26T00:45:17.943419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main issue: Poor service (delivery delay, incorrect product shipped, inadequate\n",
      "communication, unhelpful customer service)\n"
     ]
    }
   ],
   "source": [
    "### MAKE THIS AN EXERCISE TO GET A VERY CONCISE ANSWER\n",
    "\n",
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. \n",
    "Your answer should be a few words at most.\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: What is the main issue? Try to identify a single issue.\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(llm_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a lesson dedicated to question answering, but it's discussed in our NLP textbook in Chapter 7.  You could investigate this topic further in a project if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Translation\n",
    "\n",
    "The first transformer model in the paper \"Attention is All You Need\" was designed for the task of language translation. This model, known as the Transformer, introduced a novel architecture that relies entirely on self-attention mechanisms to process input sequences, making it highly effective for translation tasks. The Transformer model has since become the foundation for many state-of-the-art NLP models, including BERT, GPT, and others.\n",
    "\n",
    "Here we demonstrate how to use a pre-trained model from HuggingFace for translating English to Spanish.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:17.946147Z",
     "iopub.status.busy": "2025-08-26T00:45:17.945967Z",
     "iopub.status.idle": "2025-08-26T00:45:27.271748Z",
     "shell.execute_reply": "2025-08-26T00:45:27.270895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Translation**\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbaggett/miniconda3/envs/ds776_env/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Helsinki-NLP/opus-mt-en-es, Size: 77,943,296 parameters\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedí el Samsung Galaxy S24 Ultra de Tech Haven, esperando la entrega del día\n",
      "siguiente, pero después de tres días, yo ni siquiera había recibido una\n",
      "actualización de envío. Después de esperar 45 minutos en espera, el servicio al\n",
      "cliente me dijo que había un problema de existencias — sin embargo nadie me\n",
      "había informado! Cuando el paquete finalmente llegó una semana tarde, que\n",
      "contenía un Google Pixel 8 Pro en su lugar. El representante de apoyo era\n",
      "apologético, pero dijo que un intercambio tomaría otras dos semanas. Pagué\n",
      "$1.200 por el teléfono equivocado, trató con retrasos y mala comunicación, y\n",
      "ahora tienen que esperar incluso más tiempo. Para añadir insulto a la lesión, el\n",
      "representante de servicio al cliente con el que hablé parecía indiferente a mi\n",
      "frustración. Tuve que explicar mi situación varias veces antes de que incluso\n",
      "reconocieron el error. Toda la experiencia ha sido increíblemente decepcionante\n",
      "y me ha dejado cuestionando si alguna vez debería comprar con Tech Haven de\n",
      "nuevo. Es desconcertante cómo una empresa puede funcionar con tal falta de\n",
      "transparencia y eficiencia. Espero que esta retroalimentación llega a alguien\n",
      "que puede hacer una diferencia, ya que ningún cliente debe ir a través de lo que\n",
      "hice. Tech Haven!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\\n\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "print_pipeline_info(translation_pipeline)\n",
    "print(\"\")\n",
    "translation_result = translation_pipeline(text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you're better than I am at Spanish, but I can't read that well enough to know if it's a good translation.  However, let's now use a similar model to translate it from Spanish back into English and compare it to the orginal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:27.273604Z",
     "iopub.status.busy": "2025-08-26T00:45:27.273421Z",
     "iopub.status.idle": "2025-08-26T00:45:35.643353Z",
     "shell.execute_reply": "2025-08-26T00:45:35.642462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Helsinki-NLP/opus-mt-es-en, Size: 77,943,296 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back Translation to English: I ordered the Samsung Galaxy S24 Ultra from Tech\n",
      "Haven, waiting for delivery the next day, but after three days, I had not even\n",
      "received a shipping update. After waiting 45 minutes on hold, the customer\n",
      "service told me that there was a stock problem — yet no one had informed me!\n",
      "When the package finally arrived a week late, containing a Google Pixel 8 Pro\n",
      "instead. The support representative was apologetic, but he said an exchange\n",
      "would take another two weeks. I paid $1,200 for the wrong phone, tried with\n",
      "delays and bad communication, and now they have to wait even longer. To add\n",
      "insult to the injury, the customer service representative with whom I spoke\n",
      "seemed indifferent to my frustration. I had to explain my situation several\n",
      "times before they even recognized the error. All the experience has been\n",
      "incredibly disappointing and has left me wondering if I should ever buy with\n",
      "Tech Haven again. It is disconcerting how a company can function with such a\n",
      "lack of transparency and efficiency. I hope this feedback comes to someone who\n",
      "can make a difference, as no customer should go through what I did. Tech Haven!\n",
      "\n",
      "Original Text: I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting\n",
      "next-day delivery, but after three days, I hadn’t even received a shipping\n",
      "update. After waiting 45 minutes on hold, customer service told me there was a\n",
      "stock issue—yet no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead. The support rep was apologetic but said an exchange would take another\n",
      "two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer. To add insult to injury, the customer service\n",
      "representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake.\n",
      "The entire experience has been incredibly disappointing and has left me\n",
      "questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and\n",
      "efficiency. I hope this feedback reaches someone who can make a difference, as\n",
      "no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n"
     ]
    }
   ],
   "source": [
    "# Extract the Spanish translation from the previous result\n",
    "spanish_translation = translation_result[0]['translation_text']\n",
    "\n",
    "# Translate the Spanish text back to English\n",
    "back_translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\", device=device)\n",
    "print_pipeline_info(back_translation_pipeline)\n",
    "back_translation_result = back_translation_pipeline(spanish_translation, max_length=300)\n",
    "print(f\"Back Translation to English: {back_translation_result[0]['translation_text']}\\n\")\n",
    "print(f\"Original Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Helsinki-NLP models are part of the OPUS-MT project, which provides pre-trained neural machine translation models for many language pairs. These models are based on the MarianMT architecture, a transformer-based model optimized for translation tasks. The MarianMT architecture leverages self-attention mechanisms to effectively process and translate text, making these models highly accurate and efficient for translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:35.645625Z",
     "iopub.status.busy": "2025-08-26T00:45:35.645391Z",
     "iopub.status.idle": "2025-08-26T00:45:36.397861Z",
     "shell.execute_reply": "2025-08-26T00:45:36.396978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(translation_pipeline)\n",
    "clear_pipeline(back_translation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation with an LLM and a Zero-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:36.399860Z",
     "iopub.status.busy": "2025-08-26T00:45:36.399625Z",
     "iopub.status.idle": "2025-08-26T00:47:39.438510Z",
     "shell.execute_reply": "2025-08-26T00:47:39.437689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le pedí la versión Samsung Galaxy S24 Ultra en Tech Haven, esperando un envío el\n",
      "día siguiente, pero después de tres días aún no había recibido una actualización\n",
      "sobre entrega. Después de esperar 45 minutos en línea, los servicios al cliente\n",
      "me informaron que existía un problema con el abastecimiento – y nadie me había\n",
      "notificado!\n",
      "Cuando por fin llegó el paquete una semana tarde, contenía un Google Pixel 8 Pro\n",
      "en su lugar. El representante de apoyo estaba arrepentido pero dijo que una\n",
      "intercambio tardaría otros dos semanas.\n",
      "Pagué $1,200 por el teléfono incorrecto, tuve retrasos y comunicación pobre, y\n",
      "ahora tengo que esperar incluso más tiempo. Para agravar las cosas, el\n",
      "representante de atención al cliente que hablé conmigo pareció indiferente a mi\n",
      "frustración. Tuve que explicarme situación varias veces antes de que se\n",
      "reconociera el error. La experiencia ha sido muy decepcionante y ha dejado a mí\n",
      "preguntándome si debería nunca volver a comprar con Tech Haven otra vez.\n",
      "Es extraño cómo una empresa puede operar con tal falta de transparencia y\n",
      "eficacia. Espero que esta información alcance a alguien capaz de hacer algo para\n",
      "mejorar la situación, ya que ningún cliente debe pasar lo que yo hice. Tech\n",
      "Haven, debes hacerlo mejor! Atentamente, Jamie.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_translation = \"\"\"You are an expert translation model. Translate the following text from English to Spanish.\"\"\"\n",
    "user_prompt_translation = f\"Text: {text}\\nTranslation:\"\n",
    "\n",
    "response_translation = llm_generate(llm_config, \n",
    "                                    user_prompt_translation, \n",
    "                                    system_prompt=system_prompt_translation,\n",
    "                                    max_new_tokens=500)\n",
    "print(response_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:47:39.441013Z",
     "iopub.status.busy": "2025-08-26T00:47:39.440769Z",
     "iopub.status.idle": "2025-08-26T00:48:56.864675Z",
     "shell.execute_reply": "2025-08-26T00:48:56.863824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I requested the Samsung Galaxy S24 Ultra from Tech Haven and expected delivery\n",
      "the next day, but after three days, I hadn't even received a shipping update.\n",
      "After waiting for 45 minutes on hold, the customer service representative told\n",
      "me there was an inventory problem – however no one had informed me! When the\n",
      "package finally arrived a week late, it contained a Google Pixel 8 Pro instead.\n",
      "The support representative was apologetic, but said that an exchange would take\n",
      "another two weeks. I paid $1,200 for the wrong phone, dealt with delays and poor\n",
      "communication, and now have to wait even more time. To add injury to insult, the\n",
      "customer service representative I spoke with seemed indifferent to my\n",
      "frustration. I had to explain my situation several times before they even\n",
      "acknowledged the mistake. Overall, this experience has been incredibly\n",
      "disappointing and left me questioning whether I should ever buy from Tech Haven\n",
      "again. It is disconcerting how a company can operate with such lack of\n",
      "transparency and efficiency. I hope this feedback reaches someone who can make a\n",
      "difference, as no customer should go through what I did. Tech Haven!\n"
     ]
    }
   ],
   "source": [
    "system_prompt_back_translation = \"\"\"You are an expert translation model. Translate the following text from Spanish to English.\"\"\"\n",
    "user_prompt_back_translation = f\"Text: {spanish_translation}\\nTranslation:\"\n",
    "\n",
    "response_back_translation = llm_generate(llm_config, \n",
    "                                         user_prompt_back_translation, \n",
    "                                         system_prompt=system_prompt_back_translation,\n",
    "                                         max_new_tokens=500)\n",
    "print(response_back_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't specifically study translation models in one of our lessons, the transformer models used for text summarization are similar in that they take an input sequence of text and produce an output sequence of text.  These models are called sequence to sequence transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Generation\n",
    "\n",
    "Of all the models we'll study, text-generation models are perhaps the most familiar since they are the machines that drive today's chatbots like ChatGPT, Gemini, Claude, and others. Given an input sequence that provides context, a text generation model predicts a likely next word, then does it again and again to generate a hopefully sensible response. These models are particularly useful for tasks such as drafting emails, writing code, creating conversational agents, and generating creative content like stories and poems.\n",
    "\n",
    "HuggingFace makes it simple to create a text generation pipeline.  Here we provide the original customer comment plus the beginning of customer service response and ask the modlel to generate 200 new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:48:56.866564Z",
     "iopub.status.busy": "2025-08-26T00:48:56.866382Z",
     "iopub.status.idle": "2025-08-26T00:49:06.521925Z",
     "shell.execute_reply": "2025-08-26T00:49:06.520973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Text Generation**\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openai-community/gpt2, Size: 124,439,808 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day\n",
      "delivery, but after three days, I hadn’t even received a shipping update. After\n",
      "waiting 45 minutes on hold, customer service told me there was a stock issue—yet\n",
      "no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead. The support rep was apologetic but said an exchange would take another\n",
      "two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer. To add insult to injury, the customer service\n",
      "representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake.\n",
      "The entire experience has been incredibly disappointing and has left me\n",
      "questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and\n",
      "efficiency. I hope this feedback reaches someone who can make a difference, as\n",
      "no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up. Please contact Tech\n",
      "Haven by e-mail or by phone to arrange a shipping replacement. Please note that\n",
      "I will not be able to resolve your problem. I will not be able to meet your\n",
      "current orders and will not be able to meet your future orders. You have placed\n",
      "an order on the website and there are no issues with the service. I have\n",
      "contacted Tech Haven to arrange an exchange but have not received a response.\n",
      "I'm sorry to hear that your order was mixed up. Please contact Tech Haven by\n",
      "e-mail or by phone to arrange a shipping replacement. Please note that I will\n",
      "not be able to resolve your problem. I will not be able to meet your current\n",
      "orders and will not be able to meet your future orders.\n",
      "\n",
      "I have been using my Google Pixel 7 Extreme for several weeks now. I bought it\n",
      "from Tech Haven and was impressed with the look and feel of the phone. I had\n",
      "never used a Pixel before, and I'm very impressed with the look and feel of it.\n",
      "I like the way the screen looks. No issues. I like the feel and feel of the\n",
      "display, as well as the battery life. I also like how the buttons are\n",
      "positioned. I feel like they are easy to use. I like that this is a product that\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n**Text Generation**\\n\")\n",
    "generator_pipeline = pipeline(\"text-generation\", device=device)\n",
    "print_pipeline_info(generator_pipeline)\n",
    "response = \"Dear Jamie, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator_pipeline(prompt, max_length=500)\n",
    "generated_text = outputs[0]['generated_text']\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the response includes the input prompt.  This is typical of text-generation models in HuggingFace, but it's easy to remove the input prompt from the output.  If you read the customer service response you can see that it's not very good.  GPT2, released by OpenAI in 2019, is a large transformer-based language model with 1.5 billion parameters. It was designed to generate coherent and contextually relevant text, but it can sometimes produce outputs that are not entirely accurate or appropriate.  Now there are much better text-generation models available in HuggingFace.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:49:06.524199Z",
     "iopub.status.busy": "2025-08-26T00:49:06.524005Z",
     "iopub.status.idle": "2025-08-26T00:49:07.037119Z",
     "shell.execute_reply": "2025-08-26T00:49:07.036274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(generator_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation with Other LLMs\n",
    "\n",
    "`llm_generate` makes it simple to experiment with different models for text generation.  Here's a generated customer service response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:49:07.038910Z",
     "iopub.status.busy": "2025-08-26T00:49:07.038724Z",
     "iopub.status.idle": "2025-08-26T00:51:19.198922Z",
     "shell.execute_reply": "2025-08-26T00:51:19.198113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Jamie,\n",
      "Thank you for taking the time to share your recent experience with us at Tech\n",
      "Haven. We sincerely regret that our service fell short of your expectations\n",
      "during your purchase of the Samsung Galaxy S24 Ultra.\n",
      "Firstly, we acknowledge the inconvenience caused by the delayed shipment and\n",
      "inadequate communication regarding the stock issue. It is essential for\n",
      "customers like yourself to be kept updated throughout the process, and we\n",
      "understand that this didn't happen in your case. We will review our procedures\n",
      "to ensure improved communication moving forward.\n",
      "Secondly, we deeply apologize for sending you the incorrect product. This error\n",
      "should not have occurred, and we appreciate your patience while dealing with our\n",
      "team member about exchanging the item. While we understand that having to wait\n",
      "additional two weeks for the exchange may cause further frustration, please know\n",
      "that our goal is to resolve this matter promptly.\n",
      "To make up for any inconvenience experienced, we kindly offer a discount code\n",
      "for your next purchase with Tech Haven, ensuring that the price of the Google\n",
      "Pixel 8 Pro will be covered entirely. Furthermore, we will expedite the exchange\n",
      "process so that you receive your correct order within one week. Our Customer\n",
      "Service team will reach out soon to finalize these arrangements.\n",
      "Lastly, we want to express our genuine apologies once more for the\n",
      "disappointment felt during this ordeal. Your feedback serves as valuable insight\n",
      "into areas where we can improve, and rest assured that we are committed to\n",
      "making things right. If there's anything else you feel needs addressing, don't\n",
      "hesitate to contact us directly.\n",
      "We value each and every one of our customers, and it pains us when we fail to\n",
      "meet their standards. Thank you again for sharing your thoughts and experiences;\n",
      "your input helps us grow and become a better version of ourselves.\n",
      "Sincerely,\n",
      "[Your Name]\n",
      "Tech Haven Customer Support Team\n"
     ]
    }
   ],
   "source": [
    "system_prompt_generation = \"\"\"You are an expert customer service representative. Generate a professional and empathetic response to the following customer feedback. Address the issues mentioned and provide a resolution.\"\"\"\n",
    "user_prompt_generation = f\"Customer Feedback: {text}\\n\\nCustomer service response:\"\n",
    "\n",
    "response_generation = llm_generate(llm_config, \n",
    "                                   user_prompt_generation, \n",
    "                                   system_prompt=system_prompt_generation,\n",
    "                                   max_new_tokens=500)\n",
    "print(response_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text generation models in Lesson 11.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Summarization\n",
    "\n",
    "**Natural Language Processing (NLP) summarization** is the process of condensing a longer text into a shorter, more concise version while retaining its key information. There are two main types of summarization: **extractive** and **abstractive**. **Extractive summarization** selects and highlights the most important sentences or phrases directly from the original text without altering their wording. In contrast, **abstractive summarization** generates a new, rephrased summary that conveys the core meaning of the original content in a more natural and human-like manner. While extractive methods rely on ranking techniques, abstractive approaches often leverage deep learning models for text generation.\n",
    "\n",
    "We'll focus on abstractive summarization using HuggingFace pipelines.  Here we use a summarization model to create a summary of our customer complaint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:19.201077Z",
     "iopub.status.busy": "2025-08-26T00:51:19.200896Z",
     "iopub.status.idle": "2025-08-26T00:51:25.096793Z",
     "shell.execute_reply": "2025-08-26T00:51:25.095614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Summarization**\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Tech Haven sent a Samsung Galaxy S24 Ultra to Tech Haven,\n",
      "expecting next-day delivery . The package arrived a week late and contained a\n",
      "Google Pixel 8 Pro instead . The customer service rep was apologetic but said an\n",
      "exchange would take two weeks .'}]\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\", device=-1)\n",
    "print_pipeline_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=100, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:25.099662Z",
     "iopub.status.busy": "2025-08-26T00:51:25.099389Z",
     "iopub.status.idle": "2025-08-26T00:51:25.410301Z",
     "shell.execute_reply": "2025-08-26T00:51:25.409403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(summarization_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART (Bidirectional and Auto-Regressive Transformers) is a denoising autoencoder for pretraining sequence-to-sequence models. It combines the bidirectional context of BERT with the autoregressive nature of GPT, making it highly effective for various NLP tasks, including text generation and summarization (Don't worry, we're going to make sense of many of those terms in future lessons...). `sshleifer/distilbart-cnn-12-6` is a distilled version of the BART model, specifically fine-tuned on the CNN/DailyMail dataset for abstractive summarization tasks. This model is designed to be smaller and faster than the original BART model while retaining most of its performance, making it efficient for generating concise summaries of longer texts.`sshleifer/distilbart-cnn-12-6` is a distilled version of the BART model, specifically fine-tuned on the CNN/DailyMail dataset for abstractive summarization tasks. This model is designed to be smaller and faster than the original BART model while retaining most of its performance, making it efficient for generating concise summaries of longer texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:25.412367Z",
     "iopub.status.busy": "2025-08-26T00:51:25.412177Z",
     "iopub.status.idle": "2025-08-26T00:53:36.020751Z",
     "shell.execute_reply": "2025-08-26T00:53:36.019873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Jamie experienced significant issues while purchasing a Samsung Galaxy\n",
      "S24 Ulta from Tech Haven. Expected next-day delivery didn't occur; neither was\n",
      "there any communication about a stock issue that led to a delay. Upon receiving\n",
      "the package, a different device (Google Pixel 8 Pro) was found instead. Despite\n",
      "apologies from customer service, resolution took additional time due to exchange\n",
      "procedures lasting two more weeks. Overall, the experience involved high\n",
      "financial cost, extended timelines, poor communication, and indifference towards\n",
      "the customer's frustration. The dissatisfaction led Jamie to question future\n",
      "shopping decisions at Tech Haven.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_summarization = \"\"\"You are an expert summarization model. Summarize the following customer feedback in a concise manner.\"\"\"\n",
    "user_prompt_summarization = f\"Customer Feedback: {text}\\n\\nSummary:\"\n",
    "\n",
    "response_summarization = llm_generate(llm_config, \n",
    "                                      user_prompt_summarization, \n",
    "                                      system_prompt=system_prompt_summarization,\n",
    "                                      max_new_tokens=150)\n",
    "print(response_summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notes on Using LLMs Programatically\n",
    "\n",
    "While LLMs can make great all-purpose NLP tools, their use has some drawbacks as well:\n",
    "\n",
    "1.  They're usually configured to give **human sounding responses** which may not be what you want depending on the task.  You'll often have to experiment with the system prompt to get closer to what you want.\n",
    "\n",
    "2.  **LLMs don't always generate the same output.**  We'll learn more about text-generation in Lesson 11, but by default LLMs include some randomness in the generated text.  You can usually configure the LLM to use a deterministic next-word search to get repeatable results.  In `llm_generate` you can do this by passing `search_strategy = 'deterministic'`.\n",
    "\n",
    "3.  It can be **difficult to get an LLM to format the output** in the way that you want.  Carefully crafting the system prompt can help, but often some post-processing of the generated text is also necessary.  Here's a [4 minute video on DataCamp](https://campus.datacamp.com/courses/chatgpt-prompt-engineering-for-developers/introduction-to-prompt-engineering-best-practices?ex=9) discussing prompting to get structured outputs. (I was able to view the video without paying, but had to sign up for an account to do much else.)  The entire class \"ChatGPT Prompt Engineering for Developers\" looks worthwhile.  Recent LLMs such as GPT-4o and Gemini can produce output following JSON schema through their APIs.\n",
    "\n",
    "4.  **LLMs are usually slower than a specialized model.**  Especially if you're running the LLM locally.  While LLMs continue to improve, often fine-tuning a specialized model is still preferable if you have enough data and resources to do so, but if you don't have much training data or just need something quick using an LLM programatically can be beneficial.\n",
    "\n",
    "5.  **Few-shot prompting can improve the results** from an LLM.  Providing a one or more examples in the prompt can improve the LLM response.  You'll explore this a bit in the homework.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestion:** If you have a Gemini or OpenAI api key, go to the first code cell above and change the LLM_MODEL to 'gpt-4o-mini' or 'gemini-flash-lite' and rerun that cell and the subsequent LLM cells to see how the responses differ.  (I think the API based models give better results, but if had the hardware we could probably get similar results running locally as well.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a6c1986350b4284896cf5a579bb3a8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "10cc2ca6a64843409aa239e2f8f41afb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "770d1da8bddc4c7491f35d73be7a2710": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "81c7470b96b845b58123c99d7784dec4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c643400f0b0740759329f64ba59ec6cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d03d872c8a2f470dbc9b111716bfa27f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d17218daac884b819a72c55b2a5679a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d03d872c8a2f470dbc9b111716bfa27f",
       "placeholder": "​",
       "style": "IPY_MODEL_0a6c1986350b4284896cf5a579bb3a8a",
       "tabbable": null,
       "tooltip": null,
       "value": " 10/10 [00:15&lt;00:00,  1.63s/it]"
      }
     },
     "d41731cd0ee44bdbb881cfe963f96552": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81c7470b96b845b58123c99d7784dec4",
       "placeholder": "​",
       "style": "IPY_MODEL_e443047dffdb42bc9ad35d52008be058",
       "tabbable": null,
       "tooltip": null,
       "value": "Local Generation: 100%"
      }
     },
     "e443047dffdb42bc9ad35d52008be058": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eb5f6b4c2d6c4f2b915d937cffbd829d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_10cc2ca6a64843409aa239e2f8f41afb",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c643400f0b0740759329f64ba59ec6cd",
       "tabbable": null,
       "tooltip": null,
       "value": 10
      }
     },
     "f65ef90340c64f5398337dc7bec9aa19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d41731cd0ee44bdbb881cfe963f96552",
        "IPY_MODEL_eb5f6b4c2d6c4f2b915d937cffbd829d",
        "IPY_MODEL_d17218daac884b819a72c55b2a5679a1"
       ],
       "layout": "IPY_MODEL_770d1da8bddc4c7491f35d73be7a2710",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}