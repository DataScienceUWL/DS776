{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f0af3e",
   "metadata": {},
   "source": [
    "# Topics\n",
    "\n",
    "- Lesson 1: Introduction to Deep Learning and Fully Connected Networks\n",
    "- Lesson 2: Convolutional Neural Networks\n",
    "- Lesson 3: Better Network Training\n",
    "- Lesson 4: Improved Network Architectures\n",
    "- Lesson 5: Object Detection and Segmentation\n",
    "- Lesson 6: Transfer Learning\n",
    "- Lesson 7: Introduction to Transformers, Hugging Face, and Using LLMs Effectively\n",
    "- Week 8: Transformer Internals - Self-Attention and Positional Encoding\n",
    "- Week 9: Text Classification with Transformers\n",
    "- Lesson 10: Named Entity Recognition (NER) and Tokenization\n",
    "- Lesson 11: Text Generation and Decoding Strategies\n",
    "- Lesson 12: Summarization with Transformers\n",
    "- Lesson 13 (not finalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81179bb",
   "metadata": {},
   "source": [
    "## Lesson 1: Introduction to Deep Learning and Fully Connected Networks\n",
    "\n",
    "### Topics\n",
    "* Introduction to PyTorch\n",
    "* Fully-connected neural networks for regression and classification\n",
    "* Modeling process: prepare data, prepare model, train model, evaluate model, make predictions\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand deep learning fundamentals**, including the structure of neural networks.\n",
    "2. **Utilize PyTorch datasets and data loaders** to efficiently handle and preprocess data for training deep learning models.\n",
    "3. **Develop and train basic fully connected neural networks** in PyTorch, applying optimization techniques and appropriate loss functions.\n",
    "4. **Evaluate model performance** by plotting loss functions and other metrics.\n",
    "5. **Make predictions for new data** by applying a trained neural network to new inputs.\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* **Course Intro Notebook / Video** (Still to come)\n",
    "\n",
    "* **(Optional) Review Neural Networks from DS740**\n",
    "\n",
    "    * You might want to review the first 14 slides of the [Lesson on Neural networks in DS740](https://media.uwex.edu/content/ds/ds740_r23/ds740_artificial-neural-networks.sbproj/).  We're covering similar material this week.  Don't review the material about neural networks in R since we'll be using Python.\n",
    "\n",
    "* **Readings from Inside Deep Learning (IDL)**\n",
    "\n",
    "    * **Chapter 1: The Mechanics of Learning**\n",
    "        - **Read Sections 1.2, 1.4, and 1.5**. Skim the other sections. No need to understand the detailed code or the backpropagation algorithm, but ensure you understand how the gradient is used in training.\n",
    "\n",
    "    * **Chapter 2: Fully Connected Networks**\n",
    "        - **Section 2.1**: Focus on understanding the training loop structure and process. Skip the code details but grasp the concept.\n",
    "            - Don’t worry about the math notation at the bottom of page 40. It's shorthand for a fully connected linear layer. If you want to learn more about matrix multiplication see the videos listed under Auxiliary Materials below.\n",
    "        - **Section 2.2**: Understand how activation functions introduce nonlinearity into networks.\n",
    "        - **Section 2.3**: Grasp the basics of softmax and cross-entropy, especially how the loss function changes for classification tasks. An example will be explained in a notebook and video.\n",
    "        - **Section 2.4**: Note the key concepts; they will be reinforced in video lectures.\n",
    "        - **Section 2.5**: Understand the importance of batch training, particularly for large datasets that won’t fit in memory.\n",
    "\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included in the lesson folder and watch the embedded video.  You can read along and work through the code examples as needed.  The notebooks for this lesson are in the Lesson_01 directory.  The notebooks are numbered in the order they should be used.\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "### Auxiliary Materials\n",
    "\n",
    "- **Background Mathematics from Dr. Anne Hsu:**\n",
    "    * [Matrices and Vectors](https://www.youtube.com/watch?v=sM2Mm6aT_HI)\n",
    "    * [Derivatives and Gradients 1](https://www.youtube.com/watch?v=Fiw0_w4AykA)\n",
    "    * [Derivatives and Gradients 2](https://www.youtube.com/watch?v=qORZmKCB0g8)\n",
    "    * [Playlist for entire Intro to Deep Learning](https://www.youtube.com/@drannehsu/playlists)\n",
    "- **Object-Oriented Programming** It helps to have a basic familiarity with classes, inheritance, and methods for finding your way around in PyTorch.  [Real Python has a great tutorial](https://realpython.com/python3-object-oriented-programming/) on the basics of OOP with many code examples that you can either read or watch (40 minutes).\n",
    "- **Deep Learning Basics: Introduction and Overview**  Lex Fridman is a well known podcast host for AI and Data Science.  Here he gives an [introductory lecture](https://youtu.be/O5xeyoRL95U?si=SrM7RLWB_iBPMiK4) for MIT's public deep learning class.  Since it was recorded in 2019 it doesn't include the latest on transformer architectures that are driving the current boom in AI (ChatGPT, etc.), it's still a great introduction that discusses many applications of deep learning.  Watch this if you want a good overview.  I also recommend his podcast.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fa14b",
   "metadata": {},
   "source": [
    "## Lesson 2: Convolutional Neural Networks\n",
    "\n",
    "### Topics\n",
    "* Image data\n",
    "* Convolutional Layers\n",
    "* Pooling Layers\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand the structure and role of Convolutional Neural Networks (CNNs)** in processing spatial data like images.\n",
    "2. **Design and implement basic CNN architectures** in PyTorch, including convolutional layers, pooling, and activation functions.\n",
    "3. **Understand how padding, stride, kernel size, and the number of output channels** interact to determine the dimensionality of the output in each convolutional layer.\n",
    "### Readings and Videos\n",
    "\n",
    "* Read Chapter 3, through Section 3.5, in Inside Deep Learning.  You can read 3.6 if you wish, we'll get into that material in the next lesson.\n",
    "* [Andrew Ng on Convolution over Volumes](https://www.youtube.com/watch?v=KTB_OFoAQcc&ab_channel=DeepLearningAI) This is one of many videos Andrew Ng has made to support his deep learning course.  Watch this to solidify your understanding convolutions after doing the reading.  About 11 minutes.\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_02 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f10fd1",
   "metadata": {},
   "source": [
    "## Lesson 3: Better Network Training\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Apply data augmentation techniques** to improve model generalization, especially with small datasets.\n",
    "2. **Understand and implement learning rate schedules**, including exponential decay, step drop, and cosine annealing.\n",
    "3. **Optimize training** using modern techniques like SGD with momentum, Adam, and gradient clipping.\n",
    "4. **Implement early stopping** to prevent overfitting and improve training efficiency by halting training when performance plateaus.\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* Read Sections 3.6, 5.1-5.3 in Inside Deep Learning.\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_03 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2553e57",
   "metadata": {},
   "source": [
    "## Lesson 4: Improved Network Architectures\n",
    "\n",
    "### Outcomes\n",
    "* **Understand and apply ReLU and LeakyReLU activations** to address vanishing gradient problems and enhance network convergence.\n",
    "* **Implement batch and layer normalization** to stabilize training and improve network performance.\n",
    "* **Analyze and utilize residual connections** to enable deeper network architectures by mitigating vanishing gradient issues.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read Sections 6.1-6.4 from Inside Deep Learning\n",
    "* **Course Notebooks with Videos** Open each of the notebooks included the lesson folder and watch the embedded video. You can read along and work through the code examples as you want. The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "* Complete the reading quiz in Canvas (10 points).\n",
    "* Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d040592",
   "metadata": {},
   "source": [
    "## Lesson 5: Object Detection and Segmentation\n",
    "\n",
    "### Topics\n",
    "* Transposed Convolutions\n",
    "* U-Net for Segmentation\n",
    "* R-CNN for Object Detection\n",
    "* Other architectures\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Differentiate Image Segmentation and Object Detection**: Explain the roles of pixel-level segmentation and object detection with bounding boxes.\n",
    "   \n",
    "2. **Build an Image Segmentation Model**: Implement segmentation using transposed convolutions and U-Net architecture.\n",
    "\n",
    "3. **Apply Bounding Box Detection**: Use Faster R-CNN for object detection with bounding boxes and assess precision trade-offs.\n",
    "\n",
    "4. **Reduce False Positives**: Explore filtering methods to improve detection accuracy by minimizing overlapping boxes.\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* Read Chapter 8 in Inside Deep Learning\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_02 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed468c26",
   "metadata": {},
   "source": [
    "## Lesson 6: Transfer Learning\n",
    "\n",
    "### Topics\n",
    "* Sources for pre-trained models:  Torch Hub, TIMM, Hugging Face, and more\n",
    "* Identifying the layers that need tweaking\n",
    "* Freezing and unfreezing Layers\n",
    "* The small data problem\n",
    "* Fine-tuning vs full Training\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Explain Transfer Learning**: Describe the concept and benefits of transfer learning for leveraging pre-trained models on new tasks.\n",
    "\n",
    "2. **Implement Model Parameter Transfer**: Apply a pre-trained model’s weights to a new problem by modifying only specific layers.\n",
    "\n",
    "3. **Optimize Training with Limited Data**: Use transfer learning techniques to improve model performance with smaller labeled datasets.\n",
    "\n",
    "4. **Adapt CNNs for New Tasks**: Fine-tune models for target datasets by adjusting layers and applying warm or frozen weights.\n",
    "\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* Read 13.1-13.3 in Inside Deep Learning\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_02 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ec5d3",
   "metadata": {},
   "source": [
    "## Lesson 7: Introduction to Transformers, Hugging Face, and Using LLMs Effectively\n",
    "\n",
    "### Topics\n",
    "* Transformer architecture overview\n",
    "* Self-attention mechanism\n",
    "* Hugging Face library introduction\n",
    "* Effective use of large language models (LLMs) and prompt engineering basics\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Describe Transformer Architecture**: Identify the core components of transformers, including the encoder-decoder structure and the role of self-attention.\n",
    "   \n",
    "2. **Explain Self-Attention Basics**: Outline how self-attention works, including the function of queries, keys, and values, and the advantages it provides for handling context.\n",
    "\n",
    "3. **Use Pre-trained Models**: Load and use a pre-trained Hugging Face model for a simple NLP task to gain familiarity with the Hugging Face transformers library.\n",
    "\n",
    "4. **Improve Prompting Techniques**: Experiment with prompt engineering to enhance response quality when interacting with LLMs, learning to refine prompts for clarity and relevance.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 1: Hello Transformers* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each of the notebooks included in the Lesson_07 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec09d7",
   "metadata": {},
   "source": [
    "## Week 8: Transformer Internals - Self-Attention and Positional Encoding\n",
    "\n",
    "### Topics\n",
    "* In-depth self-attention mechanics\n",
    "* Multi-headed attention and context capture\n",
    "* Positional encoding and sequence structure\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand Self-Attention Calculations**: Demonstrate how self-attention works by calculating attention scores for a simple sequence, showing the process of computing queries, keys, and values.\n",
    "   \n",
    "2. **Explain Multi-Headed Attention**: Describe the purpose of multi-headed attention in transformers and explain how multiple attention heads provide richer contextual understanding.\n",
    "\n",
    "3. **Discuss Positional Encoding**: Explain the role of positional encoding in maintaining sequence order within transformers and understand its mathematical formulation.\n",
    "\n",
    "4. **Experiment with Attention Mechanisms**: Use Hugging Face models to observe how multi-headed attention and positional encoding affect outputs.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 3: Transformer Anatomy* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each of the notebooks in the Lesson_08 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b75666",
   "metadata": {},
   "source": [
    "## Week 9: Text Classification with Transformers\n",
    "\n",
    "### Topics\n",
    "* Fine-tuning transformers for text classification\n",
    "* Tokenization and data preprocessing for NLP tasks\n",
    "* Using the Hugging Face Trainer API for efficient model training\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Explain the Fine-Tuning Process**: Describe how transformers are fine-tuned for text classification tasks, focusing on modifying specific layers and adjusting model parameters.\n",
    "   \n",
    "2. **Use Tokenization for Classification Tasks**: Use Hugging Face’s `AutoTokenizer` to tokenize and preprocess input text for classification, understanding the effect of different tokenization strategies on model input.\n",
    "\n",
    "3. **Fine-Tune a Transformer Model**: Fine-tune a BERT model (or similar) on a classification dataset, adjusting hyperparameters to improve model performance.\n",
    "\n",
    "4. **Evaluate Model Performance**: Analyze the model’s accuracy on the validation set, learning to interpret common metrics (accuracy, F1) and assess model quality.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 2: Text Classification* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_09 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b204bd",
   "metadata": {},
   "source": [
    "## Lesson 10: Named Entity Recognition (NER) and Tokenization\n",
    "\n",
    "### Topics\n",
    "* Overview of Named Entity Recognition (NER)\n",
    "* Subword tokenization techniques (e.g., BPE, WordPiece)\n",
    "* Multilingual considerations in tokenization and NER\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Explain Named Entity Recognition (NER)**: Define NER and its applications, identifying how it is used to label specific entities (e.g., names, locations) in text.\n",
    "   \n",
    "2. **Differentiate Tokenization Methods**: Describe different tokenization techniques (e.g., Byte-Pair Encoding, WordPiece) and their relevance in NER and multilingual settings.\n",
    "\n",
    "3. **Apply NER with Transformers**: Fine-tune a transformer model for NER tasks, learning how token-level classification works in transformers.\n",
    "\n",
    "4. **Discuss Multilingual Challenges**: Explain tokenization and NER challenges in multilingual contexts, including handling multiple languages and out-of-vocabulary (OOV) words.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 4: Multilingual Named Entity Recognition* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_10 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b118d99",
   "metadata": {},
   "source": [
    "## Lesson 11: Text Generation and Decoding Strategies\n",
    "\n",
    "### Topics\n",
    "* Overview of transformer-based text generation\n",
    "* Decoding strategies: Greedy search, beam search, top-k sampling, and nucleus sampling\n",
    "* Applications of text generation in NLP\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Explain Text Generation Basics**: Describe how transformer models like GPT-2 generate text, and identify common applications of text generation, such as chatbots, content creation, and automated summarization.\n",
    "   \n",
    "2. **Use Decoding Strategies**: Implement and compare different decoding methods (e.g., greedy search, beam search, top-k sampling, nucleus sampling) to observe how each affects text generation quality.\n",
    "\n",
    "3. **Evaluate Generated Text**: Assess the quality of generated text, discussing trade-offs in coherence, creativity, and relevance with different decoding strategies.\n",
    "\n",
    "4. **Identify Real-World Applications**: Explain practical uses of text generation in industries like customer service, media, and content creation, understanding the strengths and limitations of transformer-based text generation.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 5: Text Generation* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_11 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb13a97",
   "metadata": {},
   "source": [
    "## Lesson 12: Summarization with Transformers\n",
    "\n",
    "### Topics\n",
    "* Extractive vs. abstractive summarization\n",
    "* Transformer models for summarization (e.g., BART, T5)\n",
    "* Evaluation metrics for summarization (e.g., ROUGE scores)\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand Summarization Approaches**: Differentiate between extractive and abstractive summarization, identifying the benefits and limitations of each approach.\n",
    "   \n",
    "2. **Fine-Tune a Summarization Model**: Use a transformer model, such as BART or T5, to perform summarization on a dataset, focusing on fine-tuning techniques for high-quality summary generation.\n",
    "\n",
    "3. **Evaluate Summaries**: Apply evaluation metrics like ROUGE to assess the relevance, coherence, and completeness of generated summaries.\n",
    "\n",
    "4. **Discuss Summarization Applications**: Identify practical applications of summarization in fields like news, research, customer service, and document management.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 6: Summarization* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_12 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d3509",
   "metadata": {},
   "source": [
    "## Lesson 13 (not finalized)\n",
    "\n",
    "In the final two weeks of the course you will either:\n",
    "\n",
    "* Investigate a new topic from one of our textbooks on your own and produce a notebook that introduces the topic, explains a bit about how it works, and demonstrates one or more computational experiments related topic.  Your goal is to produce an \"educational exposition\" that highlights the topic so that it could be read by one of your peers for them to supplement their own study of the corresponding textbook chapter.  It would be similar to any of the notebooks I've provided for the class.\n",
    "\n",
    "* Apply one of the topics we've covered to a new dataset or in a new way.  For example, ...\n",
    "\n",
    "### Assessments\n",
    "* Submit your notebook in the Final Project folder in CoCalc.  (100 pts)\n",
    "\n",
    "#### Rubric:\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
