{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Using Named Entities for Analysis (6 points)\n",
    "\n",
    "NER is often used to look for trends or to do other analysis on text data.  Once you have teh NER tags you can use them to extract the entities from the text to do analysis.  \n",
    "\n",
    "Here we'll use dataset of made-up movie reviews.  The idea is to use the entity tags to extract the actors and directors from the reviews, then to figure out which actors and directors are most likely to be involved with positive sentiment movies and negative sentiment movies.  We'll load the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-ACTOR', 'I-ACTOR', 'B-CHARACTER', 'I-CHARACTER', 'B-DIRECTOR', 'I-DIRECTOR', 'B-GENRE', 'I-GENRE', 'B-TITLE', 'I-TITLE', 'B-YEAR', 'I-YEAR']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"hobbes99/fake_movie_reviews_ner_sentiment\",\n",
    "    cache_dir=\"C:/Users/bagge/huggingface_cache\"\n",
    ")\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an entry in the training set to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Even',\n",
       "  'the',\n",
       "  'usually',\n",
       "  'reliable',\n",
       "  'Logan',\n",
       "  'Dark',\n",
       "  \"can't\",\n",
       "  'save',\n",
       "  'Eternal',\n",
       "  'Oath',\n",
       "  ',',\n",
       "  'a',\n",
       "  'thriller',\n",
       "  'film',\n",
       "  'from',\n",
       "  '1988',\n",
       "  \"that's\",\n",
       "  'as',\n",
       "  'clumsy',\n",
       "  'as',\n",
       "  'they',\n",
       "  'come',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  10,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'review': \"Even the usually reliable Logan Dark can't save Eternal Oath, a thriller film from 1988 that's as clumsy as they come.\",\n",
       " 'sentiment': 'negative',\n",
       " 'entities': {'Actor': 'Logan Dark',\n",
       "  'Character': None,\n",
       "  'Director': None,\n",
       "  'Genre': 'thriller',\n",
       "  'Title': 'Eternal Oath',\n",
       "  'Year': '1988'},\n",
       " 'movie_rating': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NER tags are stored as integers corresponding to their indices in `label_list`.  You'll need to use those tags to extract the actor and director names.  You can also extract the sentiment.  \n",
    "\n",
    "For the training split, find and display in order:\n",
    "* The three actors most likely to appear in positive films.\n",
    "* The three actors most likely to appear in negative films.\n",
    "* The three directors most likely to have directed positive films.\n",
    "* The three directors most likely to have directed negative films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2 - Fine Tuning Two BERT NER Models (14 points)\n",
    "\n",
    "The MIT Movie Corpus is designed for movie-related NER tasks and includes the following entity types in BIO format:\n",
    "- **Actor**: Names of actors or actresses (e.g., \"Leonardo DiCaprio\").\n",
    "- **Character**: Names of characters in movies (e.g., \"Jack Dawson\").\n",
    "- **Director**: Names of movie directors (e.g., \"Christopher Nolan\").\n",
    "- **Genre**: Movie genres (e.g., \"Action\", \"Drama\").\n",
    "- **Title**: Titles of movies (e.g., \"Inception\").\n",
    "- **Year**: Year the movie was made.\n",
    "\n",
    "The original movie corpus includes more entity types, but we've produced a simplified version for this assignment.\n",
    "\n",
    "In this part of the assignment you should fine-tune \"distilbert-base-uncased\" and \"bert-base-uncased\" for NER on the dataset \"hobbes99/mit-movie-ner-simplified\".  The dataset has \"train\" and \"valid\" splits.  Use the \"train\" split for fine-tuning and evaluate the metrics using seqeval as shown in the lesson.\n",
    "* Figure out a way to plot precision, recall, and F1 by entity type.\n",
    "* Find two movie reviews on the internet and run inference on them to extract the named entities.\n",
    "* Write a brief summary of the results.  Include answers to:\n",
    "    * Which entity types does the model struggle with?  \n",
    "    * Which does it do well on?\n",
    "* The \"distilbert-based-uncased\" model is a distilled version \"bert-based-uncased\" model (distillation means a smaller model that was trained using the larger trained model as a \"teacher\").  The \"bert-based-uncased\" model should lead to better results here.  Does it?  Discuss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Using an LLM for NERs. (14 points)\n",
    "\n",
    "For the first 100 texts in the \"valid\" split,  mimic what we did in the lesson to extract the \"Actor\", \"Character\", \"Director\", \"Genre\", \"Title\" and \"Year\" entities using an LLM.  Start with just a few examples to refine your prompt and instructions, then ramp up to 100 or more examples.  Get the final evaluation metrics as shown in the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Comparison and Reflection (6 points)\n",
    "\n",
    "* Compare the results of two entity recognition techniques both quantitatively and qualitatively.  Consider the difficulty of obtained labeled data in your comparison. It's time-consuming and/or costly to get tagged text, but that's not necessary for the LLM approach which may be less accurate...\n",
    "\n",
    "* Give a brief summary of what you learned in this assignment.\n",
    "\n",
    "* What did you find most difficult to understand?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
