{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Homework 10: Named Entity Recognition\n",
    "\n",
    "**Total Points: 50**\n",
    "- Reading Questions: 8 points\n",
    "- Part 1 (Named Entities for Analysis): 6 points\n",
    "- Part 2 (Fine-tune BERT Models): 12 points\n",
    "- Part 3 (LLM for NER): 12 points\n",
    "- Part 4 (Comparison): 8 points\n",
    "- Part 5 (Reflection): 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Reading Questions (8 points)\n",
    "\n",
    "Answer the following questions based on Chapter 4: Multilingual Named Entity Recognition from *Natural Language Processing with Transformers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "**Question 1 (2 points):** Explain the BIO tagging scheme used in Named Entity Recognition. What do the B-, I-, and O tags represent, and why is this tagging scheme necessary for NER tasks instead of simply labeling entity types?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "**Question 2 (2 points):** Compare the traditional BiLSTM-CRF architecture to transformer-based models (like BERT) for NER tasks. What are the main advantages of using transformer models for NER, and what challenges remain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "**Question 3 (2 points):** Explain the difference between token-level and entity-level evaluation for NER. Why is entity-level F1 score (using metrics like seqeval) generally preferred over token-level accuracy for evaluating NER models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "**Question 4 (2 points):** What are nested entities in NER, and why do they pose a challenge for traditional sequence labeling approaches? Provide an example of nested entities and explain how they complicate the BIO tagging scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 1 - Using Named Entities for Analysis (6 points)\n",
    "\n",
    "NER is often used to look for trends or to do other analysis on text data. Once you have the NER tags you can use them to extract the entities from the text to do analysis.\n",
    "\n",
    "Here we'll use a dataset of made-up movie reviews. The idea is to use the entity tags to extract the actors and directors from the reviews, then to figure out which actors and directors are most likely to be involved with positive sentiment movies and negative sentiment movies. We'll load the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hobbes99/fake_movie_reviews_ner_sentiment\"\n",
    ")\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "Here's an entry in the training set to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "Notice that NER tags are stored as integers corresponding to their indices in `label_list`. You'll need to use those tags to extract the actor and director names. You can also extract the sentiment.\n",
    "\n",
    "For the training split, find and display in order:\n",
    "* The three actors most likely to appear in positive films.\n",
    "* The three actors most likely to appear in negative films.\n",
    "* The three directors most likely to have directed positive films.\n",
    "* The three directors most likely to have directed negative films."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 2 - Fine Tuning Two BERT NER Models (12 points)\n",
    "\n",
    "The MIT Movie Corpus is designed for movie-related NER tasks and includes the following entity types in BIO format:\n",
    "- **Actor**: Names of actors or actresses (e.g., \"Leonardo DiCaprio\").\n",
    "- **Character**: Names of characters in movies (e.g., \"Jack Dawson\").\n",
    "- **Director**: Names of movie directors (e.g., \"Christopher Nolan\").\n",
    "- **Genre**: Movie genres (e.g., \"Action\", \"Drama\").\n",
    "- **Title**: Titles of movies (e.g., \"Inception\").\n",
    "- **Year**: Year the movie was made.\n",
    "\n",
    "The original movie corpus includes more entity types, but we've produced a simplified version for this assignment.\n",
    "\n",
    "In this part of the assignment you should fine-tune \"distilbert-base-uncased\" and \"bert-base-uncased\" for NER on the dataset \"hobbes99/mit-movie-ner-simplified\". The dataset has \"train\" and \"valid\" splits. Use the \"train\" split for fine-tuning and evaluate the metrics using seqeval as shown in the lesson.\n",
    "* Figure out a way to plot precision, recall, and F1 by entity type.\n",
    "* Find two movie reviews on the internet and run inference on them to extract the named entities.\n",
    "* Write a brief summary of the results. Include answers to:\n",
    "    * Which entity types does the model struggle with?\n",
    "    * Which does it do well on?\n",
    "* The \"distilbert-base-uncased\" model is a distilled version of the \"bert-base-uncased\" model (distillation means a smaller model that was trained using the larger trained model as a \"teacher\"). The \"bert-base-uncased\" model should lead to better results here. Does it? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Part 3 - Using an LLM for NER (12 points)\n",
    "\n",
    "For the first 100 texts in the \"valid\" split, mimic what we did in the lesson to extract the \"Actor\", \"Character\", \"Director\", \"Genre\", \"Title\" and \"Year\" entities using an LLM. Start with just a few examples to refine your prompt and instructions, then ramp up to 100 or more examples. Get the final evaluation metrics as shown in the lesson.\n",
    "\n",
    "**Hint:** You can import the `llm_ner_extractor` function from `Lesson_10_Helpers` to streamline your LLM-based extraction, similar to how we used `llm_classifier` in Lesson 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Part 4 - Comparison (8 points)\n",
    "\n",
    "* Compare the results of the two entity recognition techniques (fine-tuned BERT models vs LLM zero-shot) both quantitatively and qualitatively.\n",
    "* Consider the difficulty of obtaining labeled data in your comparison. It's time-consuming and/or costly to get tagged text, but that's not necessary for the LLM approach which may be less accurate.\n",
    "* Which approach would you choose for a production system and why? Consider accuracy, speed, cost, and maintenance requirements.\n",
    "* Give a brief summary of what you learned in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "üìù **YOUR COMPARISON AND SUMMARY HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Part 5 - Reflection (2 points)\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for the lesson? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
