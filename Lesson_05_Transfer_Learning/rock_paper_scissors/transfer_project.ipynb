{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "from torchvision.models import MobileNet_V3_Small_Weights\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from introdl.utils import get_device, load_results, load_model\n",
    "from introdl.idlmam import train_network\n",
    "from introdl.visul import plot_training_metrics\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]  # Set the default figure size (width, height) in inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paper', 'rock', 'scissors']\n"
     ]
    }
   ],
   "source": [
    "# data is in Rock-Paper-Scissors folders test, train, validation with subfolders for each class set up as ImageFolder\n",
    "data_dir = 'Rock-Paper-Scissors'\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "image_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "# set up the transforms resize to 224x224, convert to tensor, normalize for ImageNet.  separate for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])  \n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# create the ImageFolder datasets for training and validation and testing\n",
    "train_all_dataset = ImageFolder(root=f'{data_dir}/train', transform=train_transform)\n",
    "\n",
    "train_size = int(0.8 * len(train_all_dataset))\n",
    "val_size = len(train_all_dataset) - train_size\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(train_all_dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = ImageFolder(root=f'{data_dir}/test', transform=val_transform)\n",
    "\n",
    "# create the DataLoader for training and validation and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# display the class names\n",
    "class_names = train_all_dataset.classes\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained model with new API\n",
    "model = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "# Change the number of output classes to 3\n",
    "model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, 3)\n",
    "\n",
    "# Freeze all layers except the classifier\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "MobileNetV3                                        [32, 3]                   --\n",
       "├─Sequential: 1-1                                  [32, 576, 7, 7]           --\n",
       "│    └─Conv2dNormActivation: 2-1                   [32, 16, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-1                            [32, 16, 112, 112]        (432)\n",
       "│    │    └─BatchNorm2d: 3-2                       [32, 16, 112, 112]        (32)\n",
       "│    │    └─Hardswish: 3-3                         [32, 16, 112, 112]        --\n",
       "│    └─InvertedResidual: 2-2                       [32, 16, 56, 56]          --\n",
       "│    │    └─Sequential: 3-4                        [32, 16, 56, 56]          (744)\n",
       "│    └─InvertedResidual: 2-3                       [32, 24, 28, 28]          --\n",
       "│    │    └─Sequential: 3-5                        [32, 24, 28, 28]          (3,864)\n",
       "│    └─InvertedResidual: 2-4                       [32, 24, 28, 28]          --\n",
       "│    │    └─Sequential: 3-6                        [32, 24, 28, 28]          (5,416)\n",
       "│    └─InvertedResidual: 2-5                       [32, 40, 14, 14]          --\n",
       "│    │    └─Sequential: 3-7                        [32, 40, 14, 14]          (13,736)\n",
       "│    └─InvertedResidual: 2-6                       [32, 40, 14, 14]          --\n",
       "│    │    └─Sequential: 3-8                        [32, 40, 14, 14]          (57,264)\n",
       "│    └─InvertedResidual: 2-7                       [32, 40, 14, 14]          --\n",
       "│    │    └─Sequential: 3-9                        [32, 40, 14, 14]          (57,264)\n",
       "│    └─InvertedResidual: 2-8                       [32, 48, 14, 14]          --\n",
       "│    │    └─Sequential: 3-10                       [32, 48, 14, 14]          (21,968)\n",
       "│    └─InvertedResidual: 2-9                       [32, 48, 14, 14]          --\n",
       "│    │    └─Sequential: 3-11                       [32, 48, 14, 14]          (29,800)\n",
       "│    └─InvertedResidual: 2-10                      [32, 96, 7, 7]            --\n",
       "│    │    └─Sequential: 3-12                       [32, 96, 7, 7]            (91,848)\n",
       "│    └─InvertedResidual: 2-11                      [32, 96, 7, 7]            --\n",
       "│    │    └─Sequential: 3-13                       [32, 96, 7, 7]            (294,096)\n",
       "│    └─InvertedResidual: 2-12                      [32, 96, 7, 7]            --\n",
       "│    │    └─Sequential: 3-14                       [32, 96, 7, 7]            (294,096)\n",
       "│    └─Conv2dNormActivation: 2-13                  [32, 576, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-15                           [32, 576, 7, 7]           (55,296)\n",
       "│    │    └─BatchNorm2d: 3-16                      [32, 576, 7, 7]           (1,152)\n",
       "│    │    └─Hardswish: 3-17                        [32, 576, 7, 7]           --\n",
       "├─AdaptiveAvgPool2d: 1-2                           [32, 576, 1, 1]           --\n",
       "├─Sequential: 1-3                                  [32, 3]                   --\n",
       "│    └─Linear: 2-14                                [32, 1024]                590,848\n",
       "│    └─Hardswish: 2-15                             [32, 1024]                --\n",
       "│    └─Dropout: 2-16                               [32, 1024]                --\n",
       "│    └─Linear: 2-17                                [32, 3]                   3,075\n",
       "====================================================================================================\n",
       "Total params: 1,520,931\n",
       "Trainable params: 593,923\n",
       "Non-trainable params: 927,008\n",
       "Total mult-adds (Units.GIGABYTES): 1.78\n",
       "====================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 724.34\n",
       "Params size (MB): 6.08\n",
       "Estimated Total Size (MB): 749.69\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 3, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.AdamW(model.parameters())  # Adam optimizer\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "ckpt_file = 'models/model_mnv3.pt'\n",
    "epochs = 5\n",
    "\n",
    "score_funcs = {'ACC':accuracy_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [00:48<00:00,  9.73s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "resume_from_checkpoint = False\n",
    "\n",
    "if not resume_from_checkpoint:\n",
    "    # Load the pre-trained model with new API\n",
    "    model = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "    # Change the number of output classes to 3\n",
    "    model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, 3)\n",
    "\n",
    "    # Freeze all layers except the classifier\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "'''\n",
    "\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=val_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file,\n",
    "                        resume_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>total time</th>\n",
       "      <th>train loss</th>\n",
       "      <th>val loss</th>\n",
       "      <th>train ACC</th>\n",
       "      <th>val ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.833302</td>\n",
       "      <td>0.164662</td>\n",
       "      <td>0.615922</td>\n",
       "      <td>0.951389</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.619367</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.602077</td>\n",
       "      <td>0.994048</td>\n",
       "      <td>0.775794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23.246278</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>0.274159</td>\n",
       "      <td>0.997520</td>\n",
       "      <td>0.878968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>30.803239</td>\n",
       "      <td>0.007612</td>\n",
       "      <td>0.280689</td>\n",
       "      <td>0.998016</td>\n",
       "      <td>0.898810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>38.465750</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>0.999504</td>\n",
       "      <td>0.998016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  total time  train loss  val loss  train ACC   val ACC\n",
       "0      0    7.833302    0.164662  0.615922   0.951389  0.777778\n",
       "1      1   15.619367    0.021400  0.602077   0.994048  0.775794\n",
       "2      2   23.246278    0.011062  0.274159   0.997520  0.878968\n",
       "3      3   30.803239    0.007612  0.280689   0.998016  0.898810\n",
       "4      4   38.465750    0.004637  0.014747   0.999504  0.998016"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use DINOv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-small')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load DINOv2 Model and Processor\n",
    "model_name = \"facebook/dinov2-small\"\n",
    "dinov2 = AutoModel.from_pretrained(model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the classifier model\n",
    "num_classes = 10  # Replace with the number of classes in your dataset\n",
    "model = DinoClassifier(dinov2, num_classes)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Example Dataset and Dataloader (replace with your own dataset)\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        # Preprocess image using DINOv2 processor\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)  # Remove batch dimension\n",
    "        return pixel_values, label\n",
    "\n",
    "# Use CIFAR-10 as an example (replace with your custom dataset)\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataset = CustomImageDataset(train_dataset, processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.fc.parameters(), lr=1e-3)  # Train only classification head initially\n",
    "\n",
    "# Step 1: Freeze the backbone and train the classification head\n",
    "model.freeze_backbone()\n",
    "\n",
    "# Phase 1: Train only the classification head\n",
    "num_epochs_phase_1 = 3  # Train for a few epochs with the backbone frozen\n",
    "for epoch in range(num_epochs_phase_1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for pixel_values, labels in tqdm(train_loader):\n",
    "        pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(pixel_values)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Phase 1 Epoch {epoch+1}/{num_epochs_phase_1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "# Step 2: Unfreeze the backbone and fine-tune the entire model\n",
    "model.unfreeze_backbone()\n",
    "\n",
    "# Update optimizer to include all model parameters for fine-tuning\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # Lower learning rate for fine-tuning\n",
    "\n",
    "# Phase 2: Fine-tune the entire model\n",
    "num_epochs_phase_2 = 5  # Fine-tune for a few more epochs\n",
    "for epoch in range(num_epochs_phase_2):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for pixel_values, labels in tqdm(train_loader):\n",
    "        pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(pixel_values)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Phase 2 Epoch {epoch+1}/{num_epochs_phase_2}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
