{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Homework 11: Text Generation and Decoding Strategies\n",
    "\n",
    "**Total Points: 50**\n",
    "- Reading Questions: 10 points\n",
    "- Part 1 (Decoding Strategies Comparison): 10 points\n",
    "- Part 2 (Building API Helper Functions): 8 points\n",
    "- Part 3 (Model Size Comparison): 8 points\n",
    "- Part 4 (Creative Text Generation): 8 points\n",
    "- Part 5 (Analysis and Comparison): 4 points\n",
    "- Part 6 (Reflection): 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Reading Questions (10 points)\n",
    "\n",
    "Answer the following questions based on Chapter 5: Text Generation from *Natural Language Processing with Transformers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "**Question 1 (2 points):** Explain how autoregressive (causal) language models like GPT-2 generate text. What is conditional text generation, and how does the chain rule of probability factor into the text generation process? Be specific about how the model predicts each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "**Question 2 (2 points):** Why do we use log probabilities instead of regular probabilities when scoring sequences in text generation? Explain the numerical stability problem that arises with regular probabilities and how log probabilities solve it. Include a brief discussion of the mathematical transformation involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "**Question 3 (2 points):** Compare and contrast greedy search decoding with beam search decoding. What are the advantages of beam search over greedy search? What problem do both methods share, and how can the `no_repeat_ngram_size` parameter help address it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "**Question 4 (2 points):** Explain the role of the temperature parameter in sampling-based text generation. How does temperature affect the probability distribution over tokens? What happens when temperature is very low (T << 1) versus very high (T >> 1), and what are the trade-offs in terms of text quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "**Question 5 (2 points):** Describe top-k and nucleus (top-p) sampling methods. How do they differ in how they restrict the vocabulary for sampling? According to the textbook, which decoding methods should you use for tasks that require factual correctness (like summarization) versus tasks that benefit from creativity (like story generation)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Part 1 - Decoding Strategies Comparison (10 points)\n",
    "\n",
    "In this part, you'll experiment with different decoding strategies to see how they affect generated text quality and diversity.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load a text generation model (e.g., GPT-2 or a Llama model like \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\")\n",
    "2. Create an interesting prompt for text generation (e.g., a story opening, a technical explanation, or a dialogue start)\n",
    "3. Generate text using at least 5 different decoding configurations:\n",
    "   - Greedy search (do_sample=False)\n",
    "   - Beam search with num_beams=5\n",
    "   - Beam search with no_repeat_ngram_size=2\n",
    "   - Sampling with temperature=0.5\n",
    "   - Sampling with temperature=1.5\n",
    "   - Top-k sampling (top_k=50)\n",
    "   - Nucleus sampling (top_p=0.9)\n",
    "   \n",
    "4. For each generated text:\n",
    "   - Display the full generated text\n",
    "   - Calculate and display the sequence log probability\n",
    "   - Comment on the coherence, diversity, and quality\n",
    "   \n",
    "5. Write a summary (3-4 paragraphs) comparing the different decoding strategies. Which produced the most coherent text? Which was most creative? Which would you choose for different use cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Define functions for log probability calculation (similar to textbook examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Experiment with different decoding strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "üìù **YOUR COMPARISON SUMMARY HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Part 2 - Building API Helper Functions (8 points)\n",
    "\n",
    "One of the key skills for working with modern LLMs is building reusable helper functions that wrap API calls. In this part, you'll build a simplified version of the `llm_generate()` function from the `introdl` package.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a function `simple_api_generate()` that:\n",
    "   - Takes parameters: `prompt` (str), `model` (str, default=\"openai/gpt-4o-mini\"), `temperature` (float, default=0.7), `max_tokens` (int, default=500), `api_key` (optional)\n",
    "   - Loads the API key from environment variables if not provided\n",
    "   - Creates an OpenAI client with base_url=\"https://openrouter.ai/api/v1\"\n",
    "   - Sends a chat completion request\n",
    "   - Returns the generated text\n",
    "   - Handles errors gracefully with try/except\n",
    "\n",
    "2. Test your function with at least 3 different prompts and 2 different models (you can use models from OpenRouter like \"openai/gpt-4o-mini\", \"anthropic/claude-3-5-haiku-20241022\", \"meta-llama/llama-3.3-70b-instruct\")\n",
    "\n",
    "3. Create an enhanced version `advanced_api_generate()` that:\n",
    "   - Includes a system message parameter\n",
    "   - Supports conversation history (list of previous messages)\n",
    "   - Returns both the generated text and token usage information\n",
    "\n",
    "4. Write a brief comparison (1-2 paragraphs) of the outputs from different models. How do they differ in style, length, or quality?\n",
    "\n",
    "**Note:** Make sure you have your OpenRouter API key set in a `.env` file or environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement simple_api_generate() function\n",
    "\n",
    "def simple_api_generate(prompt, model=\"openai/gpt-4o-mini\", \n",
    "                        temperature=0.7, max_tokens=500, api_key=None):\n",
    "    \"\"\"\n",
    "    Generate text using an LLM via OpenRouter.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt\n",
    "        model: Model name on OpenRouter\n",
    "        temperature: Sampling temperature\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        api_key: OpenRouter API key (optional, loads from env if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text or error message\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Test simple_api_generate() with different prompts and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement advanced_api_generate() function\n",
    "\n",
    "def advanced_api_generate(prompt, system_message=None, conversation_history=None,\n",
    "                         model=\"openai/gpt-4o-mini\", temperature=0.7, \n",
    "                         max_tokens=500, api_key=None):\n",
    "    \"\"\"\n",
    "    Enhanced version with system messages and conversation history.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user prompt\n",
    "        system_message: Optional system message for behavior control\n",
    "        conversation_history: Optional list of previous messages\n",
    "        model: Model name on OpenRouter\n",
    "        temperature: Sampling temperature\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        api_key: OpenRouter API key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'text' and 'usage' keys\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Test advanced_api_generate() with multi-turn conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "üìù **YOUR MODEL COMPARISON HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Part 3 - Model Size Comparison (8 points)\n",
    "\n",
    "Different model sizes offer different trade-offs between quality and computational cost. In this part, you'll compare text generation across different model sizes.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load at least two different-sized models (e.g., 3B and 8B parameter models). Suggested options:\n",
    "   - \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" (3B parameters, 4-bit quantized)\n",
    "   - \"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\" (8B parameters, 4-bit quantized)\n",
    "   \n",
    "2. Create 3 diverse prompts that test different capabilities:\n",
    "   - A factual knowledge question\n",
    "   - A creative writing task\n",
    "   - A reasoning/problem-solving task\n",
    "   \n",
    "3. Generate responses from each model for each prompt using the same decoding parameters (e.g., temperature=0.7, top_p=0.9)\n",
    "\n",
    "4. For each generation, measure and record:\n",
    "   - Generation time\n",
    "   - Memory usage (if possible)\n",
    "   - Response length\n",
    "   - Qualitative assessment of quality\n",
    "   \n",
    "5. Create visualizations:\n",
    "   - Bar chart comparing generation times\n",
    "   - Any other relevant comparisons\n",
    "   \n",
    "6. Write an analysis (2-3 paragraphs) discussing:\n",
    "   - Performance differences between model sizes\n",
    "   - Quality differences in outputs\n",
    "   - When you would choose each model size\n",
    "   - Trade-offs between speed and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Load different-sized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create prompts and generate responses with timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create visualizations comparing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "üìù **YOUR ANALYSIS HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Part 4 - Creative Text Generation Application (8 points)\n",
    "\n",
    "Apply what you've learned about text generation to build a creative application. Choose ONE of the following:\n",
    "\n",
    "### Option A: Story Continuation System\n",
    "- Create a system that takes a story opening and generates multiple continuations using different decoding strategies\n",
    "- Allow the user to select which continuation they prefer\n",
    "- Continue the story iteratively, building on the selected continuations\n",
    "- Generate at least 3 rounds of story development\n",
    "\n",
    "### Option B: Dialogue Generator\n",
    "- Create a multi-character dialogue system\n",
    "- Define personalities for 2-3 characters (via system prompts)\n",
    "- Generate a conversation between the characters on a given topic\n",
    "- Experiment with different decoding parameters for each character to reflect their personality\n",
    "\n",
    "### Option C: Writing Style Transformer\n",
    "- Take a piece of text and rewrite it in different styles (formal, casual, poetic, technical, etc.)\n",
    "- Use both local models and API-based models\n",
    "- Compare how different models handle style transformation\n",
    "- Test with at least 3 different input texts and 4 different target styles\n",
    "\n",
    "**Requirements:**\n",
    "- Use at least 2 different models (one local, one via API)\n",
    "- Experiment with at least 3 different decoding configurations\n",
    "- Include clear output formatting and labeling\n",
    "- Write a reflection (2-3 paragraphs) on what worked well and what didn't, and what you learned about text generation from this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement your chosen creative application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "üìù **YOUR REFLECTION HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Part 5 - Analysis and Comparison (4 points)\n",
    "\n",
    "Synthesize your findings from all the previous parts into a comprehensive analysis.\n",
    "\n",
    "**Write 2-3 paragraphs addressing:**\n",
    "\n",
    "1. **Local vs. API-Based Generation:**\n",
    "   - Compare the experience of using local models vs. API-based models\n",
    "   - Discuss trade-offs in terms of control, cost, latency, and quality\n",
    "   - When would you choose each approach for a production application?\n",
    "\n",
    "2. **Decoding Strategy Selection:**\n",
    "   - Which decoding strategies performed best for different types of tasks?\n",
    "   - How did you balance coherence and diversity in your experiments?\n",
    "   - What general principles would you follow when choosing decoding parameters?\n",
    "\n",
    "3. **Practical Applications:**\n",
    "   - What real-world applications would benefit from the techniques you explored?\n",
    "   - What challenges did you encounter that would need to be addressed for production use?\n",
    "   - What additional features or improvements would make your implementations more robust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "üìù **YOUR ANALYSIS HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Part 6 - Reflection (2 points)\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for the lesson? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
