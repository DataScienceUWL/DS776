{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TOKEN = os.environ.get('HF_TOKEN')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Specify the model name or path\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\" # 1B model\n",
    "#MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\" # 3B model\n",
    "MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import TextWrapper\n",
    "\n",
    "def wrap_print_text(print):\n",
    "    \"\"\"Adapted from: https://stackoverflow.com/questions/27621655/how-to-overload-print-function-to-expand-its-functionality/27621927\"\"\"\n",
    "\n",
    "    def wrapped_func(text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        wrapper = TextWrapper(\n",
    "            width=80,\n",
    "            break_long_words=True,\n",
    "            break_on_hyphens=False,\n",
    "            replace_whitespace=False,\n",
    "        )\n",
    "        return print(\"\\n\".join(wrapper.fill(line) for line in text.split(\"\\n\")))\n",
    "\n",
    "    return wrapped_func\n",
    "\n",
    "# Wrap the print function\n",
    "print = wrap_print_text(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_model(model_name=MODEL_NAME, device='cuda', token=TOKEN):\n",
    "    \"\"\"\n",
    "    Load the LLaMA model and tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,token=token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",  # Automatically selects FP16 if GPU is used\n",
    "        device_map=\"auto\",    # Automatically maps the model to GPU\n",
    "        token=token\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_llama_response(prompt, model, tokenizer, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate a response to a given prompt using the LLaMA model.\n",
    "    \"\"\"\n",
    "    # Ensure the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,  # Ensures padding is applied if needed\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "def unload_model(model):\n",
    "    \"\"\"\n",
    "    Unload the model and clear GPU memory.\n",
    "    \"\"\"\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_llama_response(\"How many moons does Mars have?\", model, tokenizer, max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many moons does Mars have? Mars has two small natural satellites, Phobos and\n",
      "Deimos. They were discovered in 1877 by astronomer Asaph Hall. Phobos is about\n",
      "22 km in diameter and Deimos is about 12 km in diameter. They are thought to be\n",
      "captured asteroids.\n",
      "Phobos orbits Mars at an average distance of 6,000 km, which is closer than the\n",
      "planet's own geosynchronous orbit. Phobos is tidally locked to Mars, which means\n",
      "that it always shows the same face to the planet as it orbits. Deimos orbits at\n",
      "an average distance of 20,000 km, which is farther than Phobos but still within\n",
      "the planet's Hill sphere.\n",
      "Both Phobos and Deimos were likely formed in the asteroid belt and were captured\n",
      "by Mars' gravity. The exact origin of these moons is still a topic of research\n",
      "and debate. The moons were discovered in 1877 by astronomer Asaph Hall and were\n",
      "named after characters in Greek mythology. Phobos is named after the god of fear\n",
      "and panic, while Deimos is named after the god of fear and terror.\n",
      "The moons of Mars are of great interest to scientists because they provide\n",
      "insights into the planet's geology, atmosphere, and potential habitability. The\n",
      "moons also offer opportunities for future space missions, such as landing on\n",
      "their surfaces or studying their orbits to better understand the Martian system.\n",
      "Phobos and Deimos are small, irregularly shaped bodies that are thought to be\n",
      "remnants of a larger object that was destroyed in a collision with Mars. They\n",
      "are composed primarily of carbonaceous chondrite material and are thought to be\n",
      "some of the oldest objects in the solar system. The moons are of great interest\n",
      "to scientists because they provide insights into the early formation and\n",
      "evolution of the solar system. Their study also helps scientists understand the\n",
      "potential hazards associated with asteroid and comet impacts on planets.\n",
      "The study of Phobos and Deimos has also led to a better understanding of the\n",
      "Martian system and its potential for hosting life. The moons are thought to be\n",
      "the result of a massive impact that occurred early in the planet's history, and\n",
      "their study has helped scientists understand the geological and atmospheric\n",
      "processes that shape Mars. The study of Phobos and Deimos also provides insights\n",
      "into the potential for life on Mars, as the moons are thought to be similar in\n",
      "composition to the Martian surface.\n",
      "The study of Phobos and Deimos has also\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
