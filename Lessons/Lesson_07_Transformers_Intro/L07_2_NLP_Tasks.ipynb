{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ introdl v1.6.21 already up to date\n"
     ]
    }
   ],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:18.532720Z",
     "iopub.status.busy": "2025-08-26T00:43:18.532532Z",
     "iopub.status.idle": "2025-08-26T00:43:33.211450Z",
     "shell.execute_reply": "2025-08-26T00:43:33.210391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment: Unknown Environment | Course root: /mnt/e/GDrive_baggett.jeff/Teaching/Classes_current/2025-2026_Fall_DS776/DS776\n",
      "   Using workspace: <DS776_ROOT_DIR>/home_workspace\n",
      "\n",
      "üìÇ Storage Configuration:\n",
      "   DATA_PATH: <DS776_ROOT_DIR>/home_workspace/data\n",
      "   MODELS_PATH: <DS776_ROOT_DIR>/Lessons/Lesson_07_Transformers_Intro/Lesson_07_Models (local to this notebook)\n",
      "   CACHE_PATH: <DS776_ROOT_DIR>/home_workspace/downloads\n",
      "üîë API keys: 9 loaded from home_workspace/api_keys.env\n",
      "üîê Available: ANTHROPIC_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY... (9 total)\n",
      "‚úÖ HuggingFace Hub: Logged in\n",
      "‚úÖ Loaded pricing for 330 OpenRouter models\n",
      "‚úÖ Cost tracking initialized ($9.92 credit remaining)\n",
      "üì¶ introdl v1.6.21 ready\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from introdl import (\n",
    "    get_device, \n",
    "    wrap_print_text, \n",
    "    config_paths_keys,\n",
    "    llm_generate, \n",
    "    clear_pipeline, \n",
    "    print_pipeline_info, \n",
    "    display_markdown,\n",
    "    show_session_spending\n",
    ")\n",
    "\n",
    "# Wrap print to format text nicely at 120 characters\n",
    "print = wrap_print_text(print, width=120)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "paths = config_paths_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L07_2_NLP_Tasks Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_2_nlp_tasks\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_2_nlp_tasks\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/omj2ldze713\" target=\"_blank\">Open Descript version of video in new tab</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Tasks with Transformer Models\n",
    "\n",
    "In this notebook we'll demonstrate solutions to some common Natural Language Processing (NLP) tasks that use transformer models.  We expand on the material in our NLP textbook Chapter 1 - Hello Transformers.  We'll add a little background about the underlying models.  We'll also demonstrate how these same tasks come be done using a large language model with either \"zero-shot prompting\" or \"few-shot prompting\".\n",
    "\n",
    "Over the next five lessons we'll go into some of these NLP tasks in detail and a learn a bit about the transformer neural network architecture.  For each of the NLP tasks that follows we'll demonstrate how to do the task two ways.  The first is by using a pre-trained transformer-based model downloaded from HuggingFace.  In the second approach we'll use a a large language model and prompting.\n",
    "\n",
    "Using LLMs for various NLP tasks is common when there isn't much labeled data available.  Zero-shot prompting means that no examples are provided to the LLM.  Few-shot prompting means that a small number of examples are provided to the LLM.  In this notebook we'll demonstrate zero-shot prompting, but in the lessons to come we'll include few-shot prompting examples.\n",
    "\n",
    "Throughout this notebook we'll use the following customer feedback message that as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:33.214197Z",
     "iopub.status.busy": "2025-08-26T00:43:33.213694Z",
     "iopub.status.idle": "2025-08-26T00:43:33.218096Z",
     "shell.execute_reply": "2025-08-26T00:43:33.217310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three days, I hadn‚Äôt even\n",
      "received a shipping update. After waiting 45 minutes on hold, customer service told me there was a stock issue‚Äîyet no\n",
      "one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The support rep was apologetic\n",
      "but said an exchange would take another two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. To add\n",
      "insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake. The entire experience has been incredibly\n",
      "disappointing and has left me questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and efficiency. I hope this feedback reaches\n",
      "someone who can make a difference, as no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three days, I hadn‚Äôt even received a shipping update. After waiting 45 minutes on hold, customer service told me there was a stock issue‚Äîyet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. To add insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. I had to explain my situation multiple times before they even acknowledged the mistake. The entire experience has been incredibly disappointing and has left me questioning whether I should ever shop with Tech Haven again. \n",
    "\n",
    "It's baffling how a company can operate with such a lack of transparency and efficiency. I hope this feedback reaches someone who can make a difference, as no customer should have to go through what I did. Tech Haven, you need to do better! Sincerely, Jamie.\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Classification\n",
    "\n",
    "Text classification is the process of assigning predefined categories to text. It involves analyzing the content of the text and categorizing it based on its subject, sentiment, or other criteria. One common application of text classification is sentiment analysis, which determines the sentiment expressed in a piece of text, such as positive, negative, or neutral. Sentiment analysis is widely used in customer feedback analysis, social media monitoring, and market research to gauge public opinion and customer satisfaction.\n",
    "\n",
    "### Sentiment Analysis with a Specialized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for sentiment analysis and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:33.220072Z",
     "iopub.status.busy": "2025-08-26T00:43:33.219895Z",
     "iopub.status.idle": "2025-08-26T00:43:34.243427Z",
     "shell.execute_reply": "2025-08-26T00:43:34.242537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english, Size: 66,955,010 parameters\n",
      "[{'label': 'NEGATIVE', 'score': 0.9989209175109863}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "print_pipeline_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a \"BERT\" model correctly classified the customer feedback as negative. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model developed by Google. It is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows BERT to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. The particular model used here is a distilled BERT model that has been fine-tuned on a sentiment dataset. A distilled model is a smaller, faster, and more efficient version of a larger model, trained using knowledge distillation, where the smaller model learns to mimic the outputs of the larger one while retaining most of its performance. In Lesson 9, we'll learn more about the family of transformer models called encoders, which include BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's never a bad idea to remove models from memory when they aren't being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:34.245423Z",
     "iopub.status.busy": "2025-08-26T00:43:34.245230Z",
     "iopub.status.idle": "2025-08-26T00:43:34.915933Z",
     "shell.execute_reply": "2025-08-26T00:43:34.915078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(sentiment_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentiment Analysis with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "A system prompt is used to give instructions to an LLM while a user prompt is the specific input you want the LLM to respond to.  Here we define a system prompt for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:34.917831Z",
     "iopub.status.busy": "2025-08-26T00:43:34.917619Z",
     "iopub.status.idle": "2025-08-26T00:43:38.262730Z",
     "shell.execute_reply": "2025-08-26T00:43:38.261791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "response_zero_shot = llm_generate('gemini-flash-lite', user_prompt, system_prompt=system_prompt)\n",
    "print(response_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also handle batches of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:38.265132Z",
     "iopub.status.busy": "2025-08-26T00:43:38.264867Z",
     "iopub.status.idle": "2025-08-26T00:43:53.851512Z",
     "shell.execute_reply": "2025-08-26T00:43:53.850636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fast shipping and great customer support. Highly recommend!\n",
      "Sentiment: positive\n",
      "\n",
      "Text: The item arrived damaged and the return process was a nightmare.\n",
      "Sentiment: negative\n",
      "\n",
      "Text: I'm very satisfied with my purchase. Will buy again.\n",
      "Sentiment: positive\n",
      "\n",
      "Text: The website is user-friendly and the prices are unbeatable.\n",
      "Sentiment: positive\n",
      "\n",
      "Text: Received the wrong item and customer service was unhelpful.\n",
      "Sentiment: negative\n",
      "\n",
      "Text: Fantastic experience from start to finish.\n",
      "Sentiment: positive\n",
      "\n",
      "Text: The product is okay, but not worth the price.\n",
      "Sentiment: negative\n",
      "\n",
      "Text: Excellent quality and quick delivery. Very happy!\n",
      "Sentiment: positive\n",
      "\n",
      "Text: The product works as expected, nothing more, nothing less.\n",
      "Sentiment: neutral\n",
      "\n",
      "Text: I have mixed feelings about the service; it was both good and bad.\n",
      "Sentiment: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customer_comments = [\n",
    "    \"Fast shipping and great customer support. Highly recommend!\",\n",
    "    \"The item arrived damaged and the return process was a nightmare.\",\n",
    "    \"I'm very satisfied with my purchase. Will buy again.\",\n",
    "    \"The website is user-friendly and the prices are unbeatable.\",\n",
    "    \"Received the wrong item and customer service was unhelpful.\",\n",
    "    \"Fantastic experience from start to finish.\",\n",
    "    \"The product is okay, but not worth the price.\",\n",
    "    \"Excellent quality and quick delivery. Very happy!\",\n",
    "    \"The product works as expected, nothing more, nothing less.\",\n",
    "    \"I have mixed feelings about the service; it was both good and bad.\"\n",
    "]\n",
    "\n",
    "user_prompts = [f\"Text: {comment}\\nSentiment:\" for comment in customer_comments]\n",
    "\n",
    "responses_zero_shot = llm_generate('gemini-flash-lite', user_prompts, system_prompt=system_prompt)\n",
    "\n",
    "for comment, response_zero_shot in zip(customer_comments, responses_zero_shot):\n",
    "    print(f\"Text: {comment}\\nSentiment: {response_zero_shot}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text classification more in Lesson 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to Write Better Prompts\n",
    "\n",
    "There are many prompt engineering resources available on the internet.  I encourage you to look at those as needed.  I've also had good luck asking ChatGPT how to craft prompts.  One particularly useful resource is [ChatGPT Prompt Engineering for Developers](https://app.datacamp.com/learn/courses/chatgpt-prompt-engineering-for-developers) on DataCamp.  It's not free, but it is cheap.  I've viewed parts of this course and found it to be a very good introduction to programatic prompt writing.  It's tailored to ChatGPT, but you can use the OpenAI API with Gemini as well.  Working through this short course (they say it takes 4 hours) is likely worthwhile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Practical examples of NER include:\n",
    "- **Business Application**: Extracting company names, dates, and monetary amounts from financial reports to automate data entry and analysis.\n",
    "- **Healthcare**: Identifying patient names, medical conditions, and treatment dates from clinical notes to improve patient record management.\n",
    "- **News Aggregation**: Categorizing and tagging entities like people, places, and events in news articles to enhance search and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for NER and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:53.853339Z",
     "iopub.status.busy": "2025-08-26T00:43:53.853145Z",
     "iopub.status.idle": "2025-08-26T00:43:56.291882Z",
     "shell.execute_reply": "2025-08-26T00:43:56.290706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Named Entity Recognition**\n",
      "\n",
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889 parameters\n",
      "\n",
      "[{'entity_group': 'MISC', 'score': 0.990973, 'word': 'Samsung Galaxy S24 Ultra', 'start': 14, 'end': 38},\n",
      "{'entity_group': 'ORG', 'score': 0.994846, 'word': 'Tech Haven', 'start': 44, 'end': 54}, {'entity_group': 'MISC',\n",
      "'score': 0.9928634, 'word': 'Google Pixel 8 Pro', 'start': 323, 'end': 341}, {'entity_group': 'ORG', 'score': 0.9964845,\n",
      "'word': 'Tech Haven', 'start': 863, 'end': 873}, {'entity_group': 'ORG', 'score': 0.9887396, 'word': 'Tech Haven',\n",
      "'start': 1089, 'end': 1099}, {'entity_group': 'PER', 'score': 0.9780477, 'word': 'Jamie', 'start': 1135, 'end': 1140}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\\n\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print_pipeline_info(ner_pipeline)\n",
    "print(\"\")\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That output is hard to read, but we can easily convert it to a Pandas data frame for display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:56.294124Z",
     "iopub.status.busy": "2025-08-26T00:43:56.293807Z",
     "iopub.status.idle": "2025-08-26T00:43:56.304805Z",
     "shell.execute_reply": "2025-08-26T00:43:56.303849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity_group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "end",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "4bc8e55e-9ea6-4836-9da5-af9318581a2c",
       "rows": [
        [
         "0",
         "MISC",
         "0.990973",
         "Samsung Galaxy S24 Ultra",
         "14",
         "38"
        ],
        [
         "1",
         "ORG",
         "0.994846",
         "Tech Haven",
         "44",
         "54"
        ],
        [
         "2",
         "MISC",
         "0.9928634",
         "Google Pixel 8 Pro",
         "323",
         "341"
        ],
        [
         "3",
         "ORG",
         "0.9964845",
         "Tech Haven",
         "863",
         "873"
        ],
        [
         "4",
         "ORG",
         "0.9887396",
         "Tech Haven",
         "1089",
         "1099"
        ],
        [
         "5",
         "PER",
         "0.9780477",
         "Jamie",
         "1135",
         "1140"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990973</td>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.994846</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.992863</td>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>323</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996485</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>863</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.988740</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>1089</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.978048</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1135</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                      word  start   end\n",
       "0         MISC  0.990973  Samsung Galaxy S24 Ultra     14    38\n",
       "1          ORG  0.994846                Tech Haven     44    54\n",
       "2         MISC  0.992863        Google Pixel 8 Pro    323   341\n",
       "3          ORG  0.996485                Tech Haven    863   873\n",
       "4          ORG  0.988740                Tech Haven   1089  1099\n",
       "5          PER  0.978048                     Jamie   1135  1140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.DataFrame(ner_result)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"BERT\" model used here is the `dbmdz/bert-large-cased-finetuned-conll03-english` model, which has been fine-tuned on the CoNLL-2003 dataset for Named Entity Recognition (NER). This fine-tuning process allows the model to accurately identify and classify entities such as names of persons, organizations, locations, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:56.307086Z",
     "iopub.status.busy": "2025-08-26T00:43:56.306815Z",
     "iopub.status.idle": "2025-08-26T00:43:58.023959Z",
     "shell.execute_reply": "2025-08-26T00:43:58.023048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(ner_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for NER.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:43:58.025960Z",
     "iopub.status.busy": "2025-08-26T00:43:58.025755Z",
     "iopub.status.idle": "2025-08-26T00:44:45.483722Z",
     "shell.execute_reply": "2025-08-26T00:44:45.482911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\"entity\": \"Samsung Galaxy S24 Ultra\", \"type\": \"PRODUCT\"},\n",
      "  {\"entity\": \"Tech Haven\", \"type\": \"ORGANIZATION\"},\n",
      "  {\"entity\": \"Google Pixel 8 Pro\", \"type\": \"PRODUCT\"},\n",
      "  {\"entity\": \"$1,200\", \"type\": \"MONEY\"},\n",
      "  {\"entity\": \"Tech Haven\", \"type\": \"ORGANIZATION\"},\n",
      "  {\"entity\": \"Tech Haven\", \"type\": \"ORGANIZATION\"},\n",
      "  {\"entity\": \"Jamie\", \"type\": \"PERSON\"}\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "response_ner = llm_generate('gemini-flash-lite', user_prompt, system_prompt=system_prompt)\n",
    "print(response_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with String Output (Traditional Approach)\n",
    "\n",
    "Our LLM returned a string containing JSON. Below we parse this string to extract the JSON and display it as a DataFrame. Different LLMs may return different formats which require different parsing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:45.485905Z",
     "iopub.status.busy": "2025-08-26T00:44:45.485611Z",
     "iopub.status.idle": "2025-08-26T00:44:45.495786Z",
     "shell.execute_reply": "2025-08-26T00:44:45.494660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7826bb80-266b-46c8-b5f8-f599d28d1fc9",
       "rows": [
        [
         "0",
         "Samsung Galaxy S24 Ultra",
         "PRODUCT"
        ],
        [
         "1",
         "Tech Haven",
         "ORGANIZATION"
        ],
        [
         "2",
         "Google Pixel 8 Pro",
         "PRODUCT"
        ],
        [
         "3",
         "$1,200",
         "MONEY"
        ],
        [
         "4",
         "Tech Haven",
         "ORGANIZATION"
        ],
        [
         "5",
         "Tech Haven",
         "ORGANIZATION"
        ],
        [
         "6",
         "Jamie",
         "PERSON"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1,200</td>\n",
       "      <td>MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jamie</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     entity          type\n",
       "0  Samsung Galaxy S24 Ultra       PRODUCT\n",
       "1                Tech Haven  ORGANIZATION\n",
       "2        Google Pixel 8 Pro       PRODUCT\n",
       "3                    $1,200         MONEY\n",
       "4                Tech Haven  ORGANIZATION\n",
       "5                Tech Haven  ORGANIZATION\n",
       "6                     Jamie        PERSON"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Clean the response_ner string\n",
    "response_ner = response_ner.strip('```json\\n').strip('\\n```')\n",
    "\n",
    "# Convert the cleaned response to a DataFrame for display\n",
    "ner_result = json.loads(response_ner)\n",
    "\n",
    "# ner_result is already a list - use it directly\n",
    "df = pd.DataFrame(ner_result)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with JSON Mode (Modern Approach)\n",
    "\n",
    "Alternatively, we can use `mode='json'` in `llm_generate()` to get structured JSON output directly without needing to parse strings. This is more reliable and works with models that support JSON output.  All the suggested models from OpenRouter support JSON output.  Most of them even support JSON output that must conform to a user-defined template.  We'll see more about that in Lesson 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the structured JSON output from the model printed in raw format:\n",
      "\n",
      "[{'entity_name': 'Samsung Galaxy S24 Ultra', 'entity_type': 'PRODUCT'}, {'entity_name': 'Tech Haven', 'entity_type':\n",
      "'ORGANIZATION'}, {'entity_name': 'Google Pixel 8 Pro', 'entity_type': 'PRODUCT'}, {'entity_name': '$1,200',\n",
      "'entity_type': 'MONEY'}, {'entity_name': 'Tech Haven', 'entity_type': 'ORGANIZATION'}, {'entity_name': 'Tech Haven',\n",
      "'entity_type': 'ORGANIZATION'}, {'entity_name': 'Jamie', 'entity_type': 'PERSON'}]\n",
      "\n",
      " That's not too helpful to look at. Let's put it in a DataFrame.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "entity_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "33f10a96-76e8-4305-9380-31b138bdb376",
       "rows": [
        [
         "0",
         "Samsung Galaxy S24 Ultra",
         "PRODUCT"
        ],
        [
         "1",
         "Tech Haven",
         "ORGANIZATION"
        ],
        [
         "2",
         "Google Pixel 8 Pro",
         "PRODUCT"
        ],
        [
         "3",
         "$1,200",
         "MONEY"
        ],
        [
         "4",
         "Tech Haven",
         "ORGANIZATION"
        ],
        [
         "5",
         "Tech Haven",
         "ORGANIZATION"
        ],
        [
         "6",
         "Jamie",
         "PERSON"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1,200</td>\n",
       "      <td>MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jamie</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                entity_name   entity_type\n",
       "0  Samsung Galaxy S24 Ultra       PRODUCT\n",
       "1                Tech Haven  ORGANIZATION\n",
       "2        Google Pixel 8 Pro       PRODUCT\n",
       "3                    $1,200         MONEY\n",
       "4                Tech Haven  ORGANIZATION\n",
       "5                Tech Haven  ORGANIZATION\n",
       "6                     Jamie        PERSON"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "response_ner = llm_generate('gemini-flash-lite', user_prompt, system_prompt=system_prompt, mode='json')\n",
    "\n",
    "print(\"Here's the structured JSON output from the model printed in raw format:\\n\")\n",
    "print(response_ner)\n",
    "print(\"\\n That's not too helpful to look at. Let's put it in a DataFrame.\")\n",
    "\n",
    "df = pd.DataFrame(response_ner)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the LLM is similar to that of the specialized model from HuggingFace.  If we want different output from the LLM we could include instructions for that in our system prompt.\n",
    "\n",
    "In Lesson 10 we'll learn more about Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Question Answering\n",
    "\n",
    "Question Answering (QA) is a subtask of information retrieval and natural language understanding that involves automatically answering questions posed by humans in a natural language. QA systems can be designed to answer questions based on a given context or a large corpus of documents. The goal is to provide accurate and relevant answers to user queries.\n",
    "\n",
    "Practical examples of QA include:\n",
    "- **Customer Support**: Providing instant answers to customer queries based on a knowledge base or FAQ, improving response times and customer satisfaction.\n",
    "- **Education**: Assisting students by answering questions related to their coursework or providing explanations for complex topics.\n",
    "- **Healthcare**: Offering medical professionals quick access to information from medical literature or patient records to support clinical decision-making.\n",
    "- **Search Engines**: Enhancing search results by directly providing answers to user queries, rather than just a list of relevant documents.\n",
    "\n",
    "Here we will let the HuggingFace transformers library provide its default model for QA and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:45.498408Z",
     "iopub.status.busy": "2025-08-26T00:44:45.498231Z",
     "iopub.status.idle": "2025-08-26T00:44:46.507507Z",
     "shell.execute_reply": "2025-08-26T00:44:46.506695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Question Answering**\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c80c16be26489395e96cd40a8897f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9019804ee03841cdb7a1abfc9224bbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a660d2ba244444ee9ae77693d3a18274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450 parameters\n",
      "\n",
      "{'score': 0.4041374623775482, 'start': 218, 'end': 231, 'answer': 'a stock issue'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\\n\")\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "print_pipeline_info(qa_pipeline)\n",
    "print(\"\")\n",
    "question = \"What is the main issue?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:46.509470Z",
     "iopub.status.busy": "2025-08-26T00:44:46.509292Z",
     "iopub.status.idle": "2025-08-26T00:44:47.175036Z",
     "shell.execute_reply": "2025-08-26T00:44:47.174188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(qa_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for QA.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:44:47.176975Z",
     "iopub.status.busy": "2025-08-26T00:44:47.176790Z",
     "iopub.status.idle": "2025-08-26T00:45:09.158225Z",
     "shell.execute_reply": "2025-08-26T00:45:09.157329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main issue is Tech Haven's mishandling of Jamie's order, including significant delays, poor communication,\n",
      "delivering the wrong product, and unhelpful customer service.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. Be succinct.\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: What is the main issue?\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate('gemini-flash-lite', user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a lesson dedicated to question answering, but it's discussed in our NLP textbook in Chapter 7.  You could investigate this topic further in a project if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Translation\n",
    "\n",
    "The first transformer model in the paper \"Attention is All You Need\" was designed for the task of language translation. This model, known as the Transformer, introduced a novel architecture that relies entirely on self-attention mechanisms to process input sequences, making it highly effective for translation tasks. The Transformer model has since become the foundation for many state-of-the-art NLP models, including BERT, GPT, and others.\n",
    "\n",
    "Here we demonstrate how to use a pre-trained model from HuggingFace for translating English to Spanish.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:17.946147Z",
     "iopub.status.busy": "2025-08-26T00:45:17.945967Z",
     "iopub.status.idle": "2025-08-26T00:45:27.271748Z",
     "shell.execute_reply": "2025-08-26T00:45:27.270895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Translation**\n",
      "\n",
      "Model: Helsinki-NLP/opus-mt-en-es, Size: 77,943,296 parameters\n",
      "\n",
      "Ped√≠ el Samsung Galaxy S24 Ultra de Tech Haven, esperando la entrega del d√≠a siguiente, pero despu√©s de tres d√≠as, yo ni\n",
      "siquiera hab√≠a recibido una actualizaci√≥n de env√≠o. Despu√©s de esperar 45 minutos en espera, el servicio al cliente me\n",
      "dijo que hab√≠a un problema de existencias ‚Äî sin embargo nadie me hab√≠a informado! Cuando el paquete finalmente lleg√≥ una\n",
      "semana tarde, que conten√≠a un Google Pixel 8 Pro en su lugar. El representante de apoyo era apolog√©tico, pero dijo que\n",
      "un intercambio tomar√≠a otras dos semanas. Pagu√© $1.200 por el tel√©fono equivocado, trat√≥ con retrasos y mala\n",
      "comunicaci√≥n, y ahora tienen que esperar incluso m√°s tiempo. Para a√±adir insulto a la lesi√≥n, el representante de\n",
      "servicio al cliente con el que habl√© parec√≠a indiferente a mi frustraci√≥n. Tuve que explicar mi situaci√≥n varias veces\n",
      "antes de que incluso reconocieron el error. Toda la experiencia ha sido incre√≠blemente decepcionante y me ha dejado\n",
      "cuestionando si alguna vez deber√≠a comprar con Tech Haven de nuevo. Es desconcertante c√≥mo una empresa puede funcionar\n",
      "con tal falta de transparencia y eficiencia. Espero que esta retroalimentaci√≥n llega a alguien que puede hacer una\n",
      "diferencia, ya que ning√∫n cliente debe ir a trav√©s de lo que hice. Tech Haven!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\\n\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "print_pipeline_info(translation_pipeline)\n",
    "print(\"\")\n",
    "translation_result = translation_pipeline(text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you're better than I am at Spanish, but I can't read that well enough to know if it's a good translation.  However, let's now use a similar model to translate it from Spanish back into English and compare it to the orginal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:27.273604Z",
     "iopub.status.busy": "2025-08-26T00:45:27.273421Z",
     "iopub.status.idle": "2025-08-26T00:45:35.643353Z",
     "shell.execute_reply": "2025-08-26T00:45:35.642462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Helsinki-NLP/opus-mt-es-en, Size: 77,943,296 parameters\n",
      "Back Translation to English: I ordered the Samsung Galaxy S24 Ultra from Tech Haven, waiting for delivery the next day,\n",
      "but after three days, I had not even received a shipping update. After waiting 45 minutes on hold, the customer service\n",
      "told me that there was a stock problem ‚Äî yet no one had informed me! When the package finally arrived a week late,\n",
      "containing a Google Pixel 8 Pro instead. The support representative was apologetic, but he said an exchange would take\n",
      "another two weeks. I paid $1,200 for the wrong phone, tried with delays and bad communication, and now they have to wait\n",
      "even longer. To add insult to the injury, the customer service representative with whom I spoke seemed indifferent to my\n",
      "frustration. I had to explain my situation several times before they even recognized the error. All the experience has\n",
      "been incredibly disappointing and has left me wondering if I should ever buy with Tech Haven again. It is disconcerting\n",
      "how a company can function with such a lack of transparency and efficiency. I hope this feedback comes to someone who\n",
      "can make a difference, as no customer should go through what I did. Tech Haven!\n",
      "\n",
      "Original Text: I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three\n",
      "days, I hadn‚Äôt even received a shipping update. After waiting 45 minutes on hold, customer service told me there was a\n",
      "stock issue‚Äîyet no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The support rep was apologetic\n",
      "but said an exchange would take another two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. To add\n",
      "insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake. The entire experience has been incredibly\n",
      "disappointing and has left me questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and efficiency. I hope this feedback reaches\n",
      "someone who can make a difference, as no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n"
     ]
    }
   ],
   "source": [
    "# Extract the Spanish translation from the previous result\n",
    "spanish_translation = translation_result[0]['translation_text']\n",
    "\n",
    "# Translate the Spanish text back to English\n",
    "back_translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\", device=device)\n",
    "print_pipeline_info(back_translation_pipeline)\n",
    "back_translation_result = back_translation_pipeline(spanish_translation, max_length=300)\n",
    "print(f\"Back Translation to English: {back_translation_result[0]['translation_text']}\\n\")\n",
    "print(f\"Original Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Helsinki-NLP models are part of the OPUS-MT project, which provides pre-trained neural machine translation models for many language pairs. These models are based on the MarianMT architecture, a transformer-based model optimized for translation tasks. The MarianMT architecture leverages self-attention mechanisms to effectively process and translate text, making these models highly accurate and efficient for translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:35.645625Z",
     "iopub.status.busy": "2025-08-26T00:45:35.645391Z",
     "iopub.status.idle": "2025-08-26T00:45:36.397861Z",
     "shell.execute_reply": "2025-08-26T00:45:36.396978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n",
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(translation_pipeline)\n",
    "clear_pipeline(back_translation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation with an LLM and a Zero-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:45:36.399860Z",
     "iopub.status.busy": "2025-08-26T00:45:36.399625Z",
     "iopub.status.idle": "2025-08-26T00:47:39.438510Z",
     "shell.execute_reply": "2025-08-26T00:47:39.437689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ped√≠ el Samsung Galaxy S24 Ultra en Tech Haven, esperando la entrega al d√≠a siguiente, pero despu√©s de tres d√≠as, ni\n",
      "siquiera hab√≠a recibido una actualizaci√≥n de env√≠o. Despu√©s de esperar 45 minutos en espera, el servicio al cliente me\n",
      "dijo que hab√≠a un problema de stock, ¬°pero nadie me hab√≠a informado!\n",
      "\n",
      "Cuando el paquete finalmente lleg√≥ una semana tarde, conten√≠a un Google Pixel 8 Pro en su lugar. El representante de\n",
      "soporte se disculp√≥, pero dijo que un cambio tardar√≠a otras dos semanas.\n",
      "\n",
      "Pagu√© $1,200 por el tel√©fono equivocado, tuve que lidiar con retrasos y mala comunicaci√≥n, y ahora tengo que esperar a√∫n\n",
      "m√°s. Para colmo, el representante de servicio al cliente con el que habl√© parec√≠a indiferente a mi frustraci√≥n. Tuve que\n",
      "explicar mi situaci√≥n varias veces antes de que reconocieran el error. Toda la experiencia ha sido incre√≠blemente\n",
      "decepcionante y me ha hecho cuestionar si alguna vez volver√© a comprar en Tech Haven.\n",
      "\n",
      "Es desconcertante c√≥mo una empresa puede operar con tanta falta de transparencia y eficiencia. Espero que esta\n",
      "retroalimentaci√≥n llegue a alguien que pueda hacer una diferencia, ya que ning√∫n cliente deber√≠a pasar por lo que yo\n",
      "pas√©. ¬°Tech Haven, necesitan mejorar! Atentamente, Jamie.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_translation = \"\"\"You are an expert translation model. Translate the following text from English to Spanish.\"\"\"\n",
    "user_prompt_translation = f\"Text: {text}\\nTranslation:\"\n",
    "\n",
    "response_translation = llm_generate('gemini-flash-lite',\n",
    "                                    user_prompt_translation, \n",
    "                                    system_prompt=system_prompt_translation,\n",
    "                                    max_tokens=500)\n",
    "print(response_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:47:39.441013Z",
     "iopub.status.busy": "2025-08-26T00:47:39.440769Z",
     "iopub.status.idle": "2025-08-26T00:48:56.864675Z",
     "shell.execute_reply": "2025-08-26T00:48:56.863824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three days, I hadn't even\n",
      "received a shipping update. After waiting 45 minutes on hold, customer service told me there was a stock issue ‚Äî yet no\n",
      "one had informed me! When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The\n",
      "support representative was apologetic but said an exchange would take another two weeks. I paid $1,200 for the wrong\n",
      "phone, dealt with delays and poor communication, and now have to wait even longer. To add insult to injury, the customer\n",
      "service representative I spoke with seemed indifferent to my frustration. I had to explain my situation multiple times\n",
      "before they even acknowledged the mistake. The entire experience has been incredibly disappointing and has left me\n",
      "questioning if I should ever purchase from Tech Haven again. It's baffling how a company can operate with such a lack of\n",
      "transparency and efficiency. I hope this feedback reaches someone who can make a difference, as no customer should have\n",
      "to go through what I did. Tech Haven!\n"
     ]
    }
   ],
   "source": [
    "system_prompt_back_translation = \"\"\"You are an expert translation model. Translate the following text from Spanish to English.\"\"\"\n",
    "user_prompt_back_translation = f\"Text: {spanish_translation}\\nTranslation:\"\n",
    "\n",
    "response_back_translation = llm_generate('gemini-flash-lite',\n",
    "                                         user_prompt_back_translation, \n",
    "                                         system_prompt=system_prompt_back_translation,\n",
    "                                         max_tokens=500)\n",
    "print(response_back_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't specifically study translation models in one of our lessons, the transformer models used for text summarization are similar in that they take an input sequence of text and produce an output sequence of text.  These models are called sequence to sequence transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Generation\n",
    "\n",
    "Of all the models we'll study, text-generation models are perhaps the most familiar since they are the machines that drive today's chatbots like ChatGPT, Gemini, Claude, and others. Given an input sequence that provides context, a text generation model predicts a likely next word, then does it again and again to generate a hopefully sensible response. These models are particularly useful for tasks such as drafting emails, writing code, creating conversational agents, and generating creative content like stories and poems.\n",
    "\n",
    "HuggingFace makes it simple to create a text generation pipeline.  Here we provide the original customer comment plus the beginning of customer service response and ask the modlel to generate 200 new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:48:56.866564Z",
     "iopub.status.busy": "2025-08-26T00:48:56.866382Z",
     "iopub.status.idle": "2025-08-26T00:49:06.521925Z",
     "shell.execute_reply": "2025-08-26T00:49:06.520973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Text Generation**\n",
      "\n",
      "Model: openai-community/gpt2, Size: 124,439,808 parameters\n",
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three days, I hadn‚Äôt even\n",
      "received a shipping update. After waiting 45 minutes on hold, customer service told me there was a stock issue‚Äîyet no\n",
      "one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The support rep was apologetic\n",
      "but said an exchange would take another two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. To add\n",
      "insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake. The entire experience has been incredibly\n",
      "disappointing and has left me questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and efficiency. I hope this feedback reaches\n",
      "someone who can make a difference, as no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up. The package arrived on time and the exchange was sent late.\n",
      "I do not know what to do. I will continue to shop with you, but I want to be sure that the customer service\n",
      "representative I spoke to is aware of all of the issues with my order. I am able to confirm that your order was shipped\n",
      "within the timeframe I requested. I will be calling you to confirm that the order is still in order, but will not be\n",
      "able to confirm the situation with anyone. In the meantime, I will be contacting you to confirm to you that you received\n",
      "the order. In the meantime, I ask that you try to contact Tech Haven again to see if any issues have been resolved. I am\n",
      "extremely grateful that your customers support is as good as it's going to get.\n",
      "\n",
      "Customer support response:\n",
      "\n",
      "Dear Jamie, I received my order today and the package was shipped late. There is no way to know, and I am not aware of\n",
      "any other issues. I am able to confirm that your order was shipped within the timeframe I requested. I will be\n",
      "contacting you to confirm that the order is still in order, but will not be able to confirm the situation with anyone.\n",
      "In the meantime, I will be contacting you to confirm that they received the order.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n**Text Generation**\\n\")\n",
    "generator_pipeline = pipeline(\"text-generation\", device=device)\n",
    "print_pipeline_info(generator_pipeline)\n",
    "response = \"Dear Jamie, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator_pipeline(prompt, max_length=500)\n",
    "generated_text = outputs[0]['generated_text']\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the response includes the input prompt.  This is typical of text-generation models in HuggingFace, but it's easy to remove the input prompt from the output.  If you read the customer service response you can see that it's not very good.  GPT2, released by OpenAI in 2019, is a large transformer-based language model with 1.5 billion parameters. It was designed to generate coherent and contextually relevant text, but it can sometimes produce outputs that are not entirely accurate or appropriate.  Now there are much better text-generation models available in HuggingFace.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:49:06.524199Z",
     "iopub.status.busy": "2025-08-26T00:49:06.524005Z",
     "iopub.status.idle": "2025-08-26T00:49:07.037119Z",
     "shell.execute_reply": "2025-08-26T00:49:07.036274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(generator_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation with Other LLMs\n",
    "\n",
    "`llm_generate` makes it simple to experiment with different models for text generation.  Here's a generated customer service response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:49:07.038910Z",
     "iopub.status.busy": "2025-08-26T00:49:07.038724Z",
     "iopub.status.idle": "2025-08-26T00:51:19.198922Z",
     "shell.execute_reply": "2025-08-26T00:51:19.198113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Dear Jamie,\n",
       "\n",
       "Please accept our sincerest apologies for the deeply disappointing experience you've had with your recent order of the Samsung Galaxy S24 Ultra. We understand your frustration and disappointment, and we are truly sorry that we failed to meet your expectations, not just once, but on multiple occasions.\n",
       "\n",
       "Your feedback is incredibly important to us, and we want to assure you that it has been received and is being taken very seriously. We are actively investigating the issues you've highlighted regarding the stock notification, the incorrect item shipped, the extended delivery time, and the communication breakdown throughout your experience.\n",
       "\n",
       "We are particularly concerned to hear about the lack of proactive communication regarding the stock issue and the subsequent shipment of the wrong device. This is not the standard of service we strive to provide, and we are reviewing our internal processes to prevent such errors from happening in the future. We also regret that you felt the customer service representative you spoke with was indifferent. Our team is trained to be empathetic and supportive, and we will be addressing this feedback with the relevant individuals to ensure all customers feel heard and valued.\n",
       "\n",
       "We understand that waiting another two weeks for an exchange is unacceptable, especially after the significant inconvenience you've already endured. To rectify this situation promptly, we would like to offer you the following:\n",
       "\n",
       "1.  **Immediate Exchange & Expedited Shipping:** We will arrange for the correct Samsung Galaxy S24 Ultra to be shipped to you immediately via our fastest available shipping method, at no additional cost. We will also provide a prepaid return label for the Google Pixel 8 Pro, and we will coordinate a pickup at your convenience to minimize further disruption.\n",
       "2.  **Full Refund of Shipping Costs:** We will refund you the full amount paid for shipping on your original order, as compensation for the delays you experienced.\n",
       "3.  **A Gesture of Goodwill:** As a further apology for the significant inconvenience and frustration this has caused, we would like to offer you a [e.g., $100 store credit, a complimentary accessory for your new phone, etc. - *choose one appropriate gesture*].\n",
       "\n",
       "We will also ensure that your case is personally overseen to guarantee the swift and accurate resolution you deserve. Please reply to this email or call us directly at [Your Direct Phone Number] and ask for [Manager's Name or \"Customer Resolution Team\"] so we can arrange these steps for you immediately.\n",
       "\n",
       "Jamie, we truly value your business and are"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt_generation = \"\"\"You are an expert customer service representative. Generate a professional and empathetic response to the following customer feedback. Address the issues mentioned and provide a resolution.\"\"\"\n",
    "user_prompt_generation = f\"Customer Feedback: {text}\\n\\nCustomer service response:\"\n",
    "\n",
    "response_generation = llm_generate('gemini-flash-lite',\n",
    "                                   user_prompt_generation, \n",
    "                                   system_prompt=system_prompt_generation,\n",
    "                                   max_tokens=500)\n",
    "\n",
    "display_markdown(response_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text generation models in Lesson 11.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Summarization\n",
    "\n",
    "**Natural Language Processing (NLP) summarization** is the process of condensing a longer text into a shorter, more concise version while retaining its key information. There are two main types of summarization: **extractive** and **abstractive**. **Extractive summarization** selects and highlights the most important sentences or phrases directly from the original text without altering their wording. In contrast, **abstractive summarization** generates a new, rephrased summary that conveys the core meaning of the original content in a more natural and human-like manner. While extractive methods rely on ranking techniques, abstractive approaches often leverage deep learning models for text generation.\n",
    "\n",
    "We'll focus on abstractive summarization using HuggingFace pipelines.  Here we use a summarization model to create a summary of our customer complaint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:19.201077Z",
     "iopub.status.busy": "2025-08-26T00:51:19.200896Z",
     "iopub.status.idle": "2025-08-26T00:51:25.096793Z",
     "shell.execute_reply": "2025-08-26T00:51:25.095614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Summarization**\n",
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n",
      "[{'summary_text': ' Tech Haven sent a Samsung Galaxy S24 Ultra to Tech Haven, expecting next-day delivery . The package\n",
      "arrived a week late and contained a Google Pixel 8 Pro instead . The customer service rep was apologetic but said an\n",
      "exchange would take two weeks .'}]\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\", device=-1)\n",
    "print_pipeline_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=100, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:25.099662Z",
     "iopub.status.busy": "2025-08-26T00:51:25.099389Z",
     "iopub.status.idle": "2025-08-26T00:51:25.410301Z",
     "shell.execute_reply": "2025-08-26T00:51:25.409403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(summarization_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART (Bidirectional and Auto-Regressive Transformers) is a denoising autoencoder for pretraining sequence-to-sequence models. It combines the bidirectional context of BERT with the autoregressive nature of GPT, making it highly effective for various NLP tasks, including text generation and summarization (Don't worry, we're going to make sense of many of those terms in future lessons...). `sshleifer/distilbart-cnn-12-6` is a distilled version of the BART model, specifically fine-tuned on the CNN/DailyMail dataset for abstractive summarization tasks. This model is designed to be smaller and faster than the original BART model while retaining most of its performance, making it efficient for generating concise summaries of longer texts.`sshleifer/distilbart-cnn-12-6` is a distilled version of the BART model, specifically fine-tuned on the CNN/DailyMail dataset for abstractive summarization tasks. This model is designed to be smaller and faster than the original BART model while retaining most of its performance, making it efficient for generating concise summaries of longer texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T00:51:25.412367Z",
     "iopub.status.busy": "2025-08-26T00:51:25.412177Z",
     "iopub.status.idle": "2025-08-26T00:53:36.020751Z",
     "shell.execute_reply": "2025-08-26T00:53:36.019873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jamie received the wrong phone (Google Pixel 8 Pro instead of Samsung Galaxy S24 Ultra) a week late due to stock issues\n",
      "and poor communication from Tech Haven. The subsequent exchange process will take another two weeks, and Jamie found the\n",
      "customer service to be unhelpful and indifferent, leading to extreme disappointment and a loss of confidence in the\n",
      "company.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_summarization = \"\"\"You are an expert summarization model. Summarize the following customer feedback in a concise manner.\"\"\"\n",
    "user_prompt_summarization = f\"Customer Feedback: {text}\\n\\nSummary:\"\n",
    "\n",
    "response_summarization = llm_generate('gemini-flash-lite',\n",
    "                                      user_prompt_summarization, \n",
    "                                      system_prompt=system_prompt_summarization,\n",
    "                                      max_tokens=150)\n",
    "print(response_summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notes on Using LLMs Programatically\n",
    "\n",
    "While LLMs can make great all-purpose NLP tools, their use has some drawbacks as well:\n",
    "\n",
    "1.  They're usually configured to give **human sounding responses** which may not be what you want depending on the task. You'll often have to experiment with the system prompt to get closer to what you want.\n",
    "\n",
    "2.  **LLMs don't always generate the same output.** We'll learn more about text-generation in Lesson 11, but by default LLMs include some randomness in the generated text. You can usually configure the LLM to use lower temperature values to get more deterministic results. In `llm_generate` you can pass `temperature=0` for more consistent outputs.\n",
    "\n",
    "3.  It can be **difficult to get an LLM to format the output** in the way that you want. Carefully crafting the system prompt can help, but often some post-processing of the generated text is also necessary. Recent LLMs such as GPT-4o, Claude, and Gemini can produce output following JSON schemas through their APIs, which we'll explore in later lessons.\n",
    "\n",
    "4.  **LLMs are usually slower than a specialized model.** Especially if you're running the LLM locally. While LLMs continue to improve, often fine-tuning a specialized model is still preferable if you have enough data and resources to do so, but if you don't have much training data or just need something quick, using an LLM programatically can be beneficial.\n",
    "\n",
    "5.  **Few-shot prompting can improve the results** from an LLM. Providing one or more examples in the prompt can improve the LLM response. You'll explore this a bit in the homework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestion:** Try changing the model in the LLM cells above to see how different models perform. For example, change `'gemini-flash-lite'` to `'gpt-4o-mini'`, `'claude-haiku'`, or `'llama-3.3-70b'` in any of the `llm_generate()` calls and rerun the cell to compare results. You can see all available models with `llm_list_models()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üí∞ Current Session Spending Summary\n",
      "======================================================================\n",
      "Total Cost:          $0.000814\n",
      "Total API Calls:     9\n",
      "Total Tokens:        2,669 in / 1,369 out\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "By Model:\n",
      "  google/gemini-2.5-flash-lite\n",
      "    Cost: $0.000814 | Calls: 9 | Tokens: 2,669 in / 1,369 out\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Total Spent this session: $0.000814\n",
      "Approximate Credit remaining: $9.92\n",
      "(Note: This balance may not reflect the most recent spending)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to see cost of llms run in this session \n",
    "show_session_spending()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds776_env)",
   "language": "python",
   "name": "ds776_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a6c1986350b4284896cf5a579bb3a8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "10cc2ca6a64843409aa239e2f8f41afb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "770d1da8bddc4c7491f35d73be7a2710": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "81c7470b96b845b58123c99d7784dec4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c643400f0b0740759329f64ba59ec6cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d03d872c8a2f470dbc9b111716bfa27f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d17218daac884b819a72c55b2a5679a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d03d872c8a2f470dbc9b111716bfa27f",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_0a6c1986350b4284896cf5a579bb3a8a",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá10/10‚Äá[00:15&lt;00:00,‚Äá‚Äá1.63s/it]"
      }
     },
     "d41731cd0ee44bdbb881cfe963f96552": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81c7470b96b845b58123c99d7784dec4",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_e443047dffdb42bc9ad35d52008be058",
       "tabbable": null,
       "tooltip": null,
       "value": "Local‚ÄáGeneration:‚Äá100%"
      }
     },
     "e443047dffdb42bc9ad35d52008be058": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eb5f6b4c2d6c4f2b915d937cffbd829d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_10cc2ca6a64843409aa239e2f8f41afb",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c643400f0b0740759329f64ba59ec6cd",
       "tabbable": null,
       "tooltip": null,
       "value": 10
      }
     },
     "f65ef90340c64f5398337dc7bec9aa19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d41731cd0ee44bdbb881cfe963f96552",
        "IPY_MODEL_eb5f6b4c2d6c4f2b915d937cffbd829d",
        "IPY_MODEL_d17218daac884b819a72c55b2a5679a1"
       ],
       "layout": "IPY_MODEL_770d1da8bddc4c7491f35d73be7a2710",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
