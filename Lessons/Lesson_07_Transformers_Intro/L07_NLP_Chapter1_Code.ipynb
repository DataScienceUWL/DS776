{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to install the course package and its dependencies, add --user to install in CoCalc base environment\n",
    "# you don't need run this cell every time unless you want to upgrade the package to the latest version\n",
    "\n",
    "# from github\n",
    "# !pip install -q git+https://github.com/DataScienceUWL/DS776.git#subdirectory=introdl\n",
    "\n",
    "# from local file\n",
    "! pip install -q ../introdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "\n",
    "The code in this notebook is largely from the class textbook: “Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf (O’Reilly). Copyright 2022 Lewis Tunstall, Leandro von Werra, and Thomas Wolf, 978-1-098-13679-6.\"\n",
    "\n",
    "It is reproduced here to allow you to easily explore the code.  You can download the original notebook from their Github Repo: https://github.com/nlp-with-transformers/notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add path settings here including HF_HOME\n",
    "\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Disable all info / warning messages\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "from introdl.utils import wrap_print_text\n",
    "\n",
    "# overload the print method to print with wrapping text\n",
    "print = wrap_print_text(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tour of Transformer Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8faf5797561412786abfd14ea9daf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3da7733c1b4581b6bf4fd076ce7b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94245a88278f4494afcf326e2d65c7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cec271beb374bd9b4bce6e69bcf9880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert/distilbert-base-uncased-finetuned-sst-2-english'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6497d0d4f7184def9607a95bc383463c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a44a230811466ba041841abb4409fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa204ec61c9a4c7f917688c020e5a52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd85a391d2dd4c409b2334ba202cdbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879011</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556570</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590255</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879011         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556570           Mega    208  212\n",
       "4          PER  0.590255         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498350        ##icons    259  264\n",
       "7         MISC  0.775362       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dbmdz/bert-large-cased-finetuned-conll03-english'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09040182fe4041cdb0951934ac2b42fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e141f62a0a245ab8fe317f46784f0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58988a8fa534a5d8f1350f32b693603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a1d0b4c4be4f80a12410ab92a912fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569600625bf044ef9082809e1c931f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert/distilbert-base-cased-distilled-squad'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbc5aa46a0a4dd1a3437ebaf1aee0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4479fff64ff5441983d6ceacb47c29dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a484be188d54980b098d6657b399dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2befc96582f439dafa80e8e16604cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923a6fc19fd04fb69d464ca3b86f7dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\transformers\\generation\\utils.py:1244: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in\n",
      "Germany. Unfortunately, when I opened the package, I discovered to my horror\n",
      "that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sshleifer/distilbart-cnn-12-6'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querida Amazona, la semana pasada ordené una figura de acción Optimus Prime de\n",
      "su tienda en línea en Alemania. Desafortunadamente, cuando abrí el paquete,\n",
      "descubrí para mi horror que me habían enviado una figura de acción de Megatron\n",
      "en su lugar! Como un enemigo de toda la vida de los Decepticons, espero que\n",
      "pueda entender mi dilema. Para resolver el problema, exijo un intercambio de\n",
      "Megatron por la figura Optimus Prime que ordené. Adjunto son copias de mis\n",
      "registros relativos a esta compra. Espero escuchar de usted pronto.\n",
      "Sinceramente, Bumblebee.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the English-to-Spanish translator\n",
    "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "# Translate the text\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "\n",
    "# Print the translated text\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Helsinki-NLP/opus-mt-en-es'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42) # Set the seed to get reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22412b24385c4ae0bb40fabe176b8f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603e4b0756c1469fb134d6629d32136f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7146b2588a2e4815bdeaf443a54b7e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe9b26828c94f3c870a812ab264480d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725883cfdc11449da00c534d072465aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc034b325cfa4285a1a103c1064c145f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782da1e2e9234561bb245bc6fff76e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online\n",
      "store in Germany. Unfortunately, when I opened the package, I discovered to my\n",
      "horror that I had been sent an action figure of Megatron instead! As a lifelong\n",
      "enemy of the Decepticons, I hope you can understand my dilemma. To resolve the\n",
      "issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered.\n",
      "Enclosed are copies of my records concerning this purchase. I expect to hear\n",
      "from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. The only thing\n",
      "I can give you is information about the particular products you received with\n",
      "your request, and I will provide that information to you when I can.\n",
      "\n",
      "You are extremely welcome to come look at your package. The first thing to do is\n",
      "to bring that package over to your local store.\n",
      "\n",
      "I will note to you\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a more recent model\n",
    "\n",
    "The \"Llama\" series of text generation models are produced by Meta (Facebook).  They aren't intended to be competitive with ChatGPT or Claude.  Rather they are efficient models intended to be used by developers and researchers.  When they are fine-tuned for particular applications they can be quite good.  We'll use the [smallest Llama-3.2 model](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) in this class.  To download and use the model yourself you'll need to first sign up for a HuggingFace account (free) and then request access to the model at the link above.  Once you're granted access you can download the model directly from HuggingFace using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online\n",
      "store in Germany. Unfortunately, when I opened the package, I discovered to my\n",
      "horror that I had been sent an action figure of Megatron instead! As a lifelong\n",
      "enemy of the Decepticons, I hope you can understand my dilemma. To resolve the\n",
      "issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered.\n",
      "Enclosed are copies of my records concerning this purchase. I expect to hear\n",
      "from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. I am happy to\n",
      "help you resolve the issue. I have checked our records, and I can confirm that\n",
      "you did indeed order an Optimus Prime action figure, but unfortunately, it\n",
      "appears that there was a miscommunication with our shipping team. I am going to\n",
      "go ahead and process an exchange for the Megatron action figure. I would\n"
     ]
    }
   ],
   "source": [
    "model_original = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "generator = pipeline(\"text-generation\", model=model_original)\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, generator, max_length=200):\n",
    "    outputs = generator(prompt, max_length=max_length)\n",
    "    return outputs[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the University of Wisconsin - La Crosse, a public research university\n",
      "located in La Crosse, Wisconsin, USA.\n",
      "The University of Wisconsin - La Crosse (UW-La Crosse) is a public research\n",
      "university located in La Crosse, Wisconsin, USA. It is part of the University of\n",
      "Wisconsin System and is known for its strong programs in education, nursing, and\n",
      "the arts.\n",
      "\n",
      "Here are some key facts about the University of Wisconsin - La Crosse:\n",
      "\n",
      "* **Academic programs:** UW-La Crosse offers over 70 undergraduate majors and 15\n",
      "graduate programs, including programs in education, nursing, business, and the\n",
      "arts.\n",
      "* **Student body:** The university has a diverse student body of around 12,000\n",
      "students, with a student-faculty ratio of 14:1.\n",
      "* **Research:** UW-La Crosse is classified as a High Research Activity\n",
      "institution by the Carnegie Foundation, and offers students opportunities to\n",
      "engage in research projects with faculty mentors.\n",
      "* **Camp\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(\"Describe the University of Wisconsin - La Crosse\", generator)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? That is the question I am trying to answer as I am\n",
      "working on my new book. I am not sure I have an answer but I do know that I am\n",
      "working on a book that I hope will answer the question of what it is that we are\n",
      "working for in life. I am not sure if I will have an answer when I finish but I\n",
      "do know that I will be working on a book that will answer the question of what\n",
      "it is that I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "ModelFolder = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ModelFolder)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ModelFolder,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "prompt = \"What is the meaning of life?\"\n",
    "generated_text = pipe(prompt, max_length=100, truncation=True)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a6215a12f540b9ad7b2e4d4433a967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c74f55783f416ebfd2ca15b03fa82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26df58ee864e42f1b4ac4b10220e8be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2062914bae004df8a55f8d42fd833768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad40f2445eab4f8784fcb620a978bf3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f101a83fc11148098e6528d32e3f747c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? The question has puzzled philosophers, theologians,\n",
      "scientists, and ordinary people for centuries. In this book, we explore the\n",
      "concept of meaning in life, its significance, and its relationship to other\n",
      "aspects of human existence.\n",
      "The meaning of life is a complex and multifaceted concept that can be approached\n",
      "from various angles. Some people believe that the meaning of life is to seek\n",
      "happiness, fulfillment, and personal growth. Others argue that it is to serve a\n",
      "higher power\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "ModelFolder = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ModelFolder)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ModelFolder,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "prompt = \"What is the meaning of life?\"\n",
    "generated_text = pipe(prompt, max_length=100, truncation=True)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"What is the meaning of life? A question that has puzzled\n",
      "philosophers, theologians, and everyday people for centuries. In this post,\n",
      "we'll explore some of the most influential theories on the meaning of life, from\n",
      "the ancient Greeks to modern thought leaders.\\n**1. The Hedonistic Theory\n",
      "(Ancient Greece)**\\nEpicurus (341-270 BCE) believed that the meaning of life is\n",
      "to seek happiness and avoid physical pain. He advocated for a life of\n",
      "moderation, friendship,\"}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the meaning of life?\"\n",
    "generated_text = pipe(prompt, max_length=100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fd7daf5d6047da9eecae3839a299f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\models--unsloth--Llama-3.2-3B-Instruct-bnb-4bit. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f30abca8c84fe787a07b53806a577a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491c3ca5536b4c42aecef9560b6185fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bd900fb6da49d7a451d54ecd3daa38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930406d2af104f8aa36b2e6e60a662a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9f527122e640a88a9247664aba84a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? A question that has puzzled philosophers,\n",
      "theologians, scientists, and everyday people for centuries. While there may not\n",
      "be a definitive answer, here are some perspectives on the meaning of life:\n",
      "\n",
      "**Philosophical Perspectives:**\n",
      "\n",
      "1. **Hedonism**: The pursuit of happiness and pleasure is the meaning of life.\n",
      "2. **Eudaimonism**: Living a virtuous and fulfilling life, marked by happiness,\n",
      "self-actualization, and personal\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "ModelFolder = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ModelFolder)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ModelFolder,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "prompt = \"What is the meaning of life?\"\n",
    "generated_text = pipe(prompt, max_length=100, truncation=True)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a pytorch model for classifying MNIST digits\n",
      "==========================================================\n",
      "\n",
      "Below is a simple example of a PyTorch model designed to classify MNIST digits.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.fc1 = nn.Linear(784, 128) # input layer (28x28 images) -> hidden\n",
      "layer (128 units)\n",
      "        self.fc2 = nn.Linear(128, 64) # hidden layer (128 units) -> hidden layer\n",
      "(64 units)\n",
      "        self.fc3 = nn.Linear(64, 10) # hidden layer (64 units) -> output layer\n",
      "(10 units)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = F.relu(self.fc1(x)) # activation function for hidden layer\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "\n",
      "# Initialize the model, loss function and optimizer\n",
      "model = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
      "\n",
      "# Train the model\n",
      "for epoch in range(10):\n",
      "    for x, y in train_loader: # assuming a train_loader object is available\n",
      "        x = x.view(-1, 784) # flatten the input\n",
      "        y = y.view(-1, 10) # flatten the target\n",
      "        optimizer.zero_grad()\n",
      "        output = model(x)\n",
      "        loss = criterion(output, y)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
      "```\n",
      "\n",
      "In the above code:\n",
      "\n",
      "*   We define a simple neural network with two hidden layers using PyTorch's\n",
      "`nn.Module` and `nn.Linear` classes.\n",
      "*   The `forward` method defines the forward pass through the network, where we\n",
      "apply the ReLU activation function to the hidden layers.\n",
      "*   We initialize the model, loss function, and optimizer using the\n",
      "`CrossEntropyLoss` function and the Stochastic Gradient Descent optimizer with a\n",
      "learning rate of 0.01 and a momentum of 0.5.\n",
      "*   We train the model for 10 epochs, where each epoch consists of iterating\n",
      "over the training data and updating the model's parameters using\n",
      "backpropagation.\n",
      "\n",
      "**Note:** The above code\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Generate a pytorch model for classifying MNIST digits\"\n",
    "generated_text = pipe(prompt, max_length=500, truncation=True)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoModelForTokenClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "def main():\n",
    "    # Sample Text\n",
    "    text = (\n",
    "        \"Hello TechEmporium, I recently purchased a set of noise-canceling headphones \"\n",
    "        \"from your online store in Canada. Upon receiving the item, I noticed a defect: \"\n",
    "        \"the left earcup doesn’t produce sound. As someone who relies on quality audio \"\n",
    "        \"for my work, this is a major inconvenience. I would appreciate a replacement or \"\n",
    "        \"a refund. Attached are the order details and proof of purchase. Please let me \"\n",
    "        \"know how we can resolve this issue promptly. Best regards, Alex.\"\n",
    "    )\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    print(\"\\n**Sentiment Analysis**\")\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "    sentiment_result = sentiment_analyzer(text)\n",
    "    print(sentiment_result)\n",
    "\n",
    "    # Named Entity Recognition (NER)\n",
    "    print(\"\\n**Named Entity Recognition**\")\n",
    "    ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "    ner_result = ner_pipeline(text)\n",
    "    print(ner_result)\n",
    "\n",
    "    # Question Answering\n",
    "    print(\"\\n**Question Answering**\")\n",
    "    qa_pipeline = pipeline(\"question-answering\")\n",
    "    question = \"What is the defect?\"\n",
    "    qa_result = qa_pipeline(question=question, context=text)\n",
    "    print(qa_result)\n",
    "\n",
    "    # Translation (English to French)\n",
    "    print(\"\\n**Translation**\")\n",
    "    translation_pipeline = pipeline(\"translation_en_to_fr\")\n",
    "    translation_result = translation_pipeline(text, max_length=200)\n",
    "    print(translation_result)\n",
    "\n",
    "    # Summarization\n",
    "    print(\"\\n**Summarization**\")\n",
    "    summarization_pipeline = pipeline(\"summarization\")\n",
    "    summarization_result = summarization_pipeline(text, max_length=50, min_length=25, do_sample=False)\n",
    "    print(summarization_result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "ChatCompletionMessage(content='The sentiment of the text is predominantly negative. The author, Alex, expresses dissatisfaction with a defective product (the noise-canceling headphones) and articulates a sense of inconvenience due to the issue, which impacts their work quality. While the request for a replacement or refund is presented in a polite manner, the overall tone reflects frustration and disappointment.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n",
      "\n",
      "**Named Entity Recognition**\n",
      "ChatCompletionMessage(content='Here are the identified named entities from the text, categorized accordingly:\\n\\n1. **Organizations**\\n   - TechEmporium\\n\\n2. **Locations**\\n   - Canada\\n\\n3. **Person**\\n   - Alex\\n\\n4. **Products**\\n   - Noise-canceling headphones\\n\\n5. **Miscellaneous**\\n   - Order details (non-specific but refers to associated documentation)\\n   - Proof of purchase (non-specific but refers to associated documentation) \\n\\nThis categorization includes the main entities relevant to the text provided.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n",
      "\n",
      "**Question Answering**\n",
      "ChatCompletionMessage(content='The defect is that the left earcup of the noise-canceling headphones does not produce sound.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n",
      "\n",
      "**Translation**\n",
      "ChatCompletionMessage(content=\"Bonjour TechEmporium, \\n\\nJ'ai récemment acheté un ensemble de casques antibruit dans votre boutique en ligne au Canada. À la réception de l'article, j'ai remarqué un défaut : l'oreillette gauche ne produit aucun son. En tant que personne qui dépend d'un audio de qualité pour mon travail, cela constitue un gros inconvénient. J'aimerais recevoir un remplacement ou un remboursement. Vous trouverez ci-joint les détails de la commande et la preuve d'achat. Merci de me faire savoir comment nous pouvons résoudre ce problème rapidement.\\n\\nCordialement,  \\nAlex.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n",
      "\n",
      "**Summarization**\n",
      "ChatCompletionMessage(content='Alex purchased noise-canceling headphones from TechEmporium but found a defect in the left earcup. This issue impacts his work quality, prompting him to request a replacement or refund, along with order details and proof of purchase for resolution.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load OpenAI API Key\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "text = (\n",
    "    \"Hello TechEmporium, I recently purchased a set of noise-canceling headphones \"\n",
    "    \"from your online store in Canada. Upon receiving the item, I noticed a defect: \"\n",
    "    \"the left earcup doesn’t produce sound. As someone who relies on quality audio \"\n",
    "    \"for my work, this is a major inconvenience. I would appreciate a replacement or \"\n",
    "    \"a refund. Attached are the order details and proof of purchase. Please let me \"\n",
    "    \"know how we can resolve this issue promptly. Best regards, Alex.\"\n",
    ")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def chatgpt_prompt(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"You are an assistant that performs various NLP tasks.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_prompt = f\"Perform sentiment analysis on the following text: \\n{text}\"\n",
    "sentiment_result = chatgpt_prompt(sentiment_prompt)\n",
    "print(sentiment_result)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\")\n",
    "ner_prompt = f\"Identify and categorize the named entities in the following text: \\n{text}\"\n",
    "ner_result = chatgpt_prompt(ner_prompt)\n",
    "print(ner_result)\n",
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\")\n",
    "question = \"What is the defect?\"\n",
    "qa_prompt = f\"Based on the following text, answer the question: \\nText: {text}\\nQuestion: {question}\"\n",
    "qa_result = chatgpt_prompt(qa_prompt)\n",
    "print(qa_result)\n",
    "\n",
    "# Translation (English to French)\n",
    "print(\"\\n**Translation**\")\n",
    "translation_prompt = f\"Translate the following text to Spanish: \\n{text}\"\n",
    "translation_result = chatgpt_prompt(translation_prompt)\n",
    "print(translation_result)\n",
    "\n",
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_prompt = f\"Summarize the following text in 25-50 words: \\n{text}\"\n",
    "summarization_result = chatgpt_prompt(summarization_prompt)\n",
    "print(summarization_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Globals to preserve model configurations across calls\n",
    "_loaded_models = {\n",
    "    \"llama\": None,\n",
    "    \"chatgpt\": None,\n",
    "}\n",
    "\n",
    "# Supported models\n",
    "supported_models = {\n",
    "    \"gpt-4o\": \"gpt-4o\",\n",
    "    \"gpt-4o-mini\": \"gpt-4o-mini\",\n",
    "    \"llama-3.2-3b\": \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "    \"llama-3.2-1b\": \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"llama-3.1-8b\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "}\n",
    "\n",
    "def generate_text(prompt, model=\"llama-3.2-3b\", system_prompt=\"You are an assistant that performs various NLP tasks.\", max_length=100):\n",
    "    \"\"\"\n",
    "    Generate text using either a LLaMA model or OpenAI's GPT models.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The input prompt for the model to generate text.\n",
    "        model (str): The model to use for text generation. Available options are:\n",
    "            - \"gpt-4o\": OpenAI GPT-4o model\n",
    "            - \"gpt-4o-mini\": OpenAI GPT-4o-mini model\n",
    "            - \"llama-3.2-3b\": LLaMA 3.2 3B model (\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\")\n",
    "            - \"llama-3.2-1b\": LLaMA 3.2 1B model (\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\")\n",
    "            - \"llama-3.1-8b\": LLaMA 3.1 8B model (\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")\n",
    "        system_prompt (str): An optional system prompt to guide the behavior of the model.\n",
    "        max_length (int): The maximum length of the generated text. For GPT models, this is enforced via \"max_tokens\".\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text from the model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unsupported model is specified or if the OpenAI API key is not set when using GPT models.\n",
    "    \"\"\"\n",
    "    global _loaded_models\n",
    "\n",
    "    if model.startswith(\"llama\"):\n",
    "        llama_model_folder = supported_models.get(model)\n",
    "        if not llama_model_folder:\n",
    "            raise ValueError(f\"Unsupported LLaMA model: {model}\")\n",
    "\n",
    "        # Reload if the model has changed\n",
    "        if _loaded_models[\"llama\"] is None or _loaded_models[\"llama\"][\"model_name\"] != model:\n",
    "            # Load and configure the LLaMA model\n",
    "            tokenizer = AutoTokenizer.from_pretrained(llama_model_folder)\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=llama_model_folder,\n",
    "                tokenizer=tokenizer,\n",
    "                device_map=\"auto\",\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            _loaded_models[\"llama\"] = {\"pipeline\": pipe, \"model_name\": model}\n",
    "\n",
    "        pipe = _loaded_models[\"llama\"][\"pipeline\"]\n",
    "\n",
    "        # Prepend system prompt to LLaMA prompt\n",
    "        if system_prompt:\n",
    "            prompt = f\"{system_prompt}\\n\\n{prompt}\"\n",
    "        output = pipe(prompt, max_length=max_length, truncation=True)\n",
    "        generated_text = output[0][\"generated_text\"]\n",
    "        # Remove the initial prompt from the output\n",
    "        return generated_text[len(prompt):].strip()\n",
    "\n",
    "    elif model.startswith(\"gpt-4o\"):\n",
    "        openai_model = supported_models.get(model)\n",
    "        if not openai_model:\n",
    "            raise ValueError(f\"Unsupported ChatGPT model: {model}\")\n",
    "\n",
    "        # Reload if the model has changed\n",
    "        if _loaded_models[\"chatgpt\"] is None or _loaded_models[\"chatgpt\"][\"model_name\"] != model:\n",
    "            # Load OpenAI API key from environment variable\n",
    "            openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not openai.api_key:\n",
    "                raise ValueError(\"OpenAI API key not set. Ensure the 'OPENAI_API_KEY' environment variable is configured.\")\n",
    "\n",
    "            # Initialize OpenAI client\n",
    "            client = openai.OpenAI()\n",
    "            _loaded_models[\"chatgpt\"] = {\"client\": client, \"model_name\": model}\n",
    "\n",
    "        client = _loaded_models[\"chatgpt\"][\"client\"]\n",
    "\n",
    "        # Ensure system_prompt is not None\n",
    "        if system_prompt is None:\n",
    "            raise ValueError(\"system_prompt must not be None for GPT models.\")\n",
    "\n",
    "        # Generate text with ChatGPT\n",
    "        completion = client.chat.completions.create(\n",
    "            model=openai_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=max_length  # Enforce max_length\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The University of Wisconsin - La Crosse (UW-La Crosse) is a public university\n",
      "located in La Crosse, Wisconsin. It is part of the University of Wisconsin\n",
      "System and is known for its strong emphasis on health sciences, education, and\n",
      "liberal arts. Established in 1909, UW-La Crosse has developed a reputation for\n",
      "quality education and has a vibrant campus life.\n",
      "\n",
      "Key features of UW-La Crosse include:\n",
      "\n",
      "1. **Academic Programs**: The university offers a range of undergraduate and\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(\"Describe the University of Wisconsin - La Crosse\", model=\"gpt-4o-mini\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The University of Wisconsin - La Crosse (UW-La Crosse or UWL) is a public university located in La Crosse, Wisconsin. It is part of the University of Wisconsin System and was founded in 1909. The university is known for its focus on health sciences, education, and comprehensive programs in liberal arts and sciences.\\n\\n### Key Features\\n\\n1. **Academic Programs**: UWL offers a variety of undergraduate and graduate programs across several colleges, including the College of Science and Health'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
