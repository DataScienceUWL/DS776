{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR IMPORTS HERE ===\n",
    "# Add any additional imports you need below this line\n",
    "\n",
    "from introdl import (\n",
    "    config_paths_keys\n",
    ")\n",
    "\n",
    "# Wrap print to format text nicely at 120 characters\n",
    "print = wrap_print_text(print, width=120)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Configure paths\n",
    "paths = config_paths_keys()\n",
    "DATA_PATH = paths['DATA_PATH']\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "# === END YOUR IMPORTS ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8:  Sarcasm Detection\n",
    "\n",
    "Using the [\"Sarcasm_News_Headline\" dataset](https://huggingface.co/datasets/raquiba/Sarcasm_News_Headline) on HuggingFace you're going to try several approaches to sarcasm detection (text classification) and write a summary at the end.\n",
    "\n",
    "**Total Points: 50**\n",
    "- Reading Questions: 8 points\n",
    "- Download and split dataset: 2 points\n",
    "- Approach 1 (TF-IDF + ML Model): 7 points\n",
    "- Approach 2 (Fine-tune DistilBERT): 7 points\n",
    "- Approach 3 Part 1 (LLM Zero-Shot): 7 points\n",
    "- Approach 3 Part 2 (LLM Few-Shot): 7 points\n",
    "- Summarize and Compare: 6 points\n",
    "- Reflection: 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Questions (8 points)\n",
    "\n",
    "Answer the following questions based on Chapter 2: Text Classification from *Natural Language Processing with Transformers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (2 points):** What are the three main advantages of DistilBERT over the original BERT model? How does DistilBERT achieve these improvements while maintaining most of BERT's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (2 points):** Compare the three main tokenization strategies discussed in the chapter: character tokenization, word tokenization, and subword tokenization (WordPiece). What are the key advantages and disadvantages of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (2 points):** Explain the difference between the feature extraction and fine-tuning approaches for transfer learning. When would you choose feature extraction over fine-tuning, and what are the trade-offs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (2 points):** The chapter demonstrates using loss-based sorting as an error analysis technique to identify mislabeled examples and difficult cases. Explain how this technique works and why sorting by prediction loss is effective for finding problematic examples in your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and split the dataset (2 points)\n",
    "\n",
    "While the dataset has \"train\" and \"test\" splits.  Ignore the \"test\" split since it almost entirely duplicates the \"train\" split.  \n",
    "\n",
    "Instead, use train_test_split with a seed of 42 to generate an 80/20 split of the original \"train\" split into training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 1 - TF-IDF Vectors + ML Model (7 points)\n",
    "\n",
    "Include code to create TF-IDF Vectors that represent each headline.  Use these vectors to train a classification model (it doesn't have to be Logistic Regression).  Make predictions on the test set and generate a classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 2 - Fine-tune DistilBERT with Classification Head (7 points)\n",
    "\n",
    "Include code for set up, training, and classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 3 - Part 1: Use an LLM Model and Zero-Shot Prompt (7 points)\n",
    "\n",
    "Using the `llm_classifier` helper function from the lesson apply your LLM classifier to the first 100 examples in the test set. Use a local model and an API-based model for comparison.  For the API-based model some possiblities include:\n",
    "* Groq: \"llama3-70b-8192\", (rate_limit = 30 requests per minute on free tier)\n",
    "* Together.AI: \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\" (rate limit = 10)\n",
    "* Gemini: \"gemini-flash-lite\" (rate_limit = 30 on free tier)\n",
    "\n",
    "Feel free to try others if you have access.\n",
    "\n",
    "Produce classification reports for both the local and best API-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lesson_08_Helpers import llm_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Approach 3 - Part 2: Use an LLM Model and Few-Shot Prompt (7 points)\n",
    "\n",
    "Build a few-shot prompt with three to five examples of each class and apply the same models used for the zero-shot prompt.  Produce classification reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2 pts] Reflection\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for the lesson? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
