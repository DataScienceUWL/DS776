{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "# Homework 06 Assignment\n**Name:** [Student Name Here]  \n**Total Points:** 40\n\n## Submission Checklist\n- [ ] All code cells executed with output saved\n- [ ] All questions answered\n- [ ] Notebook converted to HTML (use the Homework_06_Utilities notebook)\n- [ ] Canvas notebook filename includes `_GRADE_THIS_ONE`\n- [ ] Files uploaded to Canvas\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "# Computer Vision: Segmentation and Object Detection\n",
    "\n",
    "For this assignment there are two primary tasks exploring advanced computer vision applications:\n",
    "\n",
    "1. **UNet and UNet++ for Nuclei Segmentation**: Explore semantic segmentation using UNet architectures on a biomedical imaging task described in the textbook.\n",
    "2. **YOLO v11 for Pedestrian Detection**: Fine-tune a state-of-the-art YOLO model for object detection and compare results to the Faster R-CNN model from the lesson.\n",
    "\n",
    "Both tasks will help you understand the differences between segmentation (pixel-level classification) and detection (bounding box prediction) approaches in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# YOUR IMPORTS HERE\n# Add any additional imports you need below this line\n\nimport torch\nimport torchvision.transforms.v2 as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_image\nfrom torchvision import tv_tensors\nfrom pathlib import Path\n\n# Import local modules\nfrom Lesson_06_Helpers import display_yolo_predictions, prepare_penn_fudan_yolo\n\nfrom introdl.utils import config_paths_keys\n\n# Configure paths\npaths = config_paths_keys()\nDATA_PATH = paths['DATA_PATH']\nMODELS_PATH = paths['MODELS_PATH']"
  },
  {
   "cell_type": "markdown",
   "id": "j1f7ulh8c9n",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_06_Models/` | `Homework_06_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` ‚Äî never hardcode paths like `'Homework_06_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Part 1 - UNet and UNet++ Segmentation (20 pts)\n\nYou're going to use the segmentation models pytorch package as we did in the lesson to fine-tune and evaluate UNet and UNet++ models on the nuclei segmentation task shown in the textbook.\n\nWe've already prepared the data downloading process for you. The following cells contain most of a custom dataset class and transforms to get you started. You'll need to complete the code sections marked with `# === YOUR CODE HERE ===` to read images and masks, add appropriate augmentation transforms, and implement the model training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once to download the Nuclei Segmentation dataset\n",
    "\n",
    "from Lesson_06_Helpers import download_and_extract_nuclei_data\n",
    "\n",
    "# Call the function\n",
    "download_and_extract_nuclei_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Complete the NucleiDataset class and data loading setup\n# - Complete the dataset class by filling in the marked sections:\n#   * Read image from image_path, convert to float and scale to [0,1]\n#   * Read mask from mask_path, map values >0 to 1, rest to 0, convert to float\n# - Add appropriate augmentation transforms for training:\n#   * Consider: RandomHorizontalFlip, RandomVerticalFlip, RandomRotation\n#   * Use transforms that work with both images and masks simultaneously\n# - Create training and validation datasets\n# - Create DataLoaders with batch_size=8\n\nclass NucleiDataset(Dataset):\n    def __init__(self, root, transform=None):\n        \"\"\"\n        Args:\n            root (str or Path): Path to the dataset (train or val folder).\n            transform (callable, optional): Optional transforms to apply to both image and mask.\n        \"\"\"\n        self.root = Path(root)  # Convert to pathlib Path object\n        self.transform = transform\n        self.data = []  # List to store (image_tensor, mask_tensor) tuples\n\n        # Load all image and mask files\n        all_imgs = sorted((self.root / \"images\").iterdir())\n        all_masks = sorted((self.root / \"masks\").iterdir())\n\n        # Ensure that the number of images and masks are the same\n        assert len(all_imgs) == len(all_masks), \"The number of images and masks must be the same\"        \n\n        # Read and store images and masks as tensors in memory\n        for img_path, mask_path in zip(all_imgs, all_masks):\n            # YOUR CODE HERE: Read images and masks as tensors\n            # Read image from image_path, convert to float and scale to [0,1]\n            image = # TODO: Complete this line\n            \n            # Read mask from mask_path, any entries bigger than 0 map to 1, rest to 0, convert to float\n            mask = # TODO: Complete this line\n\n            # Store as tuple\n            self.data.append((tv_tensors.Image(image), tv_tensors.Mask(mask)))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image, mask = self.data[idx]\n\n        # Apply transforms if provided\n        if self.transform:\n            image, mask = self.transform(image, mask)\n\n        return image, mask\n\n# YOUR CODE HERE: Define transforms and create datasets\n# Add your augmentation transforms here for training\ntrain_transforms = transforms.Compose([\n    # TODO: Add appropriate augmentation transforms\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Define transforms for validation (without augmentation)\nval_transforms = transforms.Compose([\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# TODO: Load datasets and create dataloaders\n# train_dataset = ...\n# val_dataset = ...\n# train_loader = ...\n# val_loader = ..."
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "Now setup and train UNet and UNet++ models with a pretrained ResNet50 backbone as we did in the lesson. Model your code on the code in the \"Better Training\" part of the notebook. You should set different learning rates for the encoder and decoder and use OneCycleLR as we did. We found that 12 epochs of fine-tuning worked reasonably well.\n",
    "\n",
    "For each model display convergence graphs of the loss and IoU and sample images along with the ground truth and predicted masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Setup and train UNet and UNet++ models\n# - Import segmentation_models_pytorch as smp\n# - Create UNet model with ResNet50 encoder, pretrained weights\n# - Create UNet++ model with ResNet50 encoder, pretrained weights\n# - Set different learning rates for encoder (lower) and decoder (higher)\n# - Use OneCycleLR scheduler\n# - Train each model for 12 epochs\n# - Track loss and IoU metrics\n# - Save model checkpoints"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "üìù **Answer the following followup questions:**\n",
    "\n",
    "1. Which model performs better? Support your answer with specific metrics.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. Use AI to write a short summary of the difference between UNet and UNet++.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "3. Report the highest value of the IoU metric on the validation set. Interpret that value in the context of this problem. What is it telling you about the predicted masks for the cell nuclei?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## Part 2 - YOLO v11 Pedestrian Detection (18 pts)\n\nYOLO (You Only Look Once) models are a family of object detection models known for their speed and accuracy. Unlike traditional object detection methods that use a sliding window approach, YOLO models frame object detection as a single regression problem, directly predicting bounding boxes and class probabilities from full images in one evaluation. This makes YOLO models extremely fast, making them suitable for real-time applications.\n\nYOLO models consist of a single convolutional network that simultaneously predicts multiple bounding boxes and class probabilities for those boxes. The architecture is divided into several key components:\n\n1. **Backbone**: This is typically a convolutional neural network (CNN) that extracts essential features from the input image.\n2. **Neck**: This part of the network aggregates and combines features from different stages of the backbone. It often includes components like Feature Pyramid Networks (FPN) or Path Aggregation Networks (PAN).\n3. **Head**: The final part of the network, which predicts the bounding boxes, objectness scores, and class probabilities. It usually consists of convolutional layers that output the final detection results.\n\nYOLO models are quite easy to load and train because they provide pre-trained weights and a straightforward API for customization and fine-tuning. The hardest part may be preparing the data in the format that the API expects, but we've done that for you.\n\nRun the cell below once to prepare the Penn Fudan Pedestrian dataset in YOLO format. This dataset uses the same splits we used in the lesson to allow you to compare the results to the Faster R-CNN model we trained there."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run this once per platform, but it's safe to run multiple times\n",
    "prepare_penn_fudan_yolo(DATA_PATH)\n",
    "\n",
    "# the dataset will be here:\n",
    "dataset_path = DATA_PATH / \"PennFudanPedYOLO\"\n",
    "\n",
    "# you may wish to set an output path for the model\n",
    "output_path = MODELS_PATH / \"PennFudanPedYOLO\"\n",
    "\n",
    "# the YAML file for the dataset is here:\n",
    "yaml_path = dataset_path / \"dataset.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "Visit the ultralytics website to learn about YOLO11. You can watch a short video to learn more about it. Below, implement code to load and train a YOLO11 model using the 'yolo11s.pt' pretrained weights. Pass `project=output_path` to `model.train()` to store the output in your models directory. After training you might want to look at some of the images created in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Train YOLO11 model\n# - Import YOLO from ultralytics\n# - Load YOLO11 model with 'yolo11s.pt' pretrained weights\n# - Train the model on the Penn Fudan dataset\n# - Use dataset=yaml_path to specify the dataset\n# - Use project=output_path to save results in your models directory\n# - Train - try 10 epochs to make sure it works, >=100 for good results\n# - Monitor training progress and metrics"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "You can run the following cell to show selected images and boxes from the validation set. You can replace `indices=selected_indices` with `num_samples=3` to display 3 randomly selected images. The selected images we chose should align with the images we showed in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = [28,29,33]\n",
    "display_yolo_predictions(yaml_path, model, indices=selected_indices, show_confidence=True, conf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "üìù **Answer the following followup questions:**\n",
    "\n",
    "1. Find and plot an image with a false positive box in the validation data.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. How is the process of fine-tuning the YOLO model different than for the Faster R-CNN model in the lesson? Is it easier or harder? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "3. What did you get for mAP50 and mAP50:95 on the validation data with your YOLO model?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "4. How do those values compare to values in the lesson?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "5. How do the predicted boxes compare qualitatively to the boxes predicted by Faster R-CNN in the lesson? Do they align better or worse with the ground truth boxes?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "6. Thoroughly explain what your mAP50 value tells you about the performance of your YOLO model at detecting pedestrians in the validation data.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "## Part 3 - Reflection (2 pts)\n\n1. What, if anything, did you find difficult to understand for this lesson? Why?\n\nüìù **YOUR ANSWER HERE:**\n\n2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n\nüìù **YOUR ANSWER HERE:**"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Cleanup Note\n",
    "\n",
    "**Note:** YOLO downloads pretrained model files (like `yolo11s.pt` or `yolo11n.pt`) to your current directory on first use. These files are safe to delete after training is complete if you want to save space - they will be re-downloaded if needed in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}