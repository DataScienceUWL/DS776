{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Homework 11: Text Generation and Decoding Strategies\n",
    "\n",
    "**Total Points: 50**\n",
    "- Reading Questions: 10 points\n",
    "- Part 1 (Decoding Strategies Comparison): 10 points\n",
    "- Part 2 (Building API Helper Functions): 8 points\n",
    "- Part 3 (Model Size Comparison): 8 points\n",
    "- Part 4 (Creative Text Generation): 8 points\n",
    "- Part 5 (Analysis and Comparison): 4 points\n",
    "- Part 6 (Reflection): 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Reading Questions (10 points)\n",
    "\n",
    "Answer the following questions based on Chapter 5: Text Generation from *Natural Language Processing with Transformers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "**Question 1 (2 points):** Explain how autoregressive (causal) language models like GPT-2 generate text. What is conditional text generation, and how does the chain rule of probability factor into the text generation process? Be specific about how the model predicts each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "**Question 2 (2 points):** Why do we use log probabilities instead of regular probabilities when scoring sequences in text generation? Explain the numerical stability problem that arises with regular probabilities and how log probabilities solve it. Include a brief discussion of the mathematical transformation involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "**Question 3 (2 points):** Compare and contrast greedy search decoding with beam search decoding. What are the advantages of beam search over greedy search? What problem do both methods share, and how can the `no_repeat_ngram_size` parameter help address it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "**Question 4 (2 points):** Explain the role of the temperature parameter in sampling-based text generation. How does temperature affect the probability distribution over tokens? What happens when temperature is very low (T << 1) versus very high (T >> 1), and what are the trade-offs in terms of text quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "**Question 5 (2 points):** Describe top-k and nucleus (top-p) sampling methods. How do they differ in how they restrict the vocabulary for sampling? According to the textbook, which decoding methods should you use for tasks that require factual correctness (like summarization) versus tasks that benefit from creativity (like story generation)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Part 1 - Decoding Strategies Comparison (10 points)\n",
    "\n",
    "In this part, you'll experiment with different decoding strategies to see how they affect generated text quality and diversity.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load a text generation model (e.g., GPT-2 or a Llama model like \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\")\n",
    "2. Create an interesting prompt for text generation (e.g., a story opening, a technical explanation, or a dialogue start)\n",
    "3. Generate text using at least 5 different decoding configurations:\n",
    "   - Greedy search (do_sample=False)\n",
    "   - Beam search with num_beams=5\n",
    "   - Beam search with no_repeat_ngram_size=2\n",
    "   - Sampling with temperature=0.5\n",
    "   - Sampling with temperature=1.5\n",
    "   - Top-k sampling (top_k=50)\n",
    "   - Nucleus sampling (top_p=0.9)\n",
    "   \n",
    "4. For each generated text:\n",
    "   - Display the full generated text\n",
    "   - Calculate and display the sequence log probability\n",
    "   - Comment on the coherence, diversity, and quality\n",
    "   \n",
    "5. Write a summary (3-4 paragraphs) comparing the different decoding strategies. Which produced the most coherent text? Which was most creative? Which would you choose for different use cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Define functions for log probability calculation (similar to textbook examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Experiment with different decoding strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR COMPARISON SUMMARY HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Extending the Conversation Class with Decoding Parameters (8 points)\n",
    "\n",
    "In Lesson 11, Section 5.5, you learned about the `Conversation` class for building chatbots with APIs. In Section 7.5, you learned about decoding parameters (temperature, top_p, max_tokens) that control text generation behavior.\n",
    "\n",
    "In this part, you'll combine these concepts by extending the `Conversation` class to support decoding parameter control, allowing you to fine-tune the chatbot's behavior for different use cases.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Practice working with chat roles (system/user/assistant) and conversation history\n",
    "- Understand how decoding parameters affect model behavior\n",
    "- Apply object-oriented design patterns to build maintainable APIs\n",
    "- Test different parameter combinations for various tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Copy and Extend the Conversation Class (3 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Copy the `Conversation` class from Lesson 11, Section 5.5 (you'll find it in the \"Building a Chatbot Class\" section)\n",
    "\n",
    "2. Extend the class to support decoding parameters:\n",
    "   - Add `temperature`, `top_p`, and `max_tokens` parameters to `__init__()` with these defaults:\n",
    "     - `temperature=0.7`\n",
    "     - `top_p=0.9`\n",
    "     - `max_tokens=256`\n",
    "   - Modify the `send()` method to pass these parameters to the API call\n",
    "   - Allow per-message parameter overrides in `send()` (e.g., `send(message, temperature=0.0)`)\n",
    "\n",
    "3. Your extended class should:\n",
    "   - Store default parameters as instance variables\n",
    "   - Use defaults if no overrides provided\n",
    "   - Pass all three parameters to `client.chat.completions.create()`\n",
    "\n",
    "**Hints:**\n",
    "- The original `Conversation` class already handles the OpenAI client and message history\n",
    "- You just need to add parameter handling to `__init__()` and `send()`\n",
    "- Use `if parameter is not None` to check for overrides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Copy and extend the Conversation class\n",
    "class Conversation:\n",
    "    \"\"\"\n",
    "    Extended chatbot class with decoding parameter control.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): OpenRouter API key\n",
    "        model (str): Model name (e.g., 'google/gemini-2.5-flash-lite')\n",
    "        system_prompt (str, optional): Initial instructions for the model\n",
    "        temperature (float): Sampling temperature (default: 0.7)\n",
    "        top_p (float): Nucleus sampling parameter (default: 0.9)\n",
    "        max_tokens (int): Maximum tokens to generate (default: 256)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key, model=\"google/gemini-2.5-flash-lite\",\n",
    "                 system_prompt=None, temperature=0.7, top_p=0.9, max_tokens=256):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def send(self, user_message, temperature=None, top_p=None, max_tokens=None):\n",
    "        \"\"\"\n",
    "        Send a message and get a response.\n",
    "\n",
    "        Args:\n",
    "            user_message (str): The user's input\n",
    "            temperature (float, optional): Override default temperature\n",
    "            top_p (float, optional): Override default top_p\n",
    "            max_tokens (int, optional): Override default max_tokens\n",
    "\n",
    "        Returns:\n",
    "            str: The assistant's response\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def display(self, show_recent_only=True):\n",
    "        \"\"\"Show conversation history (you can use a simple print for now)\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def reset(self, system_prompt=None):\n",
    "        \"\"\"Reset conversation history\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "print(\"\u2713 Extended Conversation class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Test Different Parameter Recipes (3 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Create three different chatbot configurations optimized for different use cases:\n",
    "\n",
    "**Recipe 1: Factual Question Answering**\n",
    "- `temperature=0` (deterministic/greedy)\n",
    "- `top_p=1.0` (not used when temperature=0)\n",
    "- `max_tokens=150`\n",
    "- System prompt: \"You are a knowledgeable assistant providing accurate, factual information.\"\n",
    "- Test with a factual question (e.g., \"What is photosynthesis?\")\n",
    "\n",
    "**Recipe 2: Creative Writing**\n",
    "- `temperature=1.2` (more random)\n",
    "- `top_p=0.95` (wide vocabulary)\n",
    "- `max_tokens=300`\n",
    "- System prompt: \"You are a creative storyteller with a vivid imagination.\"\n",
    "- Test with a creative prompt (e.g., \"Start a science fiction story about AI\")\n",
    "\n",
    "**Recipe 3: General Conversation**\n",
    "- `temperature=0.7` (balanced)\n",
    "- `top_p=0.9` (nucleus sampling)\n",
    "- `max_tokens=200`\n",
    "- System prompt: \"You are a friendly, helpful assistant.\"\n",
    "- Test with a conversational prompt (e.g., \"Explain neural networks to a beginner\")\n",
    "\n",
    "For each recipe:\n",
    "1. Create a new `Conversation` instance with the specified parameters\n",
    "2. Send at least 2 messages to create a multi-turn conversation\n",
    "3. Display the conversation (or print the messages)\n",
    "4. Note the characteristics of the responses (consistency, creativity, length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Recipe 1: Factual Question Answering\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RECIPE 1: FACTUAL QUESTION ANSWERING\")\n",
    "print(\"(temperature=0, top_p=1.0, max_tokens=150)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create chatbot\n",
    "factual_chat = Conversation(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    model=\"google/gemini-2.5-flash-lite\",\n",
    "    system_prompt=\"You are a knowledgeable assistant providing accurate, factual information.\",\n",
    "    temperature=0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Test with multi-turn conversation\n",
    "# YOUR CODE HERE - send at least 2 messages and display responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Recipe 2: Creative Writing\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECIPE 2: CREATIVE WRITING\")\n",
    "print(\"(temperature=1.2, top_p=0.95, max_tokens=300)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# YOUR CODE HERE - create chatbot and test with creative prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Recipe 3: General Conversation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECIPE 3: GENERAL CONVERSATION\")\n",
    "print(\"(temperature=0.7, top_p=0.9, max_tokens=200)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# YOUR CODE HERE - create chatbot and test with conversational prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Analysis and Comparison (2 points)\n",
    "\n",
    "**Write 2-3 paragraphs addressing:**\n",
    "\n",
    "1. **Parameter Effects:**\n",
    "   - How did temperature affect the responses? Compare the factual (temp=0) vs creative (temp=1.2) outputs.\n",
    "   - What role did `top_p` play? Was the difference noticeable?\n",
    "   - How did `max_tokens` affect response completeness?\n",
    "\n",
    "2. **Use Case Recommendations:**\n",
    "   - Which recipe worked best for each type of task?\n",
    "   - When would you choose low temperature vs high temperature?\n",
    "   - What trade-offs did you observe between consistency and creativity?\n",
    "\n",
    "3. **Conversation History:**\n",
    "   - Did the chatbot maintain context across multiple turns?\n",
    "   - How does sending the full conversation history enable this?\n",
    "   - What are the cost implications of longer conversations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANALYSIS HERE:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Part 3 - Model Size Comparison (8 points)\n",
    "\n",
    "Different model sizes offer different trade-offs between quality and computational cost. In this part, you'll compare text generation across different model sizes.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load at least two different-sized models (e.g., 3B and 8B parameter models). Suggested options:\n",
    "   - \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" (3B parameters, 4-bit quantized)\n",
    "   - \"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\" (8B parameters, 4-bit quantized)\n",
    "   \n",
    "2. Create 3 diverse prompts that test different capabilities:\n",
    "   - A factual knowledge question\n",
    "   - A creative writing task\n",
    "   - A reasoning/problem-solving task\n",
    "   \n",
    "3. Generate responses from each model for each prompt using the same decoding parameters (e.g., temperature=0.7, top_p=0.9)\n",
    "\n",
    "4. For each generation, measure and record:\n",
    "   - Generation time\n",
    "   - Memory usage (if possible)\n",
    "   - Response length\n",
    "   - Qualitative assessment of quality\n",
    "   \n",
    "5. Create visualizations:\n",
    "   - Bar chart comparing generation times\n",
    "   - Any other relevant comparisons\n",
    "   \n",
    "6. Write an analysis (2-3 paragraphs) discussing:\n",
    "   - Performance differences between model sizes\n",
    "   - Quality differences in outputs\n",
    "   - When you would choose each model size\n",
    "   - Trade-offs between speed and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Load different-sized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create prompts and generate responses with timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create visualizations comparing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANALYSIS HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Part 4 - Creative Text Generation Application (8 points)\n",
    "\n",
    "Apply what you've learned about text generation to build a creative application. Choose ONE of the following:\n",
    "\n",
    "### Option A: Story Continuation System\n",
    "- Create a system that takes a story opening and generates multiple continuations using different decoding strategies\n",
    "- Allow the user to select which continuation they prefer\n",
    "- Continue the story iteratively, building on the selected continuations\n",
    "- Generate at least 3 rounds of story development\n",
    "\n",
    "### Option B: Dialogue Generator\n",
    "- Create a multi-character dialogue system\n",
    "- Define personalities for 2-3 characters (via system prompts)\n",
    "- Generate a conversation between the characters on a given topic\n",
    "- Experiment with different decoding parameters for each character to reflect their personality\n",
    "\n",
    "### Option C: Writing Style Transformer\n",
    "- Take a piece of text and rewrite it in different styles (formal, casual, poetic, technical, etc.)\n",
    "- Use both local models and API-based models\n",
    "- Compare how different models handle style transformation\n",
    "- Test with at least 3 different input texts and 4 different target styles\n",
    "\n",
    "**Requirements:**\n",
    "- Use at least 2 different models (one local, one via API)\n",
    "- Experiment with at least 3 different decoding configurations\n",
    "- Include clear output formatting and labeling\n",
    "- Write a reflection (2-3 paragraphs) on what worked well and what didn't, and what you learned about text generation from this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement your chosen creative application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR REFLECTION HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Part 5 - Analysis and Comparison (4 points)\n",
    "\n",
    "Synthesize your findings from all the previous parts into a comprehensive analysis.\n",
    "\n",
    "**Write 2-3 paragraphs addressing:**\n",
    "\n",
    "1. **Local vs. API-Based Generation:**\n",
    "   - Compare the experience of using local models vs. API-based models\n",
    "   - Discuss trade-offs in terms of control, cost, latency, and quality\n",
    "   - When would you choose each approach for a production application?\n",
    "\n",
    "2. **Decoding Strategy Selection:**\n",
    "   - Which decoding strategies performed best for different types of tasks?\n",
    "   - How did you balance coherence and diversity in your experiments?\n",
    "   - What general principles would you follow when choosing decoding parameters?\n",
    "\n",
    "3. **Practical Applications:**\n",
    "   - What real-world applications would benefit from the techniques you explored?\n",
    "   - What challenges did you encounter that would need to be addressed for production use?\n",
    "   - What additional features or improvements would make your implementations more robust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "\ud83d\udcdd **YOUR ANALYSIS HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Part 6 - Reflection (2 points)\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for the lesson? Why?\n",
    "\n",
    "\ud83d\udcdd **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "\ud83d\udcdd **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}