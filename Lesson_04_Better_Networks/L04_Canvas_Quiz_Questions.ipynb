{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the main reason for using the ReLU activation function over the sigmoid function?\n",
    "A. ReLU has a complex derivative  \n",
    "B. ReLU avoids vanishing gradients  \n",
    "C. Sigmoid functions are faster to compute  \n",
    "D. Sigmoid is preferred in CNNs  \n",
    "\n",
    "**Answer:** B\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How does Leaky ReLU differ from standard ReLU?\n",
    "A. It returns a constant for all negative inputs  \n",
    "B. It allows a small gradient for negative inputs  \n",
    "C. It outputs zero for negative inputs  \n",
    "D. It is designed for small datasets  \n",
    "\n",
    "**Answer:** B\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Which of the following is a benefit of using Batch Normalization?\n",
    "A. It increases model complexity  \n",
    "B. It eliminates the need for activation functions  \n",
    "C. It accelerates model convergence  \n",
    "D. It applies only to recurrent layers  \n",
    "\n",
    "**Answer:** C\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Where is Batch Normalization typically placed in a network architecture?\n",
    "A. Before the input layer only  \n",
    "B. After every linear or convolutional layer  \n",
    "C. Only at the final output layer  \n",
    "D. Before each activation function  \n",
    "\n",
    "**Answer:** B\n",
    "\n",
    "---\n",
    "\n",
    "### 5. In a skip connection, how is information typically passed between layers?\n",
    "A. It is concatenated or added to outputs of later layers  \n",
    "B. It is ignored in intermediate layers and used only at the output layer  \n",
    "C. It replaces the outputs of the current layer  \n",
    "D. It averages the outputs of all previous layers  \n",
    "\n",
    "**Answer:** A\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What makes skip connections especially useful for training deep networks?\n",
    "A. They avoid the problem of losing information through layers  \n",
    "B. They reduce the need for data augmentation  \n",
    "C. They decrease computation time for each layer  \n",
    "D. They eliminate the need for normalization layers  \n",
    "\n",
    "**Answer:** A\n",
    "\n",
    "---\n",
    "\n",
    "### 7. How does a residual connection differ from a general skip connection?\n",
    "A. It combines the original input with the processed output  \n",
    "B. It only applies in convolutional networks  \n",
    "C. It replaces all previous activation layers  \n",
    "D. It skips more layers than regular skip connections  \n",
    "\n",
    "**Answer:** A\n",
    "\n",
    "---\n",
    "\n",
    "### 8. What is the output of a residual block?\n",
    "A. The average of the input and output values  \n",
    "B. The sum of the input and a processed subnetwork result  \n",
    "C. A concatenation of inputs from all previous layers  \n",
    "D. The weighted average of input and output  \n",
    "\n",
    "**Answer:** B\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Which issue does adding residual connections help address?\n",
    "A. Overfitting on the validation data  \n",
    "B. Vanishing gradients in deep networks  \n",
    "C. Reducing network parameter count  \n",
    "D. Improving activation function complexity  \n",
    "\n",
    "**Answer:** B\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Which component allows the model to retain information while also learning additional transformations?\n",
    "A. Leaky ReLU  \n",
    "B. Layer Normalization  \n",
    "C. Batch Normalization  \n",
    "D. Residual Connections  \n",
    "\n",
    "**Answer:** D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
