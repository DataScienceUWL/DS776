{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 7 Reading Quiz\n",
    "\n",
    "Read XXX\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. What is the primary advantage of transformers in NLP?  \n",
    "A. They rely on recurrent connections for context awareness.  \n",
    "B. They are limited to text classification tasks.  \n",
    "C. They can process sequences of text in parallel.  \n",
    "D. They require less data to train than traditional models.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. What does the encoder in a transformer primarily do?  \n",
    "A. Processes the input sequence to create contextual representations.  \n",
    "B. Applies convolutional layers to the input sequence.  \n",
    "C. Generates tokens for text summarization.  \n",
    "D. Generates output sequences.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Which component is critical for enabling transformers to handle long-range dependencies?  \n",
    "A. Attention mechanisms  \n",
    "B. Residual connections  \n",
    "C. Feed-forward layers  \n",
    "D. Positional embeddings  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4. What is the role of transfer learning in NLP with transformers?  \n",
    "A. It eliminates the need for labeled data.  \n",
    "B. It reduces the modelâ€™s complexity.  \n",
    "C. It enables models to generalize better by pretraining on large datasets and fine-tuning on specific tasks.  \n",
    "D. It replaces traditional tokenization methods.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Which library is NOT part of the Hugging Face ecosystem?  \n",
    "A. TensorFlow  \n",
    "B. Datasets  \n",
    "C. Transformers  \n",
    "D. Tokenizers  \n",
    "\n",
    "---\n",
    "\n",
    "#### 6. What type of tasks are transformers particularly well-suited for?  \n",
    "A. Language-related tasks like translation, summarization, and question answering  \n",
    "B. Hardware-specific computations  \n",
    "C. Only numerical data processing  \n",
    "D. Visual recognition tasks exclusively  \n",
    "\n",
    "---\n",
    "\n",
    "#### 7. What is a challenge associated with transformers?  \n",
    "A. They cannot process more than one sentence at a time.  \n",
    "B. They require significant computational resources for training and inference.  \n",
    "C. They are only effective for small datasets.  \n",
    "D. They do not support multilingual tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 8. How do attention mechanisms in transformers differ from traditional models like RNNs?  \n",
    "A. Attention mechanisms only work for small datasets.  \n",
    "B. Attention mechanisms use convolutional filters.  \n",
    "C. Attention mechanisms allow transformers to process all words in a sequence simultaneously.  \n",
    "D. Attention mechanisms rely on sequential processing.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 9. What is the purpose of positional embeddings in transformers?  \n",
    "A. To indicate the relative or absolute position of tokens in a sequence.  \n",
    "B. To reduce the number of parameters in the model.  \n",
    "C. To increase the speed of training.  \n",
    "D. To add contextual meaning to each token.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 10. Which of the following is NOT a typical NLP task performed by transformers?  \n",
    "A. Named Entity Recognition (NER)  \n",
    "B. Machine translation  \n",
    "C. Image segmentation  \n",
    "D. Text classification  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
