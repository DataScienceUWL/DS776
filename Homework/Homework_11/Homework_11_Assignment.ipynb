{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "# Homework 11: Text Generation and Decoding Strategies\n\n**Total Points: 40**\n- Part 1 (Decoding Strategies Comparison): 10 points\n- Part 2 (Building API Helper Functions): 8 points\n- Part 3 (Model Size Comparison): 8 points\n- Part 4 (Creative Text Generation): 8 points\n- Part 5 (Analysis and Comparison): 4 points\n- Part 6 (Reflection): 2 points"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Decoding Strategies Comparison (10 points)\n",
    "\n",
    "In this part, you'll experiment with different decoding strategies to see how they affect generated text quality and diversity.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load a text generation model. **Suggested quantized models for A6000 GPUs (48GB VRAM):**\n",
    "\n",
    "   **Small models (3-4B parameters, ~2-3GB VRAM):**\n",
    "   - `\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"` - Fast, good for testing\n",
    "   - `\"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"` - Alternative 3B option\n",
    "\n",
    "   **Medium models (7-8B parameters, ~5-6GB VRAM):**\n",
    "   - `\"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\"` - Balanced quality/speed\n",
    "   - `\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"` - Base 8B variant\n",
    "   - `\"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"` - Strong 7B model\n",
    "\n",
    "   **Large models (14B parameters, ~8-10GB VRAM):**\n",
    "   - `\"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"` - High quality generation\n",
    "\n",
    "   **Very large models (70B+ parameters, ~40-45GB VRAM):**\n",
    "   - `\"unsloth/Meta-Llama-3.1-70B-bnb-4bit\"` - Maximum quality (use if you have exclusive GPU access)\n",
    "\n",
    "   **Note:** All these models use 4-bit quantization (bnb-4bit) which reduces memory by ~4x compared to full precision. Start with a 3B or 8B model for faster experimentation, then try larger models if you want to compare quality.\n",
    "\n",
    "2. Create an interesting prompt for text generation (e.g., a story opening, a technical explanation, or a dialogue start)\n",
    "3. Generate text using at least 5 different decoding configurations:\n",
    "   - Greedy search (do_sample=False)\n",
    "   - Beam search with num_beams=5\n",
    "   - Beam search with no_repeat_ngram_size=2\n",
    "   - Sampling with temperature=0.5\n",
    "   - Sampling with temperature=1.5\n",
    "   - Top-k sampling (top_k=50)\n",
    "   - Nucleus sampling (top_p=0.9)\n",
    "\n",
    "4. For each generated text:\n",
    "   - Display the full generated text\n",
    "   - Calculate and display the sequence log probability\n",
    "   - Comment on the coherence, diversity, and quality\n",
    "\n",
    "5. Write a summary (3-4 paragraphs) comparing the different decoding strategies. Which produced the most coherent text? Which was most creative? Which would you choose for different use cases?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Decoding Strategies and Probability Analysis (10 points)\n",
    "\n",
    "In this part, you'll implement three core decoding strategies and analyze how they affect both the generated text and the probability distributions. You'll learn to extract and visualize token probabilities from model outputs, giving you insight into how language models make decisions.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how to extract probabilities from model logits\n",
    "- Visualize probability distributions across decoding strategies\n",
    "- Compare greedy search, beam search, and sampling methods\n",
    "- Analyze the trade-offs between coherence, diversity, and model confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Understanding Model Outputs (2 points)\n",
    "\n",
    "Before implementing decoding strategies, you need to understand how to extract and display token probabilities from the model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Load a text generation model (use one of the quantized models suggested in the overview)\n",
    "2. Define a prompt and get the model's output for the next token prediction\n",
    "3. Complete the provided `get_top_tokens_with_probs()` function to:\n",
    "   - Extract logits from model output\n",
    "   - Convert logits to probabilities using softmax\n",
    "   - Return the top-k tokens with their probabilities\n",
    "4. Create a visualization showing the probability distribution\n",
    "5. Answer the analysis questions below\n",
    "\n",
    "**Hints:**\n",
    "- Review Section 6 in the lesson for similar examples\n",
    "- Use `torch.softmax()` to convert logits to probabilities\n",
    "- Use `torch.topk()` to get the top tokens\n",
    "- Probabilities should sum to approximately 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # or another model from suggestions\n",
    "\n",
    "# YOUR CODE: Load tokenizer and model\n",
    "\n",
    "\n",
    "# YOUR CODE: Define a prompt and tokenize it\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "\n",
    "def get_top_tokens_with_probs(model, input_ids, k=10):\n",
    "    \"\"\"\n",
    "    Extract top-k tokens and their probabilities for next token prediction.\n",
    "\n",
    "    Args:\n",
    "        model: The language model\n",
    "        input_ids: Tokenized input (tensor)\n",
    "        k: Number of top tokens to return\n",
    "\n",
    "    Returns:\n",
    "        List of (token_string, probability) tuples\n",
    "\n",
    "    Hints:\n",
    "    - Get model output: output = model(input_ids)\n",
    "    - Extract logits for last position: logits = output.logits[0, -1, :]\n",
    "    - Convert to probabilities: probs = torch.softmax(logits, dim=-1)\n",
    "    - Get top-k: top_probs, top_ids = torch.topk(probs, k=k)\n",
    "    - Decode tokens: tokenizer.decode(token_id)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_probability_distribution(probabilities, title=\"Token Probabilities\"):\n",
    "    \"\"\"\n",
    "    Plot probability distribution for top 50 tokens and cumulative probability.\n",
    "\n",
    "    Hints:\n",
    "    - Sort probabilities: sorted_probs, _ = torch.sort(probs, descending=True)\n",
    "    - Use plt.bar() for histogram\n",
    "    - Use torch.cumsum() for cumulative probability\n",
    "    - Create a 2-subplot figure\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# YOUR CODE: Call the functions and display results\n",
    "# 1. Get top 10 tokens with probabilities\n",
    "# 2. Print them in a formatted table\n",
    "# 3. Create probability distribution plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Questions:**\n",
    "\n",
    "1. **What is the probability of the most likely token?** Is it very confident (>50%) or more uncertain?\n",
    "\n",
    "2. **How many tokens does it take to cover 90% of the probability mass?** This tells you how \"focused\" vs \"spread out\" the model's predictions are.\n",
    "\n",
    "3. **Explain in your own words:** Why do we use `torch.softmax()` to convert logits to probabilities? What does softmax do?\n",
    "\n",
    "üìù **YOUR ANSWERS HERE:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Implement Three Decoding Strategies (5 points)\n",
    "\n",
    "Now implement three core decoding strategies and compare their behavior. Use the **same prompt** for all three strategies so you can compare the results fairly.\n",
    "\n",
    "**Strategy 1: Greedy Search**\n",
    "- `do_sample=False` (no sampling parameter needed)\n",
    "- Always picks the token with highest probability\n",
    "- Deterministic - same output every time\n",
    "\n",
    "**Strategy 2: Beam Search with No-Repeat N-Gram**\n",
    "- `num_beams=5`\n",
    "- `no_repeat_ngram_size=2` (prevents repetitive phrases)\n",
    "- `do_sample=False`\n",
    "- Explores multiple paths simultaneously\n",
    "\n",
    "**Strategy 3: Nucleus (Top-P) Sampling**\n",
    "- `do_sample=True`\n",
    "- `top_p=0.9` (sample from tokens covering 90% probability mass)\n",
    "- `temperature=0.7` (slightly lower than default for more focused generation)\n",
    "- Stochastic - different output each time\n",
    "\n",
    "**For each strategy:**\n",
    "1. Generate text (max_length=100 or max_new_tokens=50)\n",
    "2. Display the generated text\n",
    "3. Show the top 5 token probabilities at **three key timesteps**:\n",
    "   - Beginning (token 1 or 2 after prompt)\n",
    "   - Middle (around token 25)\n",
    "   - End (last few tokens)\n",
    "4. Create a comparison visualization\n",
    "5. Comment on the text quality and probability patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Strategy 1: Greedy Search\n",
    "\n",
    "prompt = \"In a shocking scientific discovery, researchers found\"  # Use your own prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY 1: GREEDY SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate with greedy search\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Display generated text\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Show probability distributions at key timesteps\n",
    "# Hint: You'll need to call the model again with the generated sequence\n",
    "#       to get probabilities at different positions\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Strategy 2: Beam Search with No-Repeat N-Gram\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STRATEGY 2: BEAM SEARCH (num_beams=5, no_repeat_ngram_size=2)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Strategy 3: Nucleus (Top-P) Sampling\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STRATEGY 3: NUCLEUS SAMPLING (top_p=0.9, temperature=0.7)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Comparison and Analysis (3 points)\n",
    "\n",
    "Write 2-3 paragraphs comparing the three decoding strategies. Address:\n",
    "\n",
    "**Text Quality:**\n",
    "- Which strategy produced the most coherent text?\n",
    "- Did any strategy produce repetitive text?\n",
    "- Which was most creative/diverse?\n",
    "\n",
    "**Probability Patterns:**\n",
    "- How did the probability distributions differ between strategies?\n",
    "- Did greedy search show higher confidence (more peaked distributions)?\n",
    "- How did sampling affect the probability spread?\n",
    "- Did you notice the distributions changing from beginning to end of generation?\n",
    "\n",
    "**Use Case Recommendations:**\n",
    "- When would you use greedy search?\n",
    "- When would beam search be preferred?\n",
    "- When would you choose nucleus sampling?\n",
    "- What trade-offs exist between coherence and diversity?\n",
    "\n",
    "üìù **YOUR ANALYSIS HERE:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Extending the Conversation Class with Decoding Parameters (8 points)\n",
    "\n",
    "In Lesson 11, Section 5.5, you learned about the `Conversation` class for building chatbots with APIs. In Section 7.5, you learned about decoding parameters (temperature, top_p, max_tokens) that control text generation behavior.\n",
    "\n",
    "In this part, you'll combine these concepts by extending the `Conversation` class to support decoding parameter control, allowing you to fine-tune the chatbot's behavior for different use cases.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Practice working with chat roles (system/user/assistant) and conversation history\n",
    "- Understand how decoding parameters affect model behavior\n",
    "- Apply object-oriented design patterns to build maintainable APIs\n",
    "- Test different parameter combinations for various tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Copy and Extend the Conversation Class (3 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Copy the `Conversation` class from Lesson 11, Section 5.5 (you'll find it in the \"Building a Chatbot Class\" section)\n",
    "\n",
    "2. Extend the class to support decoding parameters:\n",
    "   - Add `temperature`, `top_p`, and `max_tokens` parameters to `__init__()` with these defaults:\n",
    "     - `temperature=0.7`\n",
    "     - `top_p=0.9`\n",
    "     - `max_tokens=256`\n",
    "   - Modify the `send()` method to pass these parameters to the API call\n",
    "   - Allow per-message parameter overrides in `send()` (e.g., `send(message, temperature=0.0)`)\n",
    "\n",
    "3. Your extended class should:\n",
    "   - Store default parameters as instance variables\n",
    "   - Use defaults if no overrides provided\n",
    "   - Pass all three parameters to `client.chat.completions.create()`\n",
    "\n",
    "**Hints:**\n",
    "- The original `Conversation` class already handles the OpenAI client and message history\n",
    "- You just need to add parameter handling to `__init__()` and `send()`\n",
    "- Use `if parameter is not None` to check for overrides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Copy and extend the Conversation class\n",
    "class Conversation:\n",
    "    \"\"\"\n",
    "    Extended chatbot class with decoding parameter control.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): OpenRouter API key\n",
    "        model (str): Model name (e.g., 'google/gemini-2.5-flash-lite')\n",
    "        system_prompt (str, optional): Initial instructions for the model\n",
    "        temperature (float): Sampling temperature (default: 0.7)\n",
    "        top_p (float): Nucleus sampling parameter (default: 0.9)\n",
    "        max_tokens (int): Maximum tokens to generate (default: 256)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key, model=\"google/gemini-2.5-flash-lite\",\n",
    "                 system_prompt=None, temperature=0.7, top_p=0.9, max_tokens=256):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def send(self, user_message, temperature=None, top_p=None, max_tokens=None):\n",
    "        \"\"\"\n",
    "        Send a message and get a response.\n",
    "\n",
    "        Args:\n",
    "            user_message (str): The user's input\n",
    "            temperature (float, optional): Override default temperature\n",
    "            top_p (float, optional): Override default top_p\n",
    "            max_tokens (int, optional): Override default max_tokens\n",
    "\n",
    "        Returns:\n",
    "            str: The assistant's response\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def display(self, show_recent_only=True):\n",
    "        \"\"\"Show conversation history (you can use a simple print for now)\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def reset(self, system_prompt=None):\n",
    "        \"\"\"Reset conversation history\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "print(\"‚úì Extended Conversation class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Test Different Parameter Recipes (3 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Create three different chatbot configurations optimized for different use cases:\n",
    "\n",
    "**Recipe 1: Factual Question Answering**\n",
    "- `temperature=0` (deterministic/greedy)\n",
    "- `top_p=1.0` (not used when temperature=0)\n",
    "- `max_tokens=150`\n",
    "- System prompt: \"You are a knowledgeable assistant providing accurate, factual information.\"\n",
    "- Test with a factual question (e.g., \"What is photosynthesis?\")\n",
    "\n",
    "**Recipe 2: Creative Writing**\n",
    "- `temperature=1.2` (more random)\n",
    "- `top_p=0.95` (wide vocabulary)\n",
    "- `max_tokens=300`\n",
    "- System prompt: \"You are a creative storyteller with a vivid imagination.\"\n",
    "- Test with a creative prompt (e.g., \"Start a science fiction story about AI\")\n",
    "\n",
    "**Recipe 3: General Conversation**\n",
    "- `temperature=0.7` (balanced)\n",
    "- `top_p=0.9` (nucleus sampling)\n",
    "- `max_tokens=200`\n",
    "- System prompt: \"You are a friendly, helpful assistant.\"\n",
    "- Test with a conversational prompt (e.g., \"Explain neural networks to a beginner\")\n",
    "\n",
    "For each recipe:\n",
    "1. Create a new `Conversation` instance with the specified parameters\n",
    "2. Send at least 2 messages to create a multi-turn conversation\n",
    "3. Display the conversation (or print the messages)\n",
    "4. Note the characteristics of the responses (consistency, creativity, length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Recipe 1: Factual Question Answering\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RECIPE 1: FACTUAL QUESTION ANSWERING\")\n",
    "print(\"(temperature=0, top_p=1.0, max_tokens=150)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create chatbot\n",
    "factual_chat = Conversation(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    model=\"google/gemini-2.5-flash-lite\",\n",
    "    system_prompt=\"You are a knowledgeable assistant providing accurate, factual information.\",\n",
    "    temperature=0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Test with multi-turn conversation\n",
    "# YOUR CODE HERE - send at least 2 messages and display responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Recipe 2: Creative Writing\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECIPE 2: CREATIVE WRITING\")\n",
    "print(\"(temperature=1.2, top_p=0.95, max_tokens=300)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# YOUR CODE HERE - create chatbot and test with creative prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Recipe 3: General Conversation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECIPE 3: GENERAL CONVERSATION\")\n",
    "print(\"(temperature=0.7, top_p=0.9, max_tokens=200)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# YOUR CODE HERE - create chatbot and test with conversational prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Analysis and Comparison (2 points)\n",
    "\n",
    "**Write 2-3 paragraphs addressing:**\n",
    "\n",
    "1. **Parameter Effects:**\n",
    "   - How did temperature affect the responses? Compare the factual (temp=0) vs creative (temp=1.2) outputs.\n",
    "   - What role did `top_p` play? Was the difference noticeable?\n",
    "   - How did `max_tokens` affect response completeness?\n",
    "\n",
    "2. **Use Case Recommendations:**\n",
    "   - Which recipe worked best for each type of task?\n",
    "   - When would you choose low temperature vs high temperature?\n",
    "   - What trade-offs did you observe between consistency and creativity?\n",
    "\n",
    "3. **Conversation History:**\n",
    "   - Did the chatbot maintain context across multiple turns?\n",
    "   - How does sending the full conversation history enable this?\n",
    "   - What are the cost implications of longer conversations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANALYSIS HERE:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Part 3 - Model Size Comparison (8 points)\n",
    "\n",
    "Different model sizes offer different trade-offs between quality and computational cost. In this part, you'll compare text generation across different model sizes.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load at least two different-sized models (e.g., 3B and 8B parameter models). Suggested options:\n",
    "   - \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" (3B parameters, 4-bit quantized)\n",
    "   - \"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\" (8B parameters, 4-bit quantized)\n",
    "   \n",
    "2. Create 3 diverse prompts that test different capabilities:\n",
    "   - A factual knowledge question\n",
    "   - A creative writing task\n",
    "   - A reasoning/problem-solving task\n",
    "   \n",
    "3. Generate responses from each model for each prompt using the same decoding parameters (e.g., temperature=0.7, top_p=0.9)\n",
    "\n",
    "4. For each generation, measure and record:\n",
    "   - Generation time\n",
    "   - Memory usage (if possible)\n",
    "   - Response length\n",
    "   - Qualitative assessment of quality\n",
    "   \n",
    "5. Create visualizations:\n",
    "   - Bar chart comparing generation times\n",
    "   - Any other relevant comparisons\n",
    "   \n",
    "6. Write an analysis (2-3 paragraphs) discussing:\n",
    "   - Performance differences between model sizes\n",
    "   - Quality differences in outputs\n",
    "   - When you would choose each model size\n",
    "   - Trade-offs between speed and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Load different-sized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create prompts and generate responses with timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create visualizations comparing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "üìù **YOUR ANALYSIS HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Part 4 - Creative Text Generation Application (8 points)\n",
    "\n",
    "Apply what you've learned about text generation to build a creative application. Choose ONE of the following:\n",
    "\n",
    "### Option A: Story Continuation System\n",
    "- Create a system that takes a story opening and generates multiple continuations using different decoding strategies\n",
    "- Allow the user to select which continuation they prefer\n",
    "- Continue the story iteratively, building on the selected continuations\n",
    "- Generate at least 3 rounds of story development\n",
    "\n",
    "### Option B: Dialogue Generator\n",
    "- Create a multi-character dialogue system\n",
    "- Define personalities for 2-3 characters (via system prompts)\n",
    "- Generate a conversation between the characters on a given topic\n",
    "- Experiment with different decoding parameters for each character to reflect their personality\n",
    "\n",
    "### Option C: Writing Style Transformer\n",
    "- Take a piece of text and rewrite it in different styles (formal, casual, poetic, technical, etc.)\n",
    "- Use both local models and API-based models\n",
    "- Compare how different models handle style transformation\n",
    "- Test with at least 3 different input texts and 4 different target styles\n",
    "\n",
    "**Requirements:**\n",
    "- Use at least 2 different models (one local, one via API)\n",
    "- Experiment with at least 3 different decoding configurations\n",
    "- Include clear output formatting and labeling\n",
    "- Write a reflection (2-3 paragraphs) on what worked well and what didn't, and what you learned about text generation from this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement your chosen creative application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "üìù **YOUR REFLECTION HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Part 5 - Analysis and Comparison (4 points)\n",
    "\n",
    "Synthesize your findings from all the previous parts into a comprehensive analysis.\n",
    "\n",
    "**Write 2-3 paragraphs addressing:**\n",
    "\n",
    "1. **Local vs. API-Based Generation:**\n",
    "   - Compare the experience of using local models vs. API-based models\n",
    "   - Discuss trade-offs in terms of control, cost, latency, and quality\n",
    "   - When would you choose each approach for a production application?\n",
    "\n",
    "2. **Decoding Strategy Selection:**\n",
    "   - Which decoding strategies performed best for different types of tasks?\n",
    "   - How did you balance coherence and diversity in your experiments?\n",
    "   - What general principles would you follow when choosing decoding parameters?\n",
    "\n",
    "3. **Practical Applications:**\n",
    "   - What real-world applications would benefit from the techniques you explored?\n",
    "   - What challenges did you encounter that would need to be addressed for production use?\n",
    "   - What additional features or improvements would make your implementations more robust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "üìù **YOUR ANALYSIS HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Part 6 - Reflection (2 points)\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for the lesson? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}