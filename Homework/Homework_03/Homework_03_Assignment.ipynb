{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Homework 03 Assignment\n**Name:** [Student Name Here]  \n**Total Points:** 40\n\n## Submission Checklist\n- [ ] All code cells executed with output saved\n- [ ] All questions answered\n- [ ] Notebook converted to HTML (use the Homework_03_Utilities notebook)\n- [ ] Canvas notebook filename includes `_GRADE_THIS_ONE`\n- [ ] Files uploaded to Canvas\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Training Techniques\n",
    "\n",
    "In this assignment you will build a deeper CNN model to improve the classification performance on the FashionMNIST dataset. Deeper models can be more difficult to train so you'll employ some of the techniques from Lesson 3 to improve the training. You'll also use data augmentation to improve the performance of the model while reducing overfitting. Along the way you'll see how to downsample a dataset to make for more efficient experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR IMPORTS HERE\n# Add any additional imports you need below this line\n\n\nfrom introdl.utils import config_paths_keys\n\n# Configure paths\npaths = config_paths_keys()\nDATA_PATH = paths['DATA_PATH']\nMODELS_PATH = paths['MODELS_PATH']"
  },
  {
   "cell_type": "markdown",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_03_Models/` | `Homework_03_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` ‚Äî never hardcode paths like `'Homework_03_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1 - Model and Data Setup (10 pts)\n\n### 1.1 [5 pts] Build the Model\n\nImplement a PyTorch model of class `nn.Module` to reproduce a model with the structure shown in the assignment. The model should have:\n- Three blocks of convolutional layers\n- Each block contains 3 Conv2d layers with ReLU activations\n- MaxPool2d after blocks 1 and 2\n- A single Linear layer as the classifier\n- Total parameters should be around 542,922"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Define the FashionMNISTModel class\n# - Block 1: 1->32 channels, 3 conv layers, MaxPool2d\n# - Block 2: 32->64 channels, 3 conv layers, MaxPool2d\n# - Block 3: 64->128 channels, 3 conv layers, no pooling\n# - Classifier: Flatten + Linear(6272, 10)\n\nclass FashionMNISTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Your implementation\n        pass\n    \n    def forward(self, x):\n        # Your implementation\n        pass\n\n# Create model and show summary\n# model = FashionMNISTModel()\n# summary(model, input_size=(64, 1, 28, 28))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 [5 pts] Setup the Data\n\nLoad the FashionMNIST dataset. Normalize with mean 0.2860 and standard deviation 0.3530. Downsample the train dataset to 10% of its original size to make experimentation quick.\n\nUse the FashionMNIST test dataset for your `valid_dataset`. For the DataLoaders try batch size 64 to start."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Load FashionMNIST with normalization\n# TODO: Downsample training set to 10% using provided code\n# TODO: Create DataLoaders\n\n# Use this code for downsampling:\n# from torch.utils.data import Subset\n# np.random.seed(42)  # use this seed for reproducibility\n# subset_indices = np.random.choice(len(train_dataset), size=int(0.1 * len(train_dataset)), replace=False)\n# train_dataset = Subset(train_dataset, subset_indices)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2 - Optimizer Comparison (10 pts)\n\n### 2.1 [5 pts] Training with SGD\n\nTrain your model with Stochastic Gradient Descent. Track the accuracy metric. You'll likely need to increase both the learning rate and the number of epochs to see the validation accuracy plateau.\n\nMake sure to instantiate a fresh model to see complete training results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create fresh model instance\n# TODO: Setup SGD optimizer\n# TODO: Train with appropriate learning rate and epochs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Load checkpoint and plot metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 [5 pts] Training with AdamW\n\nNow repeat the previous training using AdamW. You should be able to use the default learning rate of 0.001 and fewer epochs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create fresh model instance\n# TODO: Setup AdamW optimizer\n# TODO: Train with default learning rate"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Load checkpoint and plot metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Compare SGD and AdamW Training Performance\n\nMake plots of validation loss and accuracy for both SGD and AdamW."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create comparison plots"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3 - Advanced Training Techniques (13 pts)\n\n### 3.1 [5 pts] Data Augmentation\n\nNow use data augmentation. Build a transform_train pipeline that includes:\n* Random horizontal flips\n* Random crops of size 28, padding = 4\n* Random rotations up to 10 degrees\n\nUse the same seed to downsample the train_dataset to 10% of its size.\n\nIn the next cell, set up the data and augmentation transforms (don't augment the validation data). Build the DataLoaders."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create augmentation transforms\n# TODO: Setup datasets with augmentation\n# TODO: Create DataLoaders"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Train a new instance of your model with the new DataLoaders and AdamW. Training will take more epochs so you may have to experiment a little."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Train model with augmentation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Load checkpoint and plot metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Compare validation loss and accuracy for the three different approaches so far: SGD, AdamW, and AdamW with augmentation. Make appropriate graphs and comment on the three training strategies in terms of their performance on metrics and overfitting.\n\nüìù **YOUR ANALYSIS HERE:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create comparison plots for all three approaches"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 [3 pts] Early Stopping\n\nEarly stopping isn't really necessary unless the metrics on the validation or test set start to degrade. Try it anyway just to reinforce how it works. In this section implement early stopping based on the validation loss. Use AdamW and data augmentation. Add a comparison plot of the two methods. Comment on the performance with and without early stopping. Do you get comparable performance?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Implement training with early stopping\n# TODO: Compare with regular training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "üìù **YOUR ANALYSIS HERE:**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 [5 pts] OneCycleLR Scheduler\n\nCreate a new instance of the model. Implement a OneCycleLR learning rate scheduler and add it to your AdamW approach with data augmentation. You should be able to use a larger max learning rate of 0.003 or so. Experiment a little to see if you can get similar results to the above with fewer epochs (you may not be able to)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Implement OneCycleLR training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Load checkpoint and plot metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Make a plot comparing the validation losses and accuracies for all of the training approaches above (there should be 4)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Create comprehensive comparison plots"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Which approach works best? Why?\n\nüìù **YOUR ANSWER HERE:**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4 - Full Dataset Training (5 pts)\n\nTake your best approach and apply it to the full dataset (don't downsample).\n\nThis will take a little more than a minute per epoch so run your experiments with the smaller dataset above, then run this once. You can use `resume_from_checkpoint = True` if you want to extend the training.\n\nHow does this compare to the performance you achieved in HW 2? Import your best run from HW 2 and make a plot comparing the performance of your best approach from this assignment to the approach from the second assignment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# TODO: Train best model on full dataset\n# TODO: Compare with HW2 results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "üìù **YOUR ANALYSIS HERE:**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 5 - Reflection (2 pts)\n\n1. What, if anything, did you find difficult to understand for this lesson? Why?\n\nüìù **YOUR ANSWER HERE:**\n\n2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n\nüìù **YOUR ANSWER HERE:**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n\nUncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}