{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ introdl v1.6.46 already up to date\n"
     ]
    }
   ],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "In this notebook we'll\n",
    "\n",
    "* List some common applications of NER\n",
    "* Give a brief history of NER\n",
    "* Demonstrate how to setup and fine-tune a DistilBERT model for NER\n",
    "* Discuss some of the issues with using an LLM for an NER task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running that cell, you should restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of NER\n",
    "\n",
    "I wasn't really familiar with Named Entity Recognition before building this course.  However, after studying it for a bit I realize it's very similar to object detection and instance segmentation in computer vision where we're trying to \"tag\" individual objects in an image.  Now we're doing it with text.  Now that I know more about it I realize that NER is everywhere:\n",
    "\n",
    "- **Information Extraction from Text**\n",
    "  - Identify names of people, places, organizations, and dates in news articles, legal documents, and academic papers.\n",
    "\n",
    "- **Search and Question Answering**\n",
    "  - Improve retrieval and understanding by recognizing key entities in queries and documents (e.g., ‚ÄúWhere was Barack Obama born?‚Äù).\n",
    "\n",
    "- **Social Media Monitoring**\n",
    "  - Detect mentions of public figures, brands, products, and locations in tweets, posts, and comments for sentiment analysis or moderation.\n",
    "\n",
    "- **Marketing and Trend Analysis**\n",
    "  - Track mentions of brands, competitors, or topics over time to identify emerging trends and customer interests.\n",
    "\n",
    "- **Content Recommendation**\n",
    "  - Extract entities (e.g., movies, products, places) from reviews and user posts to personalize content or advertisements.\n",
    "\n",
    "- **Customer Support Automation**\n",
    "  - Identify product names, user accounts, and issue types in support chats and emails to assist routing and auto-response systems.\n",
    "\n",
    "- **Financial and Business Intelligence**\n",
    "  - Extract company names, stock tickers, monetary values, and events from reports or articles to support decision-making.\n",
    "\n",
    "- **Medical and Clinical Text Analysis**\n",
    "  - Identify diseases, medications, and procedures in clinical notes for tasks like anonymization, coding, or record analysis.\n",
    "\n",
    "- **Legal and Compliance Monitoring**\n",
    "  - Recognize case names, organizations, and laws in legal documents to support research, auditing, or compliance checks.\n",
    "\n",
    "- **Resume and Job Post Parsing**\n",
    "  - Extract structured information such as skills, education, job titles, and companies to streamline recruitment processes.\n",
    "\n",
    "\n",
    "**Side Note:**  I'm using NER heavily right now to extract structured information from radiologist and patholgist findings in electronic health records. This feeds into model training for cancer diagnosis from breast ultrasound exams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Analogy\n",
    "\n",
    "* Image Classification - classify the entire image into a category\n",
    "* Text Classification - classify the entire text into a category\n",
    "* Image Segmentation - classify each pixel in an image into a category\n",
    "* Named Entity Recognition - class each word (token) into a category\n",
    "\n",
    "So text classification is to image classification as named entity recognition is to image segmentation.\n",
    "\n",
    "Or, more simply:  **Named Entity Recognition = Text Segmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chronology of State-of-the-Art Approaches for Named Entity Recognition (NER)**  \n",
    "\n",
    "The evolution of NER closely parallels the evolution of algorithms for text classification.  Early approaches were based on statistical models, then word embeddings and recurrent neural networks, before transformer architectures revolutionized the field since 2017.  \n",
    "\n",
    "Here's a timeline of some of the key advancements in NER:\n",
    "\n",
    "---\n",
    "\n",
    "### **Pre-2010s: Rule-Based Systems and Feature Engineering**  \n",
    "Early NER systems used **hand-crafted rules**, lookup lists (called **gazetteers**), and basic statistical models like **Hidden Markov Models (HMMs)** and **Conditional Random Fields (CRFs)**.  \n",
    "- **HMMs** modeled sequences by predicting the most likely tag (e.g., PERSON, LOCATION) for each word based on probabilities.\n",
    "- **CRFs** improved on HMMs by allowing more flexible features and considering the entire sequence when making predictions.\n",
    "\n",
    "These approaches required heavy manual feature engineering‚Äîlike marking whether a word is capitalized, its part of speech, or its prefix/suffix.\n",
    "\n",
    "- **1990s‚Äì2000s**: Rule-based systems and statistical models dominated tasks like newswire NER.\n",
    "- **2003**: The CoNLL-2003 shared task standardized benchmarks and boosted interest in developing better NER models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2010s: Word Embeddings and Neural Sequence Models**  \n",
    "NER systems improved significantly with the introduction of **word embeddings** like **Word2Vec** and **GloVe**, which represented words in continuous vector space based on context. These embeddings replaced sparse, manual features.\n",
    "\n",
    "- **2013‚Äì2015**: **Word2Vec** and **GloVe** made it easier to train neural models for NER.\n",
    "- **2015‚Äì2016**: **BiLSTM-CRF** architectures became popular‚Äîcombining bidirectional LSTMs (which read sentences both forward and backward) with a CRF layer to model dependencies between entity tags.\n",
    "- **2015**: **spaCy** launched as a fast, practical NLP library with built-in NER support, making NER accessible for developers and educators.\n",
    "- **2016‚Äì2017**: Character-level embeddings and CNNs were added to improve robustness to spelling variation and rare words.\n",
    "\n",
    "---\n",
    "\n",
    "### **Late 2010s: Contextual Embeddings and Transformers**  \n",
    "NER took a major leap with **contextualized embeddings** from transformer-based models.\n",
    "\n",
    "- **2018**: **ELMo** introduced deep contextualized word representations that vary based on sentence context.\n",
    "- **2018**: **BERT** achieved state-of-the-art NER results by treating NER as a token classification problem using bidirectional transformer layers.\n",
    "- **2019**: **Flair** added character-level contextual embeddings to further improve performance on small or domain-specific datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **2020s: Prompting and Large Language Models (LLMs)**  \n",
    "Recent NER approaches increasingly use **LLMs** like **GPT-4**, **Claude**, and **Gemini**, which can extract entities using **natural language prompts** instead of token-level supervision.\n",
    "\n",
    "- **2020‚Äì2022**: Models like **RoBERTa**, **SpanBERT**, and **LUKE** fine-tuned transformer architectures for better span detection and entity-aware representations.\n",
    "- **spaCy** added support for transformer-based pipelines (e.g., `en_core_web_trf`) to make state-of-the-art NER accessible for production use.\n",
    "- **2023‚Äì2025**: Instruction-tuned models like **GLiNER** and general-purpose LLMs now handle **zero-shot or few-shot NER** using prompts like *\"Find all organizations and people in this sentence.\"* These models reduce the need for annotated datasets and allow rapid prototyping for new entity types.\n",
    "\n",
    "  While LLMs offer flexibility and ease of use, they may be less precise than traditional models. Hybrid systems often combine LLMs with structured postprocessing or constrained decoding to improve accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "We'll focus on two of these tools.  We'll fine-tune a BERT model for NER and we'll look at some of the hurdles to using LLMs for NER.  You'll explore both of these topics further in the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our main import cell before we dive into the rest of the material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment: Unknown Environment | Course root: /mnt/e/GDrive_baggett.jeff/Teaching/Classes_current/2025-2026_Fall_DS776/DS776\n",
      "   Using workspace: <DS776_ROOT_DIR>/home_workspace\n",
      "\n",
      "üìÇ Storage Configuration:\n",
      "   DATA_PATH: <DS776_ROOT_DIR>/home_workspace/data\n",
      "   MODELS_PATH: /home/jbaggett/DS776_new/Lessons/Lesson_10_Named_Entity_Recognition/Lesson_10_Models (local to this notebook)\n",
      "   CACHE_PATH: <DS776_ROOT_DIR>/home_workspace/downloads\n",
      "üîë API keys: 9 loaded from home_workspace/api_keys.env\n",
      "üîê Available: ANTHROPIC_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY... (9 total)\n",
      "‚úÖ HuggingFace Hub: Logged in\n",
      "‚úÖ Loaded pricing for 347 OpenRouter models\n",
      "‚úÖ Cost tracking initialized ($9.15 credit remaining)\n",
      "üì¶ introdl v1.6.46 ready\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Lesson_10_Helpers import (display_ner_html, display_pipeline_ner_html, format_ner_eval_results, \n",
    "                                evaluate_ner, extract_gold_entities, predict_ner_tags)\n",
    "from introdl import (config_paths_keys, wrap_print_text, llm_generate, Trainer)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate # Hugging Face library for evaluation\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          TrainingArguments, DataCollatorForTokenClassification,\n",
    "                          pipeline)\n",
    "\n",
    "print = wrap_print_text(print, width=120)\n",
    "\n",
    "paths = config_paths_keys()\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "DATA_PATH = paths['DATA_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset - CoNLL2003 for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our examples, well use the [CoNLL2003 dataset](https://www.clips.uantwerpen.be/conll2003/ner/).  It is one of the first widely used benchmarks for Named Entity Recognition (NER). It was introduced as part of the [CoNLL-2003 shared task](https://aclanthology.org/W03-0419.pdf) and contains annotated text for four entity types: **PER** (person), **LOC** (location), **ORG** (organization), and **MISC** (miscellaneous). The dataset is derived from Reuters news articles and is structured in the BIO format, making it a standard for evaluating NER models.  BIO format marks the (B) beginning token, (I) inside tokens, and (O) outside tokens of an entity.  For example, 'B-PER' would be the first token in the name of a person, 'I-PER' would tag subsequent tokens in the name, while 'O' tags any tokens that are not part of an entity -- similar to labeling background pixels in image segmentation.\n",
    "\n",
    "Multiple versions of the dataset are available in Hugging Face.  We chose \"tomaarsen/conll2003\" because the NER tags are available in BIO format and because the list of possible labels is easy to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible BIO tags ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "# Load CoNLL2003 dataset (using parquet version for compatibility with datasets 4.0+)\n",
    "dataset = load_dataset(\"tomaarsen/conll2003\", revision=\"refs/convert/parquet\")\n",
    "BIO_tags_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(\"Possible BIO tags\", BIO_tags_list)\n",
    "\n",
    "# delete the pos_tags and chunk_tags columns, as we don't need them\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns([\"pos_tags\", \"chunk_tags\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the dataset consists of a single sentence or headline.  Here is how it's stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '12', 'document_id': 1, 'sentence_id': 12, 'tokens': ['Only', 'France', 'and', 'Britain', 'backed', 'Fischler',\n",
      "\"'s\", 'proposal', '.'], 'ner_tags': [0, 5, 0, 5, 0, 1, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokens are the words in sentence split up by whitespace and punctuation.  The ner_tags correspond to indices of the entity tags in our list.  The next bit of code also shows you how to get the BIO tags corresponding to each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Tokens",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "NER Tags (IDs)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BIO Tags",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "70914d75-3b0c-44ce-bf5d-6ef50e00f896",
       "rows": [
        [
         "0",
         "Only",
         "0",
         "O"
        ],
        [
         "1",
         "France",
         "5",
         "B-LOC"
        ],
        [
         "2",
         "and",
         "0",
         "O"
        ],
        [
         "3",
         "Britain",
         "5",
         "B-LOC"
        ],
        [
         "4",
         "backed",
         "0",
         "O"
        ],
        [
         "5",
         "Fischler",
         "1",
         "B-PER"
        ],
        [
         "6",
         "'s",
         "0",
         "O"
        ],
        [
         "7",
         "proposal",
         "0",
         "O"
        ],
        [
         "8",
         ".",
         "0",
         "O"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>NER Tags (IDs)</th>\n",
       "      <th>BIO Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>5</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Britain</td>\n",
       "      <td>5</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>backed</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fischler</td>\n",
       "      <td>1</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'s</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>proposal</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tokens  NER Tags (IDs) BIO Tags\n",
       "0      Only               0        O\n",
       "1    France               5    B-LOC\n",
       "2       and               0        O\n",
       "3   Britain               5    B-LOC\n",
       "4    backed               0        O\n",
       "5  Fischler               1    B-PER\n",
       "6        's               0        O\n",
       "7  proposal               0        O\n",
       "8         .               0        O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract tokens and ner_tags from dataset[\"train\"][12]\n",
    "tokens = dataset[\"train\"][12][\"tokens\"]\n",
    "ner_tags = dataset[\"train\"][12][\"ner_tags\"]\n",
    "\n",
    "# Map ner_tags to their corresponding BIO tags using label_list\n",
    "bio_tags = [BIO_tags_list[tag] for tag in ner_tags]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"Tokens\": tokens, \"NER Tags (IDs)\": ner_tags, \"BIO Tags\": bio_tags})\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy is a whole ecosystem](https://spacy.io/) of tools for NLP that we won't really dive into much in this course, but it's worth a look if you're going to be working in this area.  They provide some great tools for visualization of tagged text.  We've use their package to make a little function called `display_ner_html` which takes lists of tokens, tag IDs, and the list of labels to produce HTML visualizations of the tags.  The function is in helper.py if you're curious.  Here's how we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Only \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    France\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       " backed \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fischler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " 's proposal .</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokens and ner_tags were defined in the previous code cell\n",
    "\n",
    "display_ner_html(tokens, ner_tags, BIO_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Germany\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       " 's representative to the \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    European\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e5c494; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Union\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       " 's veterinary committee \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Werner\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #b3b3b3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zwingmann\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       " said on Wednesday consumers should buy sheepmeat from countries other than \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       " until the scientific advice was clearer .</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here's another example\n",
    "display_ner_html(dataset[\"train\"][4][\"tokens\"], dataset[\"train\"][4][\"ner_tags\"], BIO_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune DistilBERT for ConNLL2003\n",
    "\n",
    "**NOTE:** I've updated this section since Spring 2025 and will make a new video if time allows.  The main changes:\n",
    "- Use of our introdl.Trainer class which enables pretend_train (otherwise it's just like tranformers.Trainer)\n",
    "- Now we're using a `pipeline` to make inferences on new texts.  This simplifies using a trained model greatly.  \n",
    "- Provided a new \"helper\" function that takes the output of `pipeline` and produces dictionaries with the extracted entities.\n",
    "\n",
    "#### L10_1_Fine-tune_BERT Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l10_1_fine-tune_bert/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l10_1_fine-tune_bert/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/EgBF1mreyjw\" target=\"_blank\">Open Descript version of video in new tab</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to fine-tune a BERT model so that it can provide similar tagging for new text.  First we'll load a model and its tokenizer.\n",
    "`distilbert-base-cased` is a smaller, faster, and lighter version of BERT that retains 97% of its language understanding capabilities while being 40% smaller. It is case-sensitive, meaning it distinguishes between \"Apple\" and \"apple\" which is useful for NER tasks. It was trained using masked language modeling on the same data as BERT, including the English Wikipedia and BookCorpus, but with a reduced architecture to improve efficiency. \n",
    "\n",
    "In practice, you might choose a more recent variation on a BERT model for NER tasks.  For example, the DeBERTa-v3 will typically beat BERT and RoBERTa for NER.  We focus on DistilBERT in the lessons so you can see how things work while using a smaller model for faster training.\n",
    "\n",
    "Note that we make use of `AutoModelForTokenClassification` which adds a classification head to the backbone the same way we did for transfer learning applications in image classification.  The backbone uses pretrained weights while the classification head weights are randomly initialized and learned during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(BIO_tags_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the main issues we'll need to deal with is to map the BIO tags to the tokens that are produced by tokenizer that comes with our selected BERT model.  That tokenizer will break some of our words into subwords.  For those subwords we'll introduce an ID of -100 that tells the model not to predict tags for those tokens.\n",
    "\n",
    "We're using the `tokenize_and_align` labels function from the [Hugging Face tutorial on NER](https://huggingface.co/learn/llm-course/en/chapter7/2) to align the BIO ID tags from the input sequence in the dataset to the output tokens in the tokenizer.  We've included some extra comments in the code if you want to study it, or you can use an AI to help you walk through the details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to align labels with tokens\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Tokenize the input text (list of tokens) while keeping track of word-to-token alignment\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    # Initialize a list to store the aligned labels for each example\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over each example in the batch\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        # Get the word-to-token mapping for the current example\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        # Initialize variables to track the previous word index and the label IDs\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        # Iterate over the word IDs corresponding to the tokens\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # If the token is a special token (e.g., [CLS], [SEP]), ignore it by assigning -100\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # If the token corresponds to a new word, assign the label of that word\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # If the token is part of the same word (e.g., subword tokens), ignore it by assigning -100\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            # Update the previous word index to the current one\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # Append the aligned label IDs for the current example\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    # Add the aligned labels to the tokenized inputs\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    # Return the tokenized inputs with aligned labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell demonstrates how our tokenizer works the alignment function to get the tokenization expected by the model and to introduce IDs of -100 for each of the subwords introduced by the tokenizer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before model tokenization:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">He said a proposal last month by \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    EU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " Farm Commissioner \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Franz\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #b3b3b3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fischler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       " to ban sheep brains , spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health .</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After model tokenization:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [CLS]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " He said a proposal last month by \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    EU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " Farm Commissioner \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Franz\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #b3b3b3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##sch\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " to ban sheep brains , s \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ens\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " and spinal cord \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " from the human and animal food chains was a highly specific and pre \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ca\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ution\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ary\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " move to protect human health . \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [SEP]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       "</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the example\n",
    "example = dataset[\"train\"][7]\n",
    "\n",
    "# Wrap in a batch of one for compatibility with tokenize_and_align_labels\n",
    "batch = {\"tokens\": [example[\"tokens\"]], \"ner_tags\": [example[\"ner_tags\"]]}\n",
    "\n",
    "# Apply the tokenization and alignment function\n",
    "tokenized = tokenize_and_align_labels(batch)\n",
    "\n",
    "# Extract and display results\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0])\n",
    "labels = tokenized[\"labels\"][0]\n",
    "\n",
    "print((\"Before model tokenization:\\n\"))\n",
    "display_ner_html(dataset[\"train\"][7][\"tokens\"], dataset[\"train\"][7][\"ner_tags\"], BIO_tags_list)\n",
    "print((\"\\nAfter model tokenization:\\n\"))\n",
    "display_ner_html(tokens, labels, BIO_tags_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the tokenizer divided some of the original words into subwords which get assigned an ID of -100 to be ignored by the model.  During training those tokens are ignored by the loss function and the outputs corresponding to those tokens are ignored during model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fine-tune the model we define a custom metrics function that does two things:\n",
    "1. Uses the `seqeval` package to evaluate entire entity spans (e.g, e.g., `B-LOC`, `I-LOC`, etc. forming `\"New York\"`) instead of evaluating individual labels as we'd do with the scikit-learn metrics.\n",
    "2. Ignores the tokens with IDs of -100 for the evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Note if you have a different list of possible tags, you'll need to change the default value of label_list\n",
    "def compute_metrics(p, label_list=BIO_tags_list):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual fine-tuning we use a similar setup to what we did for text classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Checking HuggingFace Hub: hobbes99/DS776-models/distilbert-ner\n",
      "‚úì Model cached locally to: Lesson_10_Models/distilbert-ner/best_model\n",
      "Model already trained. Loading checkpoint...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dc01582bba4a82b2324fa14c16e2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training History:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Epoch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Training Loss",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Validation Loss",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Loc",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Misc",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Org",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Per",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Overall Precision",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Overall Recall",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Overall F1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Overall Accuracy",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "597bd38d-6443-4d05-b215-73cfd139c541",
       "rows": [
        [
         "0",
         "1",
         "",
         "0.05580490455031395",
         "{'precision': 0.926777, 'recall': 0.943930, 'f1': 0.935275, 'number': 1837}",
         "{'precision': 0.835373, 'recall': 0.814534, 'f1': 0.824822, 'number': 922}",
         "{'precision': 0.854328, 'recall': 0.905295, 'f1': 0.879073, 'number': 1341}",
         "{'precision': 0.972254, 'recall': 0.932139, 'f1': 0.951774, 'number': 1842}",
         "0.909182",
         "0.911478",
         "0.910329",
         "0.984249"
        ],
        [
         "1",
         "2",
         "",
         "0.04618348181247711",
         "{'precision': 0.960925, 'recall': 0.950463, 'f1': 0.955665, 'number': 1837}",
         "{'precision': 0.833161, 'recall': 0.872017, 'f1': 0.852146, 'number': 922}",
         "{'precision': 0.902295, 'recall': 0.909023, 'f1': 0.905646, 'number': 1341}",
         "{'precision': 0.965761, 'recall': 0.964712, 'f1': 0.965236, 'number': 1842}",
         "0.928512",
         "0.933356",
         "0.930927",
         "0.988299"
        ],
        [
         "2",
         "3",
         "",
         "0.04365767911076546",
         "{'precision': 0.963247, 'recall': 0.955906, 'f1': 0.959563, 'number': 1837}",
         "{'precision': 0.840292, 'recall': 0.873102, 'f1': 0.856383, 'number': 922}",
         "{'precision': 0.895849, 'recall': 0.917226, 'f1': 0.906411, 'number': 1341}",
         "{'precision': 0.965368, 'recall': 0.968512, 'f1': 0.966938, 'number': 1842}",
         "0.928857",
         "0.938236",
         "0.933523",
         "0.989019"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Misc</th>\n",
       "      <th>Org</th>\n",
       "      <th>Per</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.055805</td>\n",
       "      <td>{'precision': 0.926777, 'recall': 0.943930, 'f...</td>\n",
       "      <td>{'precision': 0.835373, 'recall': 0.814534, 'f...</td>\n",
       "      <td>{'precision': 0.854328, 'recall': 0.905295, 'f...</td>\n",
       "      <td>{'precision': 0.972254, 'recall': 0.932139, 'f...</td>\n",
       "      <td>0.909182</td>\n",
       "      <td>0.911478</td>\n",
       "      <td>0.910329</td>\n",
       "      <td>0.984249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0.046183</td>\n",
       "      <td>{'precision': 0.960925, 'recall': 0.950463, 'f...</td>\n",
       "      <td>{'precision': 0.833161, 'recall': 0.872017, 'f...</td>\n",
       "      <td>{'precision': 0.902295, 'recall': 0.909023, 'f...</td>\n",
       "      <td>{'precision': 0.965761, 'recall': 0.964712, 'f...</td>\n",
       "      <td>0.928512</td>\n",
       "      <td>0.933356</td>\n",
       "      <td>0.930927</td>\n",
       "      <td>0.988299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0.043658</td>\n",
       "      <td>{'precision': 0.963247, 'recall': 0.955906, 'f...</td>\n",
       "      <td>{'precision': 0.840292, 'recall': 0.873102, 'f...</td>\n",
       "      <td>{'precision': 0.895849, 'recall': 0.917226, 'f...</td>\n",
       "      <td>{'precision': 0.965368, 'recall': 0.968512, 'f...</td>\n",
       "      <td>0.928857</td>\n",
       "      <td>0.938236</td>\n",
       "      <td>0.933523</td>\n",
       "      <td>0.989019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch Training Loss  Validation Loss  \\\n",
       "0      1                       0.055805   \n",
       "1      2                       0.046183   \n",
       "2      3                       0.043658   \n",
       "\n",
       "                                                 Loc  \\\n",
       "0  {'precision': 0.926777, 'recall': 0.943930, 'f...   \n",
       "1  {'precision': 0.960925, 'recall': 0.950463, 'f...   \n",
       "2  {'precision': 0.963247, 'recall': 0.955906, 'f...   \n",
       "\n",
       "                                                Misc  \\\n",
       "0  {'precision': 0.835373, 'recall': 0.814534, 'f...   \n",
       "1  {'precision': 0.833161, 'recall': 0.872017, 'f...   \n",
       "2  {'precision': 0.840292, 'recall': 0.873102, 'f...   \n",
       "\n",
       "                                                 Org  \\\n",
       "0  {'precision': 0.854328, 'recall': 0.905295, 'f...   \n",
       "1  {'precision': 0.902295, 'recall': 0.909023, 'f...   \n",
       "2  {'precision': 0.895849, 'recall': 0.917226, 'f...   \n",
       "\n",
       "                                                 Per Overall Precision  \\\n",
       "0  {'precision': 0.972254, 'recall': 0.932139, 'f...          0.909182   \n",
       "1  {'precision': 0.965761, 'recall': 0.964712, 'f...          0.928512   \n",
       "2  {'precision': 0.965368, 'recall': 0.968512, 'f...          0.928857   \n",
       "\n",
       "  Overall Recall Overall F1 Overall Accuracy  \n",
       "0       0.911478   0.910329         0.984249  \n",
       "1       0.933356   0.930927         0.988299  \n",
       "2       0.938236   0.933523         0.989019  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Best model: Epoch 3 | Overall F1: 0.9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=0, training_loss=0.0, metrics={})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= MODELS_PATH / \"distilbert-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=1,  # Only keep the best model\n",
    "    load_best_model_at_end=True,  # Load best model at end of training\n",
    "    metric_for_best_model=\"eval_overall_f1\",  # Use F1 as the best model metric\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Trainer setup with pretend_train mode\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    pretend_train=True,  # Enable smart loading: local ‚Üí train from scratch\n",
    ")\n",
    "\n",
    "# Train the model (or load if already trained)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions and Evaluating Performance\n",
    "\n",
    "Of course, we have the metrics for the validation set shown in the training output above.  However, we can use the trainer's predict method to make predictions on any dataset, including the test set, and pass them to compute_metrics like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set evaluation results:\n",
      "{'LOC': {'precision': 0.9195402298850575, 'recall': 0.9112709832134293, 'f1': 0.9153869316470943, 'number': 1668},\n",
      "'MISC': {'precision': 0.7235142118863049, 'recall': 0.7977207977207977, 'f1': 0.7588075880758809, 'number': 702}, 'ORG':\n",
      "{'precision': 0.8451352907311457, 'recall': 0.8838049367850692, 'f1': 0.8640376692171866, 'number': 1661}, 'PER':\n",
      "{'precision': 0.961875, 'recall': 0.9517625231910947, 'f1': 0.9567920422754119, 'number': 1617}, 'overall_precision':\n",
      "0.8825468424705066, 'overall_recall': 0.9006728045325779, 'overall_f1': 0.8915177006659657, 'overall_accuracy':\n",
      "0.9780122752234306}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test set using predict (compatible with pretend_train)\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "\n",
    "# Extract predictions and labels\n",
    "pred_logits = predictions.predictions\n",
    "pred_labels = np.argmax(pred_logits, axis=2)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Compute metrics using the same compute_metrics function\n",
    "results_BERT = compute_metrics((pred_logits, true_labels))\n",
    "\n",
    "print(\"\\nTest set evaluation results:\")\n",
    "print(results_BERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary of metrics certainly isn't very pretty, but all the metrics are there.  You can display it as pandas.DataFrame or you could use our little utility `Lesson_10_Helpers.format_ner_eval_results` (or write your own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "LOC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MISC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ORG",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PER",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "overall_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "overall_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "overall_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "overall_accuracy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b888cc7d-9aa1-46b7-ad45-f31eebb4696a",
       "rows": [
        [
         "precision",
         "0.9195402298850575",
         "0.7235142118863049",
         "0.8451352907311457",
         "0.961875",
         "0.8825468424705066",
         "0.9006728045325779",
         "0.8915177006659657",
         "0.9780122752234306"
        ],
        [
         "recall",
         "0.9112709832134293",
         "0.7977207977207977",
         "0.8838049367850692",
         "0.9517625231910947",
         "0.8825468424705066",
         "0.9006728045325779",
         "0.8915177006659657",
         "0.9780122752234306"
        ],
        [
         "f1",
         "0.9153869316470943",
         "0.7588075880758809",
         "0.8640376692171866",
         "0.9567920422754119",
         "0.8825468424705066",
         "0.9006728045325779",
         "0.8915177006659657",
         "0.9780122752234306"
        ],
        [
         "number",
         "1668.0",
         "702.0",
         "1661.0",
         "1617.0",
         "0.8825468424705066",
         "0.9006728045325779",
         "0.8915177006659657",
         "0.9780122752234306"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "      <th>overall_precision</th>\n",
       "      <th>overall_recall</th>\n",
       "      <th>overall_f1</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.723514</td>\n",
       "      <td>0.845135</td>\n",
       "      <td>0.961875</td>\n",
       "      <td>0.882547</td>\n",
       "      <td>0.900673</td>\n",
       "      <td>0.891518</td>\n",
       "      <td>0.978012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.911271</td>\n",
       "      <td>0.797721</td>\n",
       "      <td>0.883805</td>\n",
       "      <td>0.951763</td>\n",
       "      <td>0.882547</td>\n",
       "      <td>0.900673</td>\n",
       "      <td>0.891518</td>\n",
       "      <td>0.978012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.915387</td>\n",
       "      <td>0.758808</td>\n",
       "      <td>0.864038</td>\n",
       "      <td>0.956792</td>\n",
       "      <td>0.882547</td>\n",
       "      <td>0.900673</td>\n",
       "      <td>0.891518</td>\n",
       "      <td>0.978012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>1668.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1661.000000</td>\n",
       "      <td>1617.000000</td>\n",
       "      <td>0.882547</td>\n",
       "      <td>0.900673</td>\n",
       "      <td>0.891518</td>\n",
       "      <td>0.978012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   LOC        MISC          ORG          PER  \\\n",
       "precision     0.919540    0.723514     0.845135     0.961875   \n",
       "recall        0.911271    0.797721     0.883805     0.951763   \n",
       "f1            0.915387    0.758808     0.864038     0.956792   \n",
       "number     1668.000000  702.000000  1661.000000  1617.000000   \n",
       "\n",
       "           overall_precision  overall_recall  overall_f1  overall_accuracy  \n",
       "precision           0.882547        0.900673    0.891518          0.978012  \n",
       "recall              0.882547        0.900673    0.891518          0.978012  \n",
       "f1                  0.882547        0.900673    0.891518          0.978012  \n",
       "number              0.882547        0.900673    0.891518          0.978012  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Entity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2c089199-7375-4792-bd16-a904fc5f5724",
       "rows": [
        [
         "0",
         "LOC",
         "0.9195",
         "0.9113",
         "0.9154",
         "1668.0",
         null
        ],
        [
         "1",
         "MISC",
         "0.7235",
         "0.7977",
         "0.7588",
         "702.0",
         null
        ],
        [
         "2",
         "ORG",
         "0.8451",
         "0.8838",
         "0.864",
         "1661.0",
         null
        ],
        [
         "3",
         "PER",
         "0.9619",
         "0.9518",
         "0.9568",
         "1617.0",
         null
        ],
        [
         "4",
         "Overall",
         "0.8825",
         "0.9007",
         "0.8915",
         null,
         "0.978"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Number</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.9195</td>\n",
       "      <td>0.9113</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>0.7977</td>\n",
       "      <td>0.7588</td>\n",
       "      <td>702.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.8838</td>\n",
       "      <td>0.8640</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overall</td>\n",
       "      <td>0.8825</td>\n",
       "      <td>0.9007</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entity  Precision  Recall      F1  Number  Accuracy\n",
       "0      LOC     0.9195  0.9113  0.9154  1668.0       NaN\n",
       "1     MISC     0.7235  0.7977  0.7588   702.0       NaN\n",
       "2      ORG     0.8451  0.8838  0.8640  1661.0       NaN\n",
       "3      PER     0.9619  0.9518  0.9568  1617.0       NaN\n",
       "4  Overall     0.8825  0.9007  0.8915     NaN     0.978"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pd.DataFrame converts nested dict to a flat table for easier viewing\n",
    "display(pd.DataFrame(results_BERT))\n",
    "\n",
    "# or make it prettier with our helper function\n",
    "display(format_ner_eval_results(results_BERT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the overall accuracy is almost 98%, which seems amazing, but this is including all the background words in the text.   We could get a very accurate model by classifying all words as background so accuracy isn't very meaningful here just like in image segmentation when we include background pixels.\n",
    "\n",
    "If you're looking for one number to quantify the performance of an NER model, use F1.  F1 is the harmonic mean (an equal blend) of precision and recall.  Particularly for imbalanced datasets, it is much better than accuracy.  In image segmentation **F1 = Dice Score**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can retrieve the evaluation metrics from training (the training loss isn't saved) for display or plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "epoch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "eval_loss",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eval_LOC",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "eval_MISC",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "eval_ORG",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "eval_PER",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "eval_overall_precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eval_overall_recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eval_overall_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eval_overall_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eval_runtime",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eval_samples_per_second",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eval_steps_per_second",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "4491f52e-e0ca-47c1-aca0-1deb50575407",
       "rows": [
        [
         "0",
         "1",
         "0.0558",
         "{'precision': 0.9267771245, 'recall': 0.9439303212000001, 'f1': 0.9352750809, 'number': 1837}",
         "{'precision': 0.8353726363, 'recall': 0.8145336226000001, 'f1': 0.8248215266000001, 'number': 922}",
         "{'precision': 0.8543279381000001, 'recall': 0.9052945563, 'f1': 0.8790731354, 'number': 1341}",
         "{'precision': 0.9722536806000001, 'recall': 0.9321389794, 'f1': 0.9517738359000001, 'number': 1842}",
         "0.9092",
         "0.9115000000000001",
         "0.9103",
         "0.9842000000000001",
         "2.1542",
         "1508.7",
         "94.7"
        ],
        [
         "1",
         "2",
         "0.046200000000000005",
         "{'precision': 0.9609246010000001, 'recall': 0.9504627109, 'f1': 0.9556650246, 'number': 1837}",
         "{'precision': 0.8331606218000001, 'recall': 0.8720173536, 'f1': 0.8521462639, 'number': 922}",
         "{'precision': 0.9022945966, 'recall': 0.9090231171, 'f1': 0.9056463596000001, 'number': 1341}",
         "{'precision': 0.9657608696000001, 'recall': 0.9647122693, 'f1': 0.9652362846, 'number': 1842}",
         "0.9285",
         "0.9334",
         "0.9309000000000001",
         "0.9883000000000001",
         "2.1495",
         "1511.989",
         "94.906"
        ],
        [
         "2",
         "3",
         "0.0437",
         "{'precision': 0.9632473944000001, 'recall': 0.9559063691, 'f1': 0.9595628415, 'number': 1837}",
         "{'precision': 0.8402922756000001, 'recall': 0.8731019523, 'f1': 0.8563829787, 'number': 922}",
         "{'precision': 0.8958485069000001, 'recall': 0.9172259508, 'f1': 0.9064112012000001, 'number': 1341}",
         "{'precision': 0.9653679654, 'recall': 0.9685124864, 'f1': 0.9669376694, 'number': 1842}",
         "0.9289000000000001",
         "0.9382",
         "0.9335",
         "0.989",
         "2.1323",
         "1524.208",
         "95.673"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_LOC</th>\n",
       "      <th>eval_MISC</th>\n",
       "      <th>eval_ORG</th>\n",
       "      <th>eval_PER</th>\n",
       "      <th>eval_overall_precision</th>\n",
       "      <th>eval_overall_recall</th>\n",
       "      <th>eval_overall_f1</th>\n",
       "      <th>eval_overall_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>{'precision': 0.9267771245, 'recall': 0.943930...</td>\n",
       "      <td>{'precision': 0.8353726363, 'recall': 0.814533...</td>\n",
       "      <td>{'precision': 0.8543279381000001, 'recall': 0....</td>\n",
       "      <td>{'precision': 0.9722536806000001, 'recall': 0....</td>\n",
       "      <td>0.9092</td>\n",
       "      <td>0.9115</td>\n",
       "      <td>0.9103</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>2.1542</td>\n",
       "      <td>1508.700</td>\n",
       "      <td>94.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>{'precision': 0.9609246010000001, 'recall': 0....</td>\n",
       "      <td>{'precision': 0.8331606218000001, 'recall': 0....</td>\n",
       "      <td>{'precision': 0.9022945966, 'recall': 0.909023...</td>\n",
       "      <td>{'precision': 0.9657608696000001, 'recall': 0....</td>\n",
       "      <td>0.9285</td>\n",
       "      <td>0.9334</td>\n",
       "      <td>0.9309</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>2.1495</td>\n",
       "      <td>1511.989</td>\n",
       "      <td>94.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>{'precision': 0.9632473944000001, 'recall': 0....</td>\n",
       "      <td>{'precision': 0.8402922756000001, 'recall': 0....</td>\n",
       "      <td>{'precision': 0.8958485069000001, 'recall': 0....</td>\n",
       "      <td>{'precision': 0.9653679654, 'recall': 0.968512...</td>\n",
       "      <td>0.9289</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>0.9335</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>2.1323</td>\n",
       "      <td>1524.208</td>\n",
       "      <td>95.673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  eval_loss                                           eval_LOC  \\\n",
       "0      1     0.0558  {'precision': 0.9267771245, 'recall': 0.943930...   \n",
       "1      2     0.0462  {'precision': 0.9609246010000001, 'recall': 0....   \n",
       "2      3     0.0437  {'precision': 0.9632473944000001, 'recall': 0....   \n",
       "\n",
       "                                           eval_MISC  \\\n",
       "0  {'precision': 0.8353726363, 'recall': 0.814533...   \n",
       "1  {'precision': 0.8331606218000001, 'recall': 0....   \n",
       "2  {'precision': 0.8402922756000001, 'recall': 0....   \n",
       "\n",
       "                                            eval_ORG  \\\n",
       "0  {'precision': 0.8543279381000001, 'recall': 0....   \n",
       "1  {'precision': 0.9022945966, 'recall': 0.909023...   \n",
       "2  {'precision': 0.8958485069000001, 'recall': 0....   \n",
       "\n",
       "                                            eval_PER  eval_overall_precision  \\\n",
       "0  {'precision': 0.9722536806000001, 'recall': 0....                  0.9092   \n",
       "1  {'precision': 0.9657608696000001, 'recall': 0....                  0.9285   \n",
       "2  {'precision': 0.9653679654, 'recall': 0.968512...                  0.9289   \n",
       "\n",
       "   eval_overall_recall  eval_overall_f1  eval_overall_accuracy  eval_runtime  \\\n",
       "0               0.9115           0.9103                 0.9842        2.1542   \n",
       "1               0.9334           0.9309                 0.9883        2.1495   \n",
       "2               0.9382           0.9335                 0.9890        2.1323   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  \n",
       "0                 1508.700                 94.700  \n",
       "1                 1511.989                 94.906  \n",
       "2                 1524.208                 95.673  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get training history as a DataFrame\n",
    "history_df = trainer.get_training_history()\n",
    "display(history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Best Model for Inference\n",
    "\n",
    "Now we'll use our fine-tuned model to make predictions on new text. Rather than using `trainer.predict`, we'll use `pipeline` like you've seen in recent lessons. The HuggingFace `pipeline` provides a simple, industry-standard interface for NER inference. We'll explore different configuration options and demonstrate how to extract entities in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to our best trained model\n",
    "best_model_path = MODELS_PATH / \"distilbert-ner\" / \"best_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text for demonstration\n",
    "example_text = \"\"\"\n",
    "It's only been a day since ChatGPT's new AI image generator went live, \n",
    "and social media feeds are already flooded with AI-generated memes in the style of Studio Ghibli, \n",
    "the cult-favorite Japanese animation studio behind blockbuster films such as \"My Neighbor Totoro\" and \"Spirited Away.\"\n",
    "\n",
    "In the last 24 hours, we've seen AI-generated images representing Studio Ghibli versions of Elon Musk, \n",
    "\"The Lord of the Rings\", and President Donald Trump. OpenAI CEO Sam Altman even seems to have made his new \n",
    "profile picture a Studio Ghibli-style image, presumably made with GPT-4o's native image generator. Users seem to be \n",
    "uploading existing images and pictures into ChatGPT and asking the chatbot to re-create it in new styles.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Aggregation Strategies\n",
    "\n",
    "The `pipeline` function supports different `aggregation_strategy` options that control how subword tokens are combined:\n",
    "\n",
    "- **`\"simple\"`** (recommended): Groups consecutive tokens with the same entity type into spans\n",
    "- **`\"first\"`**: Uses only the first subword token's prediction for each word\n",
    "- **`\"average\"`**: Averages confidence scores across all subword tokens\n",
    "- **`\"max\"`**: Takes the maximum confidence score across subword tokens\n",
    "- **`None`**: Returns raw token-level predictions without grouping (every subword gets a separate prediction)\n",
    "\n",
    "Using no strategy returns a label for every subtoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 183 individual tokens\n",
      "\n",
      "First 5 tokens:\n",
      "[{'entity': 'LABEL_0', 'score': 0.9997923, 'index': 1, 'word': 'It', 'start': 1, 'end': 3}, {'entity': 'LABEL_0',\n",
      "'score': 0.9998636, 'index': 2, 'word': \"'\", 'start': 3, 'end': 4}, {'entity': 'LABEL_0', 'score': 0.999912, 'index': 3,\n",
      "'word': 's', 'start': 4, 'end': 5}, {'entity': 'LABEL_0', 'score': 0.999918, 'index': 4, 'word': 'only', 'start': 6,\n",
      "'end': 10}, {'entity': 'LABEL_0', 'score': 0.99992347, 'index': 5, 'word': 'been', 'start': 11, 'end': 15}]\n",
      "\n",
      "Or displayed as a table:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "end",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "0f6ef50d-ce34-4351-a635-7e0ba41b3474",
       "rows": [
        [
         "0",
         "LABEL_0",
         "0.9997923",
         "1",
         "It",
         "1",
         "3"
        ],
        [
         "1",
         "LABEL_0",
         "0.9998636",
         "2",
         "'",
         "3",
         "4"
        ],
        [
         "2",
         "LABEL_0",
         "0.999912",
         "3",
         "s",
         "4",
         "5"
        ],
        [
         "3",
         "LABEL_0",
         "0.999918",
         "4",
         "only",
         "6",
         "10"
        ],
        [
         "4",
         "LABEL_0",
         "0.99992347",
         "5",
         "been",
         "11",
         "15"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>score</th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999792</td>\n",
       "      <td>1</td>\n",
       "      <td>It</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>2</td>\n",
       "      <td>'</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>3</td>\n",
       "      <td>s</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>4</td>\n",
       "      <td>only</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>5</td>\n",
       "      <td>been</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity     score  index  word  start  end\n",
       "0  LABEL_0  0.999792      1    It      1    3\n",
       "1  LABEL_0  0.999864      2     '      3    4\n",
       "2  LABEL_0  0.999912      3     s      4    5\n",
       "3  LABEL_0  0.999918      4  only      6   10\n",
       "4  LABEL_0  0.999923      5  been     11   15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pipeline with no aggregation (not recommended)\n",
    "ner_pipeline = pipeline(\"token-classification\", model=best_model_path, aggregation_strategy=None)\n",
    "\n",
    "# Make predictions\n",
    "results_raw = ner_pipeline(example_text)\n",
    "print(f\"Found {len(results_raw)} individual tokens\")\n",
    "print(\"\\nFirst 5 tokens:\")\n",
    "print(results_raw[:5])\n",
    "\n",
    "print(\"\\nOr displayed as a table:\")\n",
    "display(pd.DataFrame(results_raw[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use that to extract the entities we'd have to do quite a bit of post-processing.  All of these methods will required us map the entity labels like \"LABEL_0\" to the appropriate BIO ID tags.\n",
    "\n",
    "The \"simple\" aggregation strategy merges sub-tokens but it does it in a literal way so that \"ChatGPT\" becomes \"Chat##t##GPT\" which isn't so helpful (this is due to the way subwords are tokenized).  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 entity spans\n",
      "\n",
      "First 5 results:\n",
      "[{'entity_group': 'LABEL_0', 'score': 0.9998968, 'word': \"It ' s only been a day since\", 'start': 1, 'end': 27},\n",
      "{'entity_group': 'LABEL_3', 'score': 0.98500407, 'word': 'Cha', 'start': 28, 'end': 31}, {'entity_group': 'LABEL_0',\n",
      "'score': 0.7621933, 'word': '##t', 'start': 31, 'end': 32}, {'entity_group': 'LABEL_4', 'score': 0.9892545, 'word':\n",
      "'##GPT', 'start': 32, 'end': 35}, {'entity_group': 'LABEL_0', 'score': 0.99977636, 'word': \"' s new\", 'start': 35,\n",
      "'end': 41}]\n",
      "\n",
      "Or displayed as a table:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity_group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "end",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1a3da4cd-af8c-4841-b5cc-814335eb5bfc",
       "rows": [
        [
         "0",
         "LABEL_0",
         "0.9998968",
         "It ' s only been a day since",
         "1",
         "27"
        ],
        [
         "1",
         "LABEL_3",
         "0.98500407",
         "Cha",
         "28",
         "31"
        ],
        [
         "2",
         "LABEL_0",
         "0.7621933",
         "##t",
         "31",
         "32"
        ],
        [
         "3",
         "LABEL_4",
         "0.9892545",
         "##GPT",
         "32",
         "35"
        ],
        [
         "4",
         "LABEL_0",
         "0.99977636",
         "' s new",
         "35",
         "41"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>It ' s only been a day since</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.985004</td>\n",
       "      <td>Cha</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.762193</td>\n",
       "      <td>##t</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>0.989254</td>\n",
       "      <td>##GPT</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>' s new</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                          word  start  end\n",
       "0      LABEL_0  0.999897  It ' s only been a day since      1   27\n",
       "1      LABEL_3  0.985004                           Cha     28   31\n",
       "2      LABEL_0  0.762193                           ##t     31   32\n",
       "3      LABEL_4  0.989254                         ##GPT     32   35\n",
       "4      LABEL_0  0.999776                       ' s new     35   41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare with simple aggregation (better, but still needs some post-processing)\n",
    "ner_pipeline_simple = pipeline(\"token-classification\", model=best_model_path, aggregation_strategy=\"simple\")\n",
    "\n",
    "results_simple = ner_pipeline_simple(example_text)\n",
    "print(f\"Found {len(results_simple)} entity spans\")\n",
    "print(\"\\nFirst 5 results:\")\n",
    "print(results_simple[:5])\n",
    "\n",
    "print(\"\\nOr displayed as a table:\")\n",
    "display(pd.DataFrame(results_simple[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could process the output to contruct the complete entities, but a far simpler way is to use the \"first\" aggregation strategy which merges the entities belong to the same groups and merges the subtokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 entity spans\n",
      "\n",
      "First 5 results:\n",
      "[{'entity_group': 'LABEL_0', 'score': 0.9998968, 'word': \"It ' s only been a day since\", 'start': 1, 'end': 27},\n",
      "{'entity_group': 'LABEL_3', 'score': 0.98500407, 'word': 'ChatGPT', 'start': 28, 'end': 35}, {'entity_group': 'LABEL_0',\n",
      "'score': 0.99977636, 'word': \"' s new\", 'start': 35, 'end': 41}, {'entity_group': 'LABEL_7', 'score': 0.95365113,\n",
      "'word': 'AI', 'start': 42, 'end': 44}, {'entity_group': 'LABEL_0', 'score': 0.9998733, 'word': 'image generator went\n",
      "live, and social media feeds are already flooded with', 'start': 45, 'end': 120}]\n",
      "\n",
      "Or displayed as a table:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity_group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "end",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "fc6c678a-6c61-4b24-8c5b-1d1717a377dd",
       "rows": [
        [
         "0",
         "LABEL_0",
         "0.9998968",
         "It ' s only been a day since",
         "1",
         "27"
        ],
        [
         "1",
         "LABEL_3",
         "0.98500407",
         "ChatGPT",
         "28",
         "35"
        ],
        [
         "2",
         "LABEL_0",
         "0.99977636",
         "' s new",
         "35",
         "41"
        ],
        [
         "3",
         "LABEL_7",
         "0.95365113",
         "AI",
         "42",
         "44"
        ],
        [
         "4",
         "LABEL_0",
         "0.9998733",
         "image generator went live, and social media feeds are already flooded with",
         "45",
         "120"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>It ' s only been a day since</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.985004</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>' s new</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LABEL_7</td>\n",
       "      <td>0.953651</td>\n",
       "      <td>AI</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999873</td>\n",
       "      <td>image generator went live, and social media fe...</td>\n",
       "      <td>45</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                                               word  \\\n",
       "0      LABEL_0  0.999897                       It ' s only been a day since   \n",
       "1      LABEL_3  0.985004                                            ChatGPT   \n",
       "2      LABEL_0  0.999776                                            ' s new   \n",
       "3      LABEL_7  0.953651                                                 AI   \n",
       "4      LABEL_0  0.999873  image generator went live, and social media fe...   \n",
       "\n",
       "   start  end  \n",
       "0      1   27  \n",
       "1     28   35  \n",
       "2     35   41  \n",
       "3     42   44  \n",
       "4     45  120  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pipeline with 'first' aggregation (recommended for entity extraction)\n",
    "ner_pipeline_first = pipeline(\"token-classification\", model=best_model_path, aggregation_strategy=\"first\")\n",
    "\n",
    "# Make predictions\n",
    "results_first = ner_pipeline_first(example_text)\n",
    "print(f\"Found {len(results_first)} entity spans\")\n",
    "print(\"\\nFirst 5 results:\")\n",
    "print(results_first[:5])\n",
    "\n",
    "print(\"\\nOr displayed as a table:\")\n",
    "display(pd.DataFrame(results_first[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `aggregation_strategy=\"first\"` groups related tokens into entity spans and merges subtokens.  This makes it relatively easy to extract entities from the text because we usually don't need to do any (or at least much) post-processing.  The \"first\", \"max\", and \"average\" only differ in how they assign the confidence score to each merged group, but the extracted entities are the same.  We'll use the \"first\" aggregation strategy results in the remainder of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>It's only been a day since \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ChatGPT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       "'s new \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " image generator went live, <br>and social media feeds are already flooded with \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       "-generated memes in the style of \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #a6d854; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ghibli\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       ", <br>the cult-favorite \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Japanese\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " animation studio behind blockbuster films such as &quot;\n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    My\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Neighbor Totoro\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
       "</mark>\n",
       "&quot; and &quot;\n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Spirited\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Away\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
       "</mark>\n",
       ".&quot;<br><br>In the last 24 hours, we've seen \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       "-generated images representing \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ghibli\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
       "</mark>\n",
       " versions of \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffd92f; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Musk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       ", <br>&quot;\n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lord of the Rings\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
       "</mark>\n",
       "&quot;, and President \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Donald\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffd92f; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    OpenAI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " CEO \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ffd92f; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Altman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       " even seems to have made his new <br>profile picture a \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ghibli\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
       "</mark>\n",
       "-style image, presumably made with \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GPT-4o\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       "'s native image generator. Users seem to be <br>uploading existing images and pictures into \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ChatGPT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " and asking the chatbot to re-create it in new styles.<br></div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the tagged text with colors\n",
    "display_pipeline_ner_html(example_text, results_first, BIO_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we'll still have to do a bit of processing to extract complete entities.  We'll have to merge each B-tag with the subsequent I-tags to get complete entities so that \"Elon\" + \"Musk\" becomes \"Elon Musk\", for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Entities by Type\n",
    "\n",
    "The pipeline returns results in a list format with entity positions. For many applications, we want to extract entities organized by type (PER, ORG, LOC, MISC) as a dictionary - similar to what LLMs return naturally (we'll see this below)\n",
    "\n",
    "Let's create a helper function to convert pipeline results to this format.  If you want to dive into this function to really understand it we encourage you to work through it, perhaps with the help of AI, to figure out how it works.  You can also import this function from Lesson_10_Helper to use in your homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_dict(pipeline_results, label_list):\n",
    "    \"\"\"\n",
    "    Convert pipeline results to dictionary (or list of dictionaries) organized by entity type.\n",
    "    \n",
    "    This function works with HuggingFace token classification pipelines to extract named entities\n",
    "    and organize them by type (PER, ORG, LOC, MISC). It properly merges multi-token entities\n",
    "    (e.g., \"Elon Musk\") by combining consecutive B- and I- tags.\n",
    "    \n",
    "    Args:\n",
    "        pipeline_results: Either:\n",
    "            - Single result: List of dicts from pipeline (one text)\n",
    "            - Batch results: List of lists of dicts from pipeline (multiple texts)\n",
    "        label_list: List of BIO tags (e.g., ['O', 'B-PER', 'I-PER', 'B-LOC', ...])\n",
    "        \n",
    "    Returns:\n",
    "        If single text input:\n",
    "            dict: {'PER': ['Elon Musk', 'Sam Altman'], 'ORG': ['OpenAI'], ...}\n",
    "        If batch input:\n",
    "            list of dict: [{'PER': [...], 'ORG': [...]}, {'PER': [...], 'LOC': [...]}, ...]\n",
    "    \n",
    "    Example:\n",
    "        >>> # Single text\n",
    "        >>> results = pipeline(\"Elon Musk founded OpenAI.\")\n",
    "        >>> extract_entities_dict(results, BIO_tags_list)\n",
    "        {'PER': ['Elon Musk'], 'ORG': ['OpenAI'], 'LOC': [], 'MISC': []}\n",
    "        \n",
    "        >>> # Batch of texts\n",
    "        >>> results = pipeline([\"Elon Musk lives in Texas.\", \"OpenAI is in San Francisco.\"])\n",
    "        >>> extract_entities_dict(results, BIO_tags_list)\n",
    "        [{'PER': ['Elon Musk'], 'LOC': ['Texas'], ...}, \n",
    "         {'ORG': ['OpenAI'], 'LOC': ['San Francisco'], ...}]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if input is batched (list of lists) or single (list of dicts)\n",
    "    # Batched: [[{result1}, {result2}], [{result3}, {result4}]]\n",
    "    # Single:  [{result1}, {result2}, {result3}]\n",
    "    is_batched = isinstance(pipeline_results[0], list) if pipeline_results else False\n",
    "    \n",
    "    # If batched, recursively process each text's results\n",
    "    if is_batched:\n",
    "        return [extract_entities_dict(single_result, label_list) \n",
    "                for single_result in pipeline_results]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Single text processing starts here\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Step 1: Initialize the output dictionary with all entity types\n",
    "    # This ensures every entity type has a key, even if no entities are found\n",
    "    entities = {}\n",
    "    for label in label_list:\n",
    "        if label != 'O':  # Skip 'O' which means \"Outside\" (no entity)\n",
    "            # Extract entity type from BIO tag: 'B-PER' -> 'PER'\n",
    "            entity_type = label.split('-')[-1]\n",
    "            if entity_type not in entities:\n",
    "                entities[entity_type] = []\n",
    "    \n",
    "    # Step 2: Track the current entity being built across multiple tokens\n",
    "    # Example: \"Elon\" (B-PER) + \"Musk\" (I-PER) = \"Elon Musk\" (complete entity)\n",
    "    current_entity_tokens = []  # Accumulates tokens for current entity\n",
    "    current_entity_type = None  # Tracks which entity type we're building\n",
    "    \n",
    "    # Step 3: Process each token from the pipeline results sequentially\n",
    "    # The pipeline returns results in text order, which is crucial for merging\n",
    "    for result in pipeline_results:\n",
    "        # Pipeline outputs entity_group as 'LABEL_X' where X is the label index\n",
    "        # Example: 'LABEL_3' means index 3 in label_list\n",
    "        entity_label = result['entity_group']\n",
    "        \n",
    "        # Convert 'LABEL_X' to the actual BIO tag string\n",
    "        if entity_label.startswith('LABEL_'):\n",
    "            label_idx = int(entity_label.replace('LABEL_', ''))\n",
    "            \n",
    "            # Look up the BIO tag (e.g., 'B-PER', 'I-LOC', 'O')\n",
    "            if label_idx < len(label_list):\n",
    "                bio_label = label_list[label_idx]\n",
    "                \n",
    "                # ----------------------------------------------------------------\n",
    "                # Handle 'O' (Outside) tags - marks end of entity\n",
    "                # ----------------------------------------------------------------\n",
    "                if bio_label == 'O':\n",
    "                    # If we were building an entity, save it now\n",
    "                    if current_entity_tokens:\n",
    "                        entity_text = ' '.join(current_entity_tokens).strip()\n",
    "                        # Avoid adding duplicates or empty strings\n",
    "                        if entity_text and entity_text not in entities[current_entity_type]:\n",
    "                            entities[current_entity_type].append(entity_text)\n",
    "                        # Reset state for next entity\n",
    "                        current_entity_tokens = []\n",
    "                        current_entity_type = None\n",
    "                    continue  # Move to next token\n",
    "                \n",
    "                # ----------------------------------------------------------------\n",
    "                # Extract entity information from BIO tag\n",
    "                # ----------------------------------------------------------------\n",
    "                entity_type = bio_label.split('-')[-1]  # 'B-PER' -> 'PER'\n",
    "                entity_text = result['word'].strip()    # Token text\n",
    "                \n",
    "                # ----------------------------------------------------------------\n",
    "                # Handle 'B-' (Beginning) tags - starts new entity\n",
    "                # ----------------------------------------------------------------\n",
    "                if bio_label.startswith('B-'):\n",
    "                    # Save the previous entity if we were building one\n",
    "                    if current_entity_tokens:\n",
    "                        complete_entity = ' '.join(current_entity_tokens).strip()\n",
    "                        if complete_entity and complete_entity not in entities[current_entity_type]:\n",
    "                            entities[current_entity_type].append(complete_entity)\n",
    "                    \n",
    "                    # Start building a new entity\n",
    "                    current_entity_tokens = [entity_text]\n",
    "                    current_entity_type = entity_type\n",
    "                \n",
    "                # ----------------------------------------------------------------\n",
    "                # Handle 'I-' (Inside) tags - continues current entity\n",
    "                # ----------------------------------------------------------------\n",
    "                elif bio_label.startswith('I-'):\n",
    "                    # Check if this I- tag matches the current entity type\n",
    "                    if current_entity_type == entity_type:\n",
    "                        # Add token to current entity\n",
    "                        # Example: current=['Elon'], adding 'Musk' -> ['Elon', 'Musk']\n",
    "                        current_entity_tokens.append(entity_text)\n",
    "                    else:\n",
    "                        # Mismatched I- tag (tagging error or special case)\n",
    "                        # Treat it as starting a new entity\n",
    "                        if current_entity_tokens:\n",
    "                            complete_entity = ' '.join(current_entity_tokens).strip()\n",
    "                            if complete_entity and complete_entity not in entities[current_entity_type]:\n",
    "                                entities[current_entity_type].append(complete_entity)\n",
    "                        # Start new entity with this token\n",
    "                        current_entity_tokens = [entity_text]\n",
    "                        current_entity_type = entity_type\n",
    "    \n",
    "    # Step 4: Don't forget the last entity if text ends while building one\n",
    "    # Example: \"... lives in New York\" - need to save \"New York\" at the end\n",
    "    if current_entity_tokens:\n",
    "        complete_entity = ' '.join(current_entity_tokens).strip()\n",
    "        if complete_entity and complete_entity not in entities[current_entity_type]:\n",
    "            entities[current_entity_type].append(complete_entity)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll apply our extraction function extract the entities from our our example text based on the \"first\" aggregation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities by type:\n",
      "{\n",
      "  \"PER\": [\n",
      "    \"Elon Musk\",\n",
      "    \"Donald Trump\",\n",
      "    \"Sam Altman\"\n",
      "  ],\n",
      "  \"ORG\": [\n",
      "    \"ChatGPT\",\n",
      "    \"Studio Ghibli\",\n",
      "    \"OpenAI\"\n",
      "  ],\n",
      "  \"LOC\": [],\n",
      "  \"MISC\": [\n",
      "    \"AI\",\n",
      "    \"Japanese\",\n",
      "    \"My Neighbor Totoro\",\n",
      "    \"Spirited Away\",\n",
      "    \"Studio Ghibli\",\n",
      "    \"The Lord of the Rings\",\n",
      "    \"GPT - 4o\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from our example\n",
    "entities_dict = extract_entities_dict(results_first, BIO_tags_list)\n",
    "\n",
    "print(\"Extracted entities by type:\")\n",
    "print(json.dumps(entities_dict, indent=2)) # Pretty print the dictionary, json was imported earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER by Zero-Shot LLM Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L10_1_LLM_NER Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l10_1_llm_ner/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l10_1_llm_ner/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/EgBF1mreyjw\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "In this section we'll explore using LLMs for NER.  LLMs can do this quite well, but there are some differences to be aware of though.  LLMs are naturally better at extracting spans (the relevant words for each identified entity) or structured output, not token-level labeling, because:\n",
    "\n",
    "* The process text holistically, not token-by-token.\n",
    "* There's no inherent token alignment.\n",
    "* They can hallucinate or skip tokens when generating lists.\n",
    "* The extracted spans may not exactly match the strings in the text, e.g. \"ChatGPT's\" gets extracted as \"ChatGPT\"\n",
    "\n",
    "When we use an LLM to extract entities, we'll get lists of spans of each type.  You'll need to prompt carefully:\n",
    "* try to get the LLM to extract the entities as they appear in the text\n",
    "* you may need to provide examples or explanations of the entity types\n",
    "\n",
    "When we evaluate the results, we won't be able to compare token by token as we did above for the output of our BERT model (that kind of evaluation is similar to evaluating semantic segmentation results where we can compare every pixel in the image to every pixel in the mask).  Instead we can just determine if each found each entity and whether it had the correct entity type.  It will help to use \"fuzzy\" matching which doesn't require exacty matching of strings to accout for misspellings and different presentations of words.\n",
    "\n",
    "**Note:**  It's possible to use an LLM to produce token-level tags for each token through a combination of careful prompting and post-processing, but we'll stick with the simpler problem of identifying entities without identifying their positions in the text which is adequate for many applications.\n",
    "\n",
    "We'll use `llm_generate` as we've done previously.    Here's the list of models that are easy to use with `llm_generate`.  You can adjust the code below to use other models, or the Groq or Together.AI APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities by type from LLM:\n",
      "{\n",
      "  \"PER\": [\n",
      "    \"Elon Musk\",\n",
      "    \"Donald Trump\",\n",
      "    \"Sam Altman\"\n",
      "  ],\n",
      "  \"ORG\": [\n",
      "    \"ChatGPT\",\n",
      "    \"OpenAI\"\n",
      "  ],\n",
      "  \"LOC\": [],\n",
      "  \"MISC\": [\n",
      "    \"AI\",\n",
      "    \"Studio Ghibli\",\n",
      "    \"My Neighbor Totoro\",\n",
      "    \"Spirited Away\",\n",
      "    \"The Lord of the Rings\",\n",
      "    \"GPT-4o\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Use gemini-flash-lite as default model\n",
    "model_name = \"gemini-flash-lite\"\n",
    "#model_name = \"mistral-medium\"\n",
    "\n",
    "# System instruction for the model\n",
    "system_instruct = \"You are a helpful assistant for named entity recognition. You return entity spans in JSON.\"\n",
    "\n",
    "# Example Text\n",
    "example_text = \"\"\"It's only been a day since ChatGPT's new AI image generator went live, and social media feeds \n",
    "are already flooded with AI-generated memes in the style of Studio Ghibli, the cult-favorite \n",
    "Japanese animation studio behind blockbuster films such as \"My Neighbor Totoro\" and \"Spirited Away.\"\n",
    "\n",
    "In the last 24 hours, we've seen AI-generated images representing Studio Ghibli versions of Elon Musk, \n",
    "\"The Lord of the Rings\", and President Donald Trump. OpenAI CEO Sam Altman even seems to have made his \n",
    "new profile picture a Studio Ghibli-style image, presumably made with GPT-4o's native image generator. \n",
    "Users seem to be uploading existing images and pictures into ChatGPT and asking the chatbot to re-create \n",
    "it in new styles.\"\"\"\n",
    "\n",
    "# Prompt for CoNLL2003-style entity extraction\n",
    "prompt = \"\"\"\n",
    "Extract the following named entities from the text below, if they appear:\n",
    "- PER (Person)\n",
    "- ORG (Organization)\n",
    "- LOC (Location)\n",
    "- MISC (Miscellaneous)\n",
    "\n",
    "Only include named entities that are explicitly mentioned in the text ‚Äî do not infer or guess. \n",
    "Return each entity **exactly as it appears in the text**, preserving casing and punctuation.\n",
    "\n",
    "Return the result as a JSON object in the format:\n",
    "{{\n",
    "  \"PER\": [...],\n",
    "  \"ORG\": [...],\n",
    "  \"LOC\": [...],\n",
    "  \"MISC\": [...]\n",
    "}}\n",
    "\n",
    "Return only the JSON object, nothing else.\n",
    "\n",
    "Text: \"\"\" + example_text + \" \\nThe Entities JSON:\"\n",
    "\n",
    "response = llm_generate(model_name, prompt, system_prompt=system_instruct, \n",
    "                       mode='json', temperature=0)\n",
    "\n",
    "print(\"Extracted entities by type from LLM:\")\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities by type by DistilBERT:\n",
      "{\n",
      "  \"PER\": [\n",
      "    \"Elon Musk\",\n",
      "    \"Donald Trump\",\n",
      "    \"Sam Altman\"\n",
      "  ],\n",
      "  \"ORG\": [\n",
      "    \"ChatGPT\",\n",
      "    \"Studio Ghibli\",\n",
      "    \"OpenAI\"\n",
      "  ],\n",
      "  \"LOC\": [],\n",
      "  \"MISC\": [\n",
      "    \"AI\",\n",
      "    \"Japanese\",\n",
      "    \"My Neighbor Totoro\",\n",
      "    \"Spirited Away\",\n",
      "    \"Studio Ghibli\",\n",
      "    \"The Lord of the Rings\",\n",
      "    \"GPT - 4o\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracted entities by type by DistilBERT:\")\n",
    "print(json.dumps(entities_dict, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some differences.  Also note that our DistilBERT model isn't perfect either.  See how it tagged \"Studio Ghibli\" as both an \"ORG\" and \"MISC\".  LLM models seem to tag a lot of things as \"MISC\".  This could probably be improved by giving the LLM better instructions about what is meant my \"MISC.\"  Overall the results are pretty impressive though for using a model that hasn't been explictly trained for NER on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM for NER - Streamlining the Process\n",
    "\n",
    "Similar to the way we made `llm_text_classifier` for text classification, we'll put our pipeline together here in a single function that expects us to input a list of texts to be tagged and outputs a list of entity dictionaries.   You could alsom import this function from Lesson_10_Helpers for use in the homework.\n",
    "\n",
    "If you have to do a lot of this sort of work you should explore [LangChain](https://www.langchain.com/) which is an ecosystem of tools for developing applications powered by LLMs.  If you're curious check out the [documentation here](https://python.langchain.com/docs/introduction/).  Look at the tutorial for text classification to see how it compares to what we did in Lesson 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_ner_extractor(model_name,\n",
    "                      texts,\n",
    "                      system_prompt,\n",
    "                      prompt_template,\n",
    "                      temperature=0):\n",
    "    \"\"\"\n",
    "    Extract named entities using a Large Language Model (LLM) in zero-shot fashion.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the LLM model to use (e.g., 'gemini-flash-lite').\n",
    "        texts (list of str): List of input texts to process.\n",
    "        system_prompt (str): System prompt guiding the LLM behavior.\n",
    "        prompt_template (str): Template to construct the user prompt for each text.\n",
    "        temperature (float, optional): Temperature for generation (0 = deterministic). Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: List of JSON objects containing extracted entities for each input text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Create user prompts by formatting the prompt template with each input text.\n",
    "    # This ensures that each text is passed to the LLM with the same structure.\n",
    "    user_prompts = [prompt_template.format(text=text) for text in texts]\n",
    "\n",
    "    # Step 2: Generate json outputs from the LLM using the provided model name and prompts.\n",
    "    # The `llm_generate` function sends the prompts to the LLM and retrieves the responses.\n",
    "    json_outputs = llm_generate(model_name,\n",
    "                               user_prompts,\n",
    "                               system_prompt=system_prompt,\n",
    "                               mode='json',\n",
    "                               temperature=temperature)\n",
    "\n",
    "    return json_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll apply `llm_ner_extractor` to the first 100 texts in the validation set to extract the entity dictionaries. We'll use the `gemini-flash-lite` model which is fast and inexpensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a289e7ca2b4206b27341300760da0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/100 [00:00<?, ?prompt/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "The Entities JSON:\n",
      "{'PER': [], 'ORG': ['LEICESTERSHIRE'], 'LOC': [], 'MISC': ['CRICKET']}\n",
      "\n",
      "\n",
      "Text: LONDON 1996-08-30\n",
      "The Entities JSON:\n",
      "{'PER': [], 'ORG': [], 'LOC': ['LONDON'], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and\n",
      "39 runs in two days to take over at the head of the county championship .\n",
      "The Entities JSON:\n",
      "{'PER': ['Phil Simmons'], 'ORG': ['Leicestershire', 'Somerset'], 'LOC': [], 'MISC': ['West Indian', 'county\n",
      "championship']}\n",
      "\n",
      "\n",
      "Text: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on\n",
      "victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "The Entities JSON:\n",
      "{'PER': [], 'ORG': ['Essex', 'Derbyshire', 'Surrey', 'Kent', 'Nottinghamshire'], 'LOC': [], 'MISC': ['title']}\n",
      "\n",
      "\n",
      "Text: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first\n",
      "innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "The Entities JSON:\n",
      "{'PER': ['Andy Caddick'], 'ORG': ['Leicestershire'], 'LOC': ['Grace Road'], 'MISC': ['England']}\n",
      "\n",
      "\n",
      "Text: Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out\n",
      "for 174 .\n",
      "The Entities JSON:\n",
      "{'PER': ['Simmons'], 'ORG': ['Somerset'], 'LOC': [], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip\n",
      "on their match against Yorkshire at Headingley .\n",
      "The Entities JSON:\n",
      "{'PER': ['Nasser Hussain', 'Peter Such'], 'ORG': [], 'LOC': ['Essex', 'Yorkshire', 'Headingley'], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of\n",
      "the season , as Essex reached 372 and took a first innings lead of 82 .\n",
      "The Entities JSON:\n",
      "{'PER': ['Hussain'], 'ORG': ['Essex'], 'LOC': ['England'], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes ,\n",
      "taking four for 24 in 48 balls and leaving them hanging on 119 for five and praying for rain .\n",
      "The Entities JSON:\n",
      "{'PER': ['Such'], 'ORG': ['Yorkshire'], 'LOC': [], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: At the Oval , Surrey captain Chris Lewis , another man dumped by England , continued to silence his critics as he\n",
      "followed his four for 45 on Thursday with 80 not out on Friday in the match against Warwickshire .\n",
      "The Entities JSON:\n",
      "{'PER': ['Chris Lewis'], 'ORG': ['England'], 'LOC': ['Oval', 'Surrey', 'Warwickshire'], 'MISC': []}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gemini-flash-lite'\n",
    "\n",
    "# Extract N examples from the validation split of CoNLL2003\n",
    "N = 100\n",
    "subset = dataset[\"validation\"].select(range(N))\n",
    "\n",
    "texts = [' '.join(tokens) for tokens in subset[\"tokens\"]] # Convert tokens to text\n",
    "\n",
    "# System instruction for the model\n",
    "system_instruct = \"You are a helpful assistant for named entity recognition. You return entity spans in JSON.\"\n",
    "\n",
    "# Prompt template adapted for CoNLL2003-style entity extraction.  \n",
    "# You must keep {text} in the template for the text to be inserted.\n",
    "prompt_template = \"\"\"\n",
    "Extract the following named entities from the text below, if they appear:\n",
    "- PER (Person)\n",
    "- ORG (Organization)\n",
    "- LOC (Location)\n",
    "- MISC (Miscellaneous)\n",
    "\n",
    "Only include named entities that are explicitly mentioned in the text ‚Äî do not infer or guess. \n",
    "Return each entity **exactly as it appears in the text**, preserving casing and punctuation.\n",
    "\n",
    "Return the result as a JSON object in the format:\n",
    "{{\n",
    "  \"PER\": [...],\n",
    "  \"ORG\": [...],\n",
    "  \"LOC\": [...],\n",
    "  \"MISC\": [...]\n",
    "}}\n",
    "\n",
    "Return only the JSON object, nothing else.\n",
    "\n",
    "Text: {text}\n",
    "The Entities JSON:\n",
    "\"\"\"\n",
    "\n",
    "# Call the LLM-based NER extractor\n",
    "predicted_entities = llm_ner_extractor(\n",
    "    model_name,\n",
    "    texts,\n",
    "    system_instruct,\n",
    "    prompt_template,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Display the first few predictions for inspection\n",
    "for i, text in enumerate(texts[:10]):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"The Entities JSON:\")\n",
    "    print(predicted_entities[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look to see if there were any problems extracting JSON from the LLM output.  We can count the number of output dictionaries that include 'Error' as a key (this will depend on the LLM and your prompt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dictionaries with 'Error' as a key: 0\n"
     ]
    }
   ],
   "source": [
    "error_count = sum(1 for prediction in predicted_entities if 'Error' in prediction)\n",
    "print(f\"Number of dictionaries with 'Error' as a key: {error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  We were able to successfully extract JSON from every response.  Let's now evaluate the performance.  Since we're not comparing tags token-by-token what we'll do is:\n",
    "\n",
    "1.  Use the token-by-token tags in the dataset to compute an entity dictionary for each input text.\n",
    "\n",
    "2.  Compare the predicted entity dictionary to the \"gold\" entity dictionary for each example using fuzzy matching (inexact string matches).  In the context of NER the ground-truth labels are sometime called the \"gold\" labels!\n",
    "\n",
    "You can learn more about fuzzy string matching and the package in the [RapidFuzz Documentation](https://rapidfuzz.github.io/RapidFuzz/).\n",
    "\n",
    "We built a helper function called `extract_gold_entities` which takes an example from our dataset and extracts the \"gold\" dictionary.  For example, here's an example from the validation set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the extracted gold or ground-truth entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': ['West Indian'],\n",
       " 'PER': ['Phil Simmons'],\n",
       " 'ORG': ['Leicestershire', 'Somerset']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_entities = extract_gold_entities(subset[2], BIO_tags_list)\n",
    "gold_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While here are the predicted entities from our LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': ['Phil Simmons'],\n",
       " 'ORG': ['Leicestershire', 'Somerset'],\n",
       " 'LOC': [],\n",
       " 'MISC': ['West Indian', 'county championship']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_entities[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate Named Entity Recognition (NER), we compare the entities predicted by the model with the **gold (true)** entities from the dataset.\n",
    "\n",
    "We compute the following metrics **for each entity type** (e.g., PER, LOC, ORG):\n",
    "\n",
    "- **Precision** = Correct predictions / All predictions  \n",
    "- **Recall** = Correct predictions / All gold (true) entities  \n",
    "- **F1 score** = Harmonic mean of precision and recall  \n",
    "- **Accuracy** = Correct predictions / (Correct + Wrong + Missed predictions)\n",
    "\n",
    "We include the function `evaluate_ner` in `helpers.py` to do the computations.  It's imported above.  We show you how to use it in the next cell assuming that `subset` from above for which our LLM NER model gave us the entity `predicted_entities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Entity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ee68826a-7136-49fb-b174-8d9b48d08af7",
       "rows": [
        [
         "0",
         "PER",
         "0.9828",
         "0.9828",
         "0.9828",
         "58.0",
         null
        ],
        [
         "1",
         "ORG",
         "0.7797",
         "0.6571",
         "0.7132",
         "70.0",
         null
        ],
        [
         "2",
         "LOC",
         "0.7015",
         "0.8103",
         "0.752",
         "58.0",
         null
        ],
        [
         "3",
         "MISC",
         "0.06",
         "0.25",
         "0.0968",
         "12.0",
         null
        ],
        [
         "4",
         "Overall",
         "0.6538",
         "0.7727",
         "0.7083",
         null,
         "0.5484"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Number</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>0.8103</td>\n",
       "      <td>0.7520</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0968</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overall</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.7727</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entity  Precision  Recall      F1  Number  Accuracy\n",
       "0      PER     0.9828  0.9828  0.9828    58.0       NaN\n",
       "1      ORG     0.7797  0.6571  0.7132    70.0       NaN\n",
       "2      LOC     0.7015  0.8103  0.7520    58.0       NaN\n",
       "3     MISC     0.0600  0.2500  0.0968    12.0       NaN\n",
       "4  Overall     0.6538  0.7727  0.7083     NaN    0.5484"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract gold entities\n",
    "gold_entities = [extract_gold_entities(ex, BIO_tags_list) for ex in subset]\n",
    "\n",
    "# Evaluate\n",
    "results_llm = evaluate_ner(predicted_entities, gold_entities, labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "\n",
    "# Format the evaluation results\n",
    "df_results_llm = format_ner_eval_results(results_llm)\n",
    "display(df_results_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM NER results are terrific for people, and pretty good for locations and organizations, but only find about 25% the true MISC entities in the texts.  Maybe you can get it to work better by providing examples of MISC entities and additional instructions in the prompt.\n",
    "\n",
    "Here are the results from the BERT model (applied to the whole test set) for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Entity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "72883f27-7798-46dc-a132-853287bfe497",
       "rows": [
        [
         "0",
         "LOC",
         "0.9195",
         "0.9113",
         "0.9154",
         "1668.0",
         null
        ],
        [
         "1",
         "MISC",
         "0.7235",
         "0.7977",
         "0.7588",
         "702.0",
         null
        ],
        [
         "2",
         "ORG",
         "0.8451",
         "0.8838",
         "0.864",
         "1661.0",
         null
        ],
        [
         "3",
         "PER",
         "0.9619",
         "0.9518",
         "0.9568",
         "1617.0",
         null
        ],
        [
         "4",
         "Overall",
         "0.8825",
         "0.9007",
         "0.8915",
         null,
         "0.978"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Number</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.9195</td>\n",
       "      <td>0.9113</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>0.7977</td>\n",
       "      <td>0.7588</td>\n",
       "      <td>702.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.8838</td>\n",
       "      <td>0.8640</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overall</td>\n",
       "      <td>0.8825</td>\n",
       "      <td>0.9007</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entity  Precision  Recall      F1  Number  Accuracy\n",
       "0      LOC     0.9195  0.9113  0.9154  1668.0       NaN\n",
       "1     MISC     0.7235  0.7977  0.7588   702.0       NaN\n",
       "2      ORG     0.8451  0.8838  0.8640  1661.0       NaN\n",
       "3      PER     0.9619  0.9518  0.9568  1617.0       NaN\n",
       "4  Overall     0.8825  0.9007  0.8915     NaN     0.978"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(format_ner_eval_results(results_BERT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The accuracies are very different, in part, because they're computed differently.  In the case of the BERT model we are able to include all the tokens tagged as 'O' (other) which is most of the tokens.  This inflates the accuracy just like computing accuracy for a segmentation model in which most of the pixels are background.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
