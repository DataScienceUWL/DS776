{
 "cells": [
  {
   "cell_type": "code",
   "source": "from introdl import export_this_to_html\nexport_this_to_html()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Export to HTML for Canvas Submission\n\nRun the cell below to export this notebook to HTML for submission.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 04 Assignment\n",
    "**Name:** [Student Name Here]  \n",
    "**Total Points:** 50\n",
    "\n",
    "## Submission Checklist\n",
    "- [ ] All code cells executed with output saved\n",
    "- [ ] All questions answered\n",
    "- [ ] Notebook converted to HTML (use the Homework_04_Utilities notebook)\n",
    "- [ ] Canvas notebook filename includes `_GRADE_THIS_ONE`\n",
    "- [ ] Files uploaded to Canvas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b439c",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deeper Networks\n",
    "\n",
    "In this assignment you'll explore using a deeper, fully connected model to classify FashionMNIST images again.  The point isn't really to get the best classifier possible, rather we want you to get some experience seeing how deep networks can be difficult to train and how using batch normalization and residual connections can make a network easier to train while also increasing the performance.  Along the way you'll practice building a deep model using loops instead of typing out each individual layer.\n",
    "\n",
    "\n",
    "## Part 1 - Build a Deep Fully Connected Network and Train it on Downsample MNIST (8 pts)\n",
    "\n",
    "Our network will consist of:\n",
    "* input layer which maps the flattened 784 pixels to 64 neurons followed by a ReLU function.\n",
    "* 10 blocks, each block has linear + ReLU + linear + ReLU, the number of neurons stays at 64\n",
    "* a linear output layer that maps the 64 neurons to 10 output neurons (for 10 classes) \n",
    "\n",
    "Use this template (cut and paste into a code cell and complete the model)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super(SimpleBlock, self).__init__()\n",
    "        # fill in the repeating elements here linear(hidden_dim, hidden_dim) + ReLU + linear + ReLU\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # fill the forward methods to call linear + ReLU + linear + ReLU and return result\n",
    "        return # complete\n",
    "\n",
    "class Deep_MNIST_FC(nn.Module):\n",
    "    def __init__(self, num_blocks=10):\n",
    "        super(Deep_MNIST_FC, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = ### complete the input layer with linear and ReLU, will handle flatten in the forward below\n",
    "    \n",
    "        \n",
    "        # Repeating simple blocks\n",
    "        self.blocks = nn.Sequential(*[SimpleBlock(64) for _ in range(num_blocks)])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = ### add output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        ## call input layer\n",
    "        ## call blocks\n",
    "        ## call output layer\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Complete the SimpleBlock class\n",
    "# - Add two linear layers (hidden_dim -> hidden_dim) \n",
    "# - Add ReLU activations between and after the layers\n",
    "# - Remember to define layers in __init__ and use them in forward()\n",
    "\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super(SimpleBlock, self).__init__()\n",
    "        # Fill in the repeating elements here: linear(hidden_dim, hidden_dim) + ReLU + linear + ReLU\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Fill the forward method to call linear + ReLU + linear + ReLU and return result\n",
    "        return  # Complete this\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the downsampled (10% of training data) FashionMNIST DataSet and DataLoaders here.  Include the data augmentation from last week.  Use batchsize = 64.  If you're not doing so already, instead of test_dataset and test_loader make the second dataset (with train = False) the valid_dataset and the loader should be valid_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Complete the Deep_MNIST_FC class\n",
    "# - Fill in the input layer (784 -> 64 neurons + ReLU)\n",
    "# - The blocks are already defined for you\n",
    "# - Fill in the output layer (64 -> 10 classes)\n",
    "# - Complete the forward method calls\n",
    "\n",
    "class Deep_MNIST_FC(nn.Module):\n",
    "    def __init__(self, num_blocks=10):\n",
    "        super(Deep_MNIST_FC, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = ### Complete the input layer with linear and ReLU, will handle flatten in forward below\n",
    "    \n",
    "        # Repeating simple blocks (already provided)\n",
    "        self.blocks = nn.Sequential(*[SimpleBlock(64) for _ in range(num_blocks)])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = ### Add output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        ## Call input layer\n",
    "        ## Call blocks\n",
    "        ## Call output layer\n",
    "        return x\n",
    "\n",
    "# Create model and show summary\n",
    "# model = Deep_MNIST_FC()\n",
    "# summary(model, input_size=(32, 1, 28, 28))\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model with AdamW (lr = 0.001) for 40 epochs.  Track the accuracy.  Be sure to save a checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0803",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Train the Deep_MNIST_FC model with AdamW optimizer for 40 epochs\n",
    "# - Initialize the model with 10 blocks\n",
    "# - Use AdamW optimizer with lr=0.001\n",
    "# - Track training and validation accuracy\n",
    "# - Save checkpoint after training\n",
    "# - Use the training loop from previous assignments\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use plot_training_metrics to plot the loss and accuracy for the training and validation sets.  You don't have to display the results dataframe, just the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Plot training metrics using plot_training_metrics function\n",
    "# - Load the training results from the checkpoint\n",
    "# - Use plot_training_metrics to display loss and accuracy curves\n",
    "# - Show both training and validation metrics\n",
    "# - No need to display the results dataframe\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Use Batch Normalization (8 pts)\n",
    "\n",
    "Create a new model class called Deep_MNIST_FN based on your code from Part 1.  You should now use BatchNorm1d after every linear layer.  Create an instance of your model and train it for 40 epochs.  Produce the same graphs.  Comment on how this model trained compared to the model in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Setup FashionMNIST data with downsampling and augmentation\n",
    "# - Load FashionMNIST with appropriate transforms (normalization: mean=0.2860, std=0.3530)\n",
    "# - Downsample training set to 10% using provided code\n",
    "# - Create DataLoaders with batch_size=64\n",
    "\n",
    "# Use this code for downsampling (copy and uncomment):\n",
    "# from torch.utils.data import Subset\n",
    "# import numpy as np\n",
    "# np.random.seed(42)  # use this seed for reproducibility\n",
    "# subset_indices = np.random.choice(len(train_dataset), size=int(0.1 * len(train_dataset)), replace=False)\n",
    "# train_dataset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create Deep_MNIST_FN model class with batch normalization\n",
    "# - Base on Deep_MNIST_FC from Part 1\n",
    "# - Add BatchNorm1d after every linear layer\n",
    "# - Train for 40 epochs with same parameters\n",
    "# - Create plots comparing to Part 1 model\n",
    "# - Comment on training differences\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Use Residual Connections (8 pts)\n",
    "\n",
    "Now create a new model class called Deep_MNIST_FC_Res by modifying the model from Part 1 so that each block (with two linear layers) has a residual connection around that block.  This model should not include batch normalization.  Train for 40 epochs.  Make plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create Deep_MNIST_FC_Res model class with residual connections\n",
    "# - Modify the SimpleBlock to include residual connections around each block\n",
    "# - Add the input to the output of each block (x + block_output)\n",
    "# - No batch normalization in this version\n",
    "# - Make sure dimensions match for the residual connection\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Train the Deep_MNIST_FC_Res model for 40 epochs\n",
    "# - Use same training parameters as previous models\n",
    "# - AdamW optimizer with lr=0.001\n",
    "# - Track training and validation metrics\n",
    "# - Make plots to compare with previous models\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Use Residual Connections and Batch Normalization (8 pts)\n",
    "\n",
    "Now create a new model class called Deep_MNIST_FC_Res_BN by modifying one of the models from above so that each block (with two linear layers) has a residual connection around that block and all linear layers are followed by batch normalization.  Train for 40 epochs.  Make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create Deep_MNIST_FC_Res_BN model with both residual connections and batch normalization\n",
    "# - Combine residual connections from Part 3 with batch normalization from Part 2\n",
    "# - Add BatchNorm1d after every linear layer\n",
    "# - Include residual connections around each block\n",
    "# - This should be the most advanced model combining both techniques\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Train the Deep_MNIST_FC_Res_BN model for 40 epochs\n",
    "# - Use same training parameters as all previous models\n",
    "# - AdamW optimizer with lr=0.001\n",
    "# - Track training and validation metrics for comparison\n",
    "# - Make plots to compare with all previous models\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Analysis and Comparison (8 pts)\n",
    "\n",
    "Create two plot showing the validation loss and validation accuracy for each of the four models above.  Write a comparison of the four models.  If you could only choose batch normalization or residual connections, which would it be?  Does the model with both residual connections and batch normalization perform better than the others.  Which approach yeilds the fastest training? Do you find that both residual connections and batch normalization are necessary?  Address ALL of these questions in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create comparison plots for all four models\n",
    "# - Plot validation loss for all models on one figure\n",
    "# - Plot validation accuracy for all models on another figure\n",
    "# - Use different colors/styles for each model\n",
    "# - Add legends to identify each model clearly\n",
    "# - Include models: Deep_MNIST_FC, Deep_MNIST_FN, Deep_MNIST_FC_Res, Deep_MNIST_FC_Res_BN\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📝 **YOUR ANALYSIS HERE:**\n",
    "\n",
    "Write a comprehensive comparison of the four models addressing all the questions above:\n",
    "- Compare performance of all four models\n",
    "- Choose between batch normalization OR residual connections if you could only pick one\n",
    "- Evaluate whether the combined model (res + BN) performs best\n",
    "- Analyze which approach yields fastest training\n",
    "- Discuss necessity of both techniques\n",
    "\n",
    "Use your training results and plots to support your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8 pts] Questions from Chapter 6 Reading\n",
    "\n",
    "**Question 1 (3 pts):** The textbook discusses the vanishing gradient problem with activation functions like tanh and sigmoid (Section 6.1). In your model implementation below, you'll use ReLU activations. Explain in your own words:\n",
    "- Why do tanh and sigmoid activations lead to vanishing gradients?\n",
    "- How does the derivative of ReLU help avoid this problem?\n",
    "- What potential issue does standard ReLU have, and how does LeakyReLU address it?\n",
    "\n",
    "📝 **YOUR ANSWER HERE:**\n",
    "\n",
    "**Question 2 (3 pts):** Section 6.2.7 discusses an interesting property of batch normalization - that mathematically, a linear layer followed by batch norm is equivalent to a single linear layer with different weights. Given this equivalence:\n",
    "- Why do we still use batch normalization if it doesn't change what the network can represent?\n",
    "- How does this relate to the gap between a network's capacity and what gradient descent can actually learn?\n",
    "- Based on your training results above, do you observe faster convergence or better accuracy with normalization? Why might this be?\n",
    "\n",
    "📝 **YOUR ANSWER HERE:**\n",
    "\n",
    "**Question 4 (2 pts):** Looking at sections 6.3-6.4 on skip connections and 1×1 convolutions:\n",
    "- What is the main benefit of skip connections in terms of gradient flow?\n",
    "- How do 1×1 convolutions differ from regular convolutions in what information they capture?\n",
    "\n",
    "📝 **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2 pts] Reflection\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for this lesson? Why?\n",
    "\n",
    "📝 **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "📝 **YOUR ANSWER HERE:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}