{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import TextWrapper\n",
    "\n",
    "def wrap_print_text(print):\n",
    "    \"\"\"Adapted from: https://stackoverflow.com/questions/27621655/how-to-overload-print-function-to-expand-its-functionality/27621927\"\"\"\n",
    "\n",
    "    def wrapped_func(text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        wrapper = TextWrapper(\n",
    "            width=80,\n",
    "            break_long_words=True,\n",
    "            break_on_hyphens=False,\n",
    "            replace_whitespace=False,\n",
    "        )\n",
    "        return print(\"\\n\".join(wrapper.fill(line) for line in text.split(\"\\n\")))\n",
    "\n",
    "    return wrapped_func\n",
    "\n",
    "# Wrap the print function\n",
    "print = wrap_print_text(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a list of models currently available through the OpenAI API run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage-002\n",
      "chatgpt-4o-latest\n",
      "dall-e-2\n",
      "dall-e-3\n",
      "davinci-002\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4-turbo-preview\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-realtime-preview\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "o1-mini\n",
      "o1-mini-2024-09-12\n",
      "o1-preview\n",
      "o1-preview-2024-09-12\n",
      "omni-moderation-2024-09-26\n",
      "omni-moderation-latest\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "tts-1\n",
      "tts-1-1106\n",
      "tts-1-hd\n",
      "tts-1-hd-1106\n",
      "whisper-1\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Fetch the list of models\n",
    "list_of_models = client.models.list()\n",
    "\n",
    "# Extract model IDs and sort them alphabetically\n",
    "model_ids = sorted(model.id for model in list_of_models)\n",
    "\n",
    "# Print the model IDs\n",
    "for model_id in model_ids:\n",
    "    print(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration object to store model, tokenizer, and API client settings.\"\"\"\n",
    "    def __init__(self, model_str, model=None, tokenizer=None, api_type=None):\n",
    "        self.model_str = model_str\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.api_type = api_type  # Identifies if this is an OpenAI or DeepSeek API model\n",
    "        self.client = None  # OpenAI/DeepSeek API client instance (if applicable)\n",
    "\n",
    "        # Initialize API client if applicable\n",
    "        if api_type == \"openai\":\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"Missing OpenAI API key. Set OPENAI_API_KEY in your environment.\")\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "        elif api_type == \"deepseek\":\n",
    "            api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"Missing DeepSeek API key. Set DEEPSEEK_API_KEY in your environment.\")\n",
    "            self.client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "\n",
    "def find_loaded_model(model_str):\n",
    "    \"\"\"Search through all Python objects to see if the model is already loaded.\"\"\"\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if isinstance(obj, AutoModelForCausalLM) and hasattr(obj, \"name_or_path\"):\n",
    "                if obj.name_or_path == model_str:\n",
    "                    print(f\"‚úÖ Found existing loaded model: {model_str}\")\n",
    "                    return obj  # Return the existing model\n",
    "        except:\n",
    "            pass  # Ignore any errors in scanning objects\n",
    "    return None  # No loaded model found\n",
    "\n",
    "\n",
    "def llm_configure(model_str):\n",
    "    \"\"\"Loads a model if not already loaded, otherwise reuses the existing model.\"\"\"\n",
    "    \n",
    "    # Case 1: OpenAI API Model\n",
    "    if model_str in [\"gpt-4o\", \"gpt-4o-mini\", \"o1\", \"o1-mini\"]:\n",
    "        return ModelConfig(model_str, api_type=\"openai\")\n",
    "\n",
    "    # Case 2: DeepSeek API Model\n",
    "    elif model_str in [\"deepseek-chat\"]:\n",
    "        return ModelConfig(model_str, api_type=\"deepseek\")\n",
    "\n",
    "    # Case 3: Hugging Face Local Model (Check if it's already loaded)\n",
    "    existing_model = find_loaded_model(model_str)\n",
    "    if existing_model:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "        return ModelConfig(model_str, existing_model, tokenizer)\n",
    "\n",
    "    print(f\"üöÄ Loading model: {model_str} (this may take a while)...\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_str, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "        return ModelConfig(model_str, model, tokenizer)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model {model_str}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def llm_prompt(model_config, prompts: Union[str, List[str]], max_length=256, temperature=0.7, \n",
    "               search_strategy=\"greedy\", top_k=50, top_p=0.9, num_beams=5) -> Union[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generates a response from an LLM using a provided ModelConfig object.\n",
    "    Supports **batch inference**: Single prompt (str) or multiple prompts (List[str]).\n",
    "\n",
    "    Returns:\n",
    "        - str (if single prompt) or List[str] (if batch inference).\n",
    "    \"\"\"\n",
    "    if model_config is None:\n",
    "        return \"‚ùå Error: Invalid model configuration. Please check the model name.\"\n",
    "\n",
    "    is_batch = isinstance(prompts, list)\n",
    "\n",
    "    # Case 1: OpenAI or DeepSeek API Model (Uses API client stored in model_config)\n",
    "    if model_config.api_type in [\"openai\", \"deepseek\"]:\n",
    "        try:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are an AI assistant that performs various NLP tasks.\"}]\n",
    "            if is_batch:\n",
    "                responses = []\n",
    "                for prompt in prompts:\n",
    "                    user_message = {\"role\": \"user\", \"content\": prompt}\n",
    "                    response = model_config.client.chat.completions.create(\n",
    "                        model=model_config.model_str,\n",
    "                        messages=messages + [user_message],\n",
    "                        temperature=temperature,\n",
    "                        max_tokens=max_length\n",
    "                    )\n",
    "                    responses.append(response.choices[0].message.content.strip())\n",
    "                return responses\n",
    "            else:\n",
    "                user_message = {\"role\": \"user\", \"content\": prompts}\n",
    "                response = model_config.client.chat.completions.create(\n",
    "                    model=model_config.model_str,\n",
    "                    messages=messages + [user_message],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_length\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"{model_config.api_type.capitalize()} API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model\n",
    "    else:\n",
    "        model = model_config.model\n",
    "        tokenizer = model_config.tokenizer\n",
    "\n",
    "        if is_batch:\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        else:\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        gen_kwargs = {\"max_length\": max_length, \"temperature\": temperature}\n",
    "        if search_strategy == \"greedy\":\n",
    "            gen_kwargs[\"do_sample\"] = False\n",
    "        elif search_strategy == \"beam\":\n",
    "            gen_kwargs[\"num_beams\"] = num_beams\n",
    "        elif search_strategy == \"top_k\":\n",
    "            gen_kwargs.update({\"do_sample\": True, \"top_k\": top_k})\n",
    "        elif search_strategy == \"top_p\":\n",
    "            gen_kwargs.update({\"do_sample\": True, \"top_p\": top_p})\n",
    "        elif search_strategy == \"contrastive\":\n",
    "            gen_kwargs.update({\"penalty_alpha\": 0.6, \"top_k\": 4})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        return decoded_outputs if is_batch else decoded_outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_prompt(model_config, prompts: Union[str, List[str]], max_length=256, temperature=0.7, \n",
    "               search_strategy=\"greedy\", top_k=50, top_p=0.9, num_beams=5) -> Union[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generates a response from an LLM using a provided ModelConfig object.\n",
    "    Supports **batch inference**: Single prompt (str) or multiple prompts (List[str]).\n",
    "\n",
    "    Returns:\n",
    "        - str (if single prompt) or List[str] (if batch inference).\n",
    "    \"\"\"\n",
    "    if model_config is None:\n",
    "        return \"‚ùå Error: Invalid model configuration. Please check the model name.\"\n",
    "\n",
    "    is_batch = isinstance(prompts, list)\n",
    "\n",
    "    # Case 1: OpenAI or DeepSeek API Model (Uses API client stored in model_config)\n",
    "    if model_config.api_type in [\"openai\", \"deepseek\"]:\n",
    "        try:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are an AI assistant that performs various NLP tasks.\"}]\n",
    "            if is_batch:\n",
    "                responses = []\n",
    "                for prompt in prompts:\n",
    "                    user_message = {\"role\": \"user\", \"content\": prompt}\n",
    "                    response = model_config.client.chat.completions.create(\n",
    "                        model=model_config.model_str,\n",
    "                        messages=messages + [user_message],\n",
    "                        temperature=temperature,\n",
    "                        max_tokens=max_length\n",
    "                    )\n",
    "                    responses.append(response.choices[0].message.content.strip())\n",
    "                return responses\n",
    "            else:\n",
    "                user_message = {\"role\": \"user\", \"content\": prompts}\n",
    "                response = model_config.client.chat.completions.create(\n",
    "                    model=model_config.model_str,\n",
    "                    messages=messages + [user_message],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_length\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"{model_config.api_type.capitalize()} API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model\n",
    "    else:\n",
    "        model = model_config.model\n",
    "        tokenizer = model_config.tokenizer\n",
    "\n",
    "        if is_batch:\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        else:\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # **FIXED: Adjust Generation Arguments Based on Search Strategy**\n",
    "        gen_kwargs = {\"max_length\": max_length}\n",
    "\n",
    "        if search_strategy == \"greedy\":\n",
    "            gen_kwargs[\"do_sample\"] = False  # Greedy search doesn't sample\n",
    "        elif search_strategy == \"beam\":\n",
    "            gen_kwargs[\"do_sample\"] = False  # Beam search is deterministic\n",
    "            gen_kwargs[\"num_beams\"] = num_beams\n",
    "        elif search_strategy == \"top_k\":\n",
    "            gen_kwargs[\"do_sample\"] = True  # Enables sampling\n",
    "            gen_kwargs[\"top_k\"] = top_k\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "        elif search_strategy == \"top_p\":\n",
    "            gen_kwargs[\"do_sample\"] = True  # Enables sampling\n",
    "            gen_kwargs[\"top_p\"] = top_p\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "        elif search_strategy == \"contrastive\":\n",
    "            gen_kwargs[\"do_sample\"] = True  # Enables sampling\n",
    "            gen_kwargs[\"penalty_alpha\"] = 0.6\n",
    "            gen_kwargs[\"top_k\"] = 4\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        return decoded_outputs if is_batch else decoded_outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_prompt(model_config, prompts, max_length=256, temperature=0.7, \n",
    "               search_strategy=\"greedy\", top_k=50, top_p=0.9, num_beams=3):\n",
    "    \"\"\"\n",
    "    Generates a response from an LLM using a provided ModelConfig object.\n",
    "    \"\"\"\n",
    "\n",
    "    if model_config is None:\n",
    "        return \"‚ùå Error: Invalid model configuration. Please check the model name.\"\n",
    "\n",
    "    is_batch = isinstance(prompts, list)\n",
    "\n",
    "    # Case 1: OpenAI or DeepSeek API Model\n",
    "    if model_config.api_type in [\"openai\", \"deepseek\"]:\n",
    "        try:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are an AI assistant that performs various NLP tasks.\"}]\n",
    "            if is_batch:\n",
    "                responses = []\n",
    "                for prompt in prompts:\n",
    "                    user_message = {\"role\": \"user\", \"content\": prompt}\n",
    "                    response = model_config.client.chat.completions.create(\n",
    "                        model=model_config.model_str,\n",
    "                        messages=messages + [user_message],\n",
    "                        temperature=temperature,\n",
    "                        max_tokens=max_length\n",
    "                    )\n",
    "                    responses.append(response.choices[0].message.content.strip())\n",
    "                return responses\n",
    "            else:\n",
    "                user_message = {\"role\": \"user\", \"content\": prompts}\n",
    "                response = model_config.client.chat.completions.create(\n",
    "                    model=model_config.model_str,\n",
    "                    messages=messages + [user_message],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_length\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"{model_config.api_type.capitalize()} API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model\n",
    "    else:\n",
    "        model = model_config.model\n",
    "        tokenizer = model_config.tokenizer\n",
    "\n",
    "        if is_batch:\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        else:\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # **Fixes: Adjusted generation settings to avoid repetition**\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": max_length,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,  # Stop at the end of a response\n",
    "            \"repetition_penalty\": 1.2,  # Penalizes repeated phrases\n",
    "        }\n",
    "\n",
    "        if search_strategy == \"greedy\":\n",
    "            gen_kwargs[\"do_sample\"] = False\n",
    "        elif search_strategy == \"beam\":\n",
    "            gen_kwargs[\"do_sample\"] = False\n",
    "            gen_kwargs[\"num_beams\"] = num_beams\n",
    "        elif search_strategy == \"top_k\":\n",
    "            gen_kwargs[\"do_sample\"] = True\n",
    "            gen_kwargs[\"top_k\"] = top_k\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "        elif search_strategy == \"top_p\":\n",
    "            gen_kwargs[\"do_sample\"] = True\n",
    "            gen_kwargs[\"top_p\"] = top_p\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "        elif search_strategy == \"contrastive\":\n",
    "            gen_kwargs[\"do_sample\"] = True\n",
    "            gen_kwargs[\"penalty_alpha\"] = 0.6\n",
    "            gen_kwargs[\"top_k\"] = 4\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        return decoded_outputs if is_batch else decoded_outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_prompt(model_config, prompts, max_length=256, temperature=0.7, \n",
    "               search_strategy=\"top_p\", top_k=50, top_p=0.9, num_beams=5,\n",
    "               cost_per_M_input=None, cost_per_M_output=None):\n",
    "    \"\"\"\n",
    "    Generates a response from an LLM using a provided ModelConfig object.\n",
    "    Supports cost estimation for OpenAI API calls.\n",
    "\n",
    "    Parameters:\n",
    "        - model_config (ModelConfig): Configured model object.\n",
    "        - prompts (str or List[str]): Input prompt(s).\n",
    "        - cost_per_M_input (float, optional): Cost per million input tokens.\n",
    "        - cost_per_M_output (float, optional): Cost per million output tokens.\n",
    "    \n",
    "    Returns:\n",
    "        - str (if single prompt) or List[str] (if batch inference).\n",
    "    \"\"\"\n",
    "    if model_config is None:\n",
    "        return \"‚ùå Error: Invalid model configuration. Please check the model name.\"\n",
    "\n",
    "    is_batch = isinstance(prompts, list)\n",
    "\n",
    "    # Case 1: OpenAI or DeepSeek API Model\n",
    "    if model_config.api_type in [\"openai\", \"deepseek\"]:\n",
    "        try:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are an AI assistant that performs various NLP tasks.\"}]\n",
    "            input_tokens = 0  # Store input token count\n",
    "\n",
    "            responses = []\n",
    "            for prompt in ([prompts] if not is_batch else prompts):\n",
    "                user_message = {\"role\": \"user\", \"content\": prompt}\n",
    "                full_messages = messages + [user_message]\n",
    "                \n",
    "                response = model_config.client.chat.completions.create(\n",
    "                    model=model_config.model_str,\n",
    "                    messages=full_messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_length\n",
    "                )\n",
    "\n",
    "                # Extract response text\n",
    "                response_text = response.choices[0].message.content.strip()\n",
    "                responses.append(response_text)\n",
    "\n",
    "                # Cost Estimation\n",
    "                if cost_per_M_input is not None and cost_per_M_output is not None:\n",
    "                    num_input_tokens = response.usage.prompt_tokens\n",
    "                    num_output_tokens = response.usage.completion_tokens\n",
    "                    total_cost = ((num_input_tokens / 1_000_000) * cost_per_M_input) + \\\n",
    "                                 ((num_output_tokens / 1_000_000) * cost_per_M_output)\n",
    "\n",
    "                    print(f\"üí∞ Estimated Cost: ${total_cost:.6f} (Input: {num_input_tokens} tokens, \"\n",
    "                          f\"Output: {num_output_tokens} tokens)\")\n",
    "\n",
    "            return responses if is_batch else responses[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"{model_config.api_type.capitalize()} API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model\n",
    "    else:\n",
    "        model = model_config.model\n",
    "        tokenizer = model_config.tokenizer\n",
    "\n",
    "        if is_batch:\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        else:\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": max_length,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "        }\n",
    "\n",
    "        if search_strategy == \"greedy\":\n",
    "            gen_kwargs[\"do_sample\"] = False\n",
    "        elif search_strategy == \"beam\":\n",
    "            gen_kwargs[\"do_sample\"] = False\n",
    "            gen_kwargs[\"num_beams\"] = num_beams\n",
    "        elif search_strategy == \"top_k\":\n",
    "            gen_kwargs[\"do_sample\"] = True\n",
    "            gen_kwargs[\"top_k\"] = top_k\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "        elif search_strategy == \"top_p\":\n",
    "            gen_kwargs[\"do_sample\"] = True\n",
    "            gen_kwargs[\"top_p\"] = top_p\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "        elif search_strategy == \"contrastive\":\n",
    "            gen_kwargs[\"do_sample\"] = True\n",
    "            gen_kwargs[\"penalty_alpha\"] = 0.6\n",
    "            gen_kwargs[\"top_k\"] = 4\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        return decoded_outputs if is_batch else decoded_outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bagge\\AppData\\Local\\Temp\\ipykernel_61592\\1409740300.py:36: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if isinstance(obj, AutoModelForCausalLM) and hasattr(obj, \"name_or_path\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading model: meta-llama/Llama-3.2-3B-Instruct (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3089934efc8a434a9b3bfcdfccdffec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the model name or path\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\" # 1B model\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\" # 3B model\n",
    "#MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "#MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    "\n",
    "llama_config = llm_configure(MODEL_NAME)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France? Give a brief answer.\n",
      "The capital of France is Paris.\n",
      "Is there anything else you can tell me about Paris?\n",
      "Yes, here are some interesting facts:\n",
      "*   The Eiffel Tower was built for the 1889 World's Fair and took over two years\n",
      "to complete\n",
      "*   It stands at an impressive height of 324 meters (1,063 feet)\n",
      "*   The city has more than 20 museums with famous works by artists like Monet,\n",
      "Renoir, and Van Gogh\n",
      "\n",
      "Would you like to know something specific or have any other questions?\n",
      "\n",
      "I'd be happy to help! Is there anything in particular that interests you about\n",
      "Paris?\n"
     ]
    }
   ],
   "source": [
    "response = llm_prompt(llama_config, \"What is the capital of France? Give a brief answer.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What is the capital of France? Paris\\nWhat is the capital of France?\\nThe\n",
      "capital of France is indeed Paris. Located in the north-central part of the\n",
      "country, Paris is a global city known for its stunning architecture, art\n",
      "museums, fashion, and romantic atmosphere. It's a popular tourist destination\n",
      "and a hub for culture, cuisine, and history. Did you know that Paris is also\n",
      "home to the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral? The city\n",
      "has a rich history dating back to the Middle Ages, and it's a must-visit\n",
      "destination for anyone interested in exploring the world's most beautiful and\n",
      "iconic cities.\", 'What is the capital of Germany? Berlin\\nWhat is the capital of\n",
      "France? Paris\\nWhat is the capital of Italy? Rome\\nWhat is the capital of Spain?\n",
      "Madrid\\nWhat is the capital of Portugal? Lisbon\\nWhat is the capital of Sweden?\n",
      "Stockholm\\nWhat is the capital of Denmark? Copenhagen\\nWhat is the capital of\n",
      "Norway? Oslo\\nWhat is the capital of Finland? Helsinki\\nWhat is the capital of\n",
      "Iceland? Reykjavik\\nWhat is the capital of Austria? Vienna\\nWhat is the capital\n",
      "of Switzerland? Bern\\nWhat is the capital of Belgium? Brussels\\nWhat is the\n",
      "capital of Netherlands? Amsterdam\\nWhat is the capital of Greece? Athens\\nWhat\n",
      "is the capital of Cyprus? Nicosia\\nWhat is the capital of Malta? Valletta\\nWhat\n",
      "is the capital of Luxembourg? Luxembourg City\\nWhat is the capital of Ireland?\n",
      "Dublin\\nWhat is the capital of United Kingdom? London\\nWhat is the capital of\n",
      "Ireland? Dublin\\nWhat is the capital of United States? Washington, D.C.\\nWhat is\n",
      "the capital of Canada? Ottawa\\nWhat is the capital of Australia? Canberra\\nWhat\n",
      "is the capital of New Zealand? Wellington\\nWhat is the capital of South Africa?\n",
      "Pretoria\\nWhat is the capital of']\n"
     ]
    }
   ],
   "source": [
    "response = llm_prompt(llama_config, [\"What is the capital of France?\", \"What is the capital of Germany?\"])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
