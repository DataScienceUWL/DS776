{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 8000\n",
      "Test dataset size: 2000\n",
      "Tokens: ['<PAD>', '<BOS>', '<EOS>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '*', '(', ')']\n",
      "Vocabulary size: 18\n",
      "Vocabulary: {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '+': 13, '-': 14, '*': 15, '(': 16, ')': 17}\n",
      "torch.Size([100, 57])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class InfixToPostfixDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset to generate unique infix and postfix sequences with specified depths.\n",
    "\n",
    "    Parameters:\n",
    "    - num_samples: Number of unique sequences to generate.\n",
    "    - min_depth: Minimum depth of the expression tree.\n",
    "    - max_depth: Maximum depth of the expression tree.\n",
    "    - seed: Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, min_depth=1, max_depth=3, seed=42):\n",
    "        self.num_samples = num_samples\n",
    "        self.min_depth = min_depth\n",
    "        self.max_depth = max_depth\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.data = self._generate_data()\n",
    "\n",
    "    def _generate_data(self):\n",
    "        \"\"\"Generate unique infix and postfix sequences.\"\"\"\n",
    "        data = set()\n",
    "        while len(data) < self.num_samples:\n",
    "            infix, postfix = self._generate_random_expression()\n",
    "            if infix not in {x[0] for x in data}:  # Ensure unique infix expressions\n",
    "                data.add((infix, postfix))\n",
    "        return list(data)\n",
    "\n",
    "    def _generate_random_expression(self):\n",
    "        \"\"\"Generate a random infix and postfix expression pair.\"\"\"\n",
    "        def random_digit():\n",
    "            return str(np.random.randint(1, 10))\n",
    "\n",
    "        def random_operator():\n",
    "            return np.random.choice(['+', '-', '*'])\n",
    "\n",
    "        def generate_expression(depth=0):\n",
    "            if depth >= self.max_depth or (depth >= self.min_depth and np.random.random() > 0.7):\n",
    "                num = random_digit()\n",
    "                return num, num\n",
    "            else:\n",
    "                left_infix, left_postfix = generate_expression(depth + 1)\n",
    "                right_infix, right_postfix = generate_expression(depth + 1)\n",
    "                operator = random_operator()\n",
    "                infix = f\"({left_infix} {operator} {right_infix})\"\n",
    "                postfix = f\"{left_postfix} {right_postfix} {operator}\"\n",
    "                return infix, postfix\n",
    "\n",
    "        return generate_expression()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        infix, postfix = self.data[idx]\n",
    "        return {\n",
    "            'infix': infix,\n",
    "            'postfix': postfix\n",
    "        }\n",
    "\n",
    "dataset = InfixToPostfixDataset(num_samples=10000, min_depth=1, max_depth=4)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the sizes of each set\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "PAD_TOKEN = '<PAD>'\n",
    "BOS_TOKEN = '<BOS>'\n",
    "EOS_TOKEN = '<EOS>'\n",
    "tokens = [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN] + list('0123456789+-*()')\n",
    "vocab = {token: i for i, token in enumerate(tokens)}\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "def pad_batch(batch):\n",
    "    \"\"\"\n",
    "    Custom collation function to pad batches of tokenized input and target sequences,\n",
    "    ignoring spaces during tokenization.\n",
    "\n",
    "    Parameters:\n",
    "    - batch: List of dictionaries with 'infix' and 'postfix' keys.\n",
    "\n",
    "    Returns:\n",
    "    - padded_infix: Padded tensor of tokenized infix sequences.\n",
    "    - padded_postfix: Padded tensor of tokenized postfix sequences.\n",
    "    \"\"\"\n",
    "    # Extract infix and postfix sequences\n",
    "    infix_seqs = [item['infix'] for item in batch]\n",
    "    postfix_seqs = [item['postfix'] for item in batch]\n",
    "\n",
    "    # Tokenize sequences, ignoring spaces\n",
    "    tokenized_infix = [[vocab[char] for char in seq if char != ' '] for seq in infix_seqs]\n",
    "    tokenized_postfix = [[vocab[char] for char in seq if char != ' '] for seq in postfix_seqs]\n",
    "\n",
    "    # Find the maximum length in the batch\n",
    "    max_infix_len = max(len(seq) for seq in tokenized_infix)\n",
    "    max_postfix_len = max(len(seq) for seq in tokenized_postfix)\n",
    "\n",
    "    # Pad sequences\n",
    "    padded_infix = [seq + [vocab[PAD_TOKEN]] * (max_infix_len - len(seq)) for seq in tokenized_infix]\n",
    "    padded_postfix = [seq + [vocab[PAD_TOKEN]] * (max_postfix_len - len(seq)) for seq in tokenized_postfix]\n",
    "\n",
    "    # Convert to tensors\n",
    "    padded_infix = torch.tensor(padded_infix, dtype=torch.long)\n",
    "    padded_postfix = torch.tensor(padded_postfix, dtype=torch.long)\n",
    "\n",
    "    return padded_infix, padded_postfix\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, collate_fn=pad_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, collate_fn=pad_batch)\n",
    "\n",
    "x, labels = next(iter(train_loader))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 61])\n"
     ]
    }
   ],
   "source": [
    "x, labels = next(iter(train_loader))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerInfixToPostfix(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, num_heads=8, num_encoder_layers=3,\n",
    "                 num_decoder_layers=3, ff_hidden_dim=512, max_len=50, dropout=0.1, pad_idx=0):\n",
    "        super(TransformerInfixToPostfix, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        # Embedding layers for input and output tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = self._create_positional_encoding(embed_size, max_len)\n",
    "\n",
    "        # Transformer with batch_first=True\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=ff_hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Use batch_first=True to simplify input dimensions\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Embed source and target sequences\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:src.size(1), :]\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:tgt.size(1), :]\n",
    "\n",
    "        # Generate masks\n",
    "        src_mask = None  # No causal masking for the source\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "        src_padding_mask = self._generate_padding_mask(src).to(src.device)\n",
    "        tgt_padding_mask = self._generate_padding_mask(tgt).to(src.device)\n",
    "\n",
    "        # Pass through Transformer (no need to permute dimensions)\n",
    "        output = self.transformer(\n",
    "            src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask\n",
    "        )\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        output = self.fc_out(output)  # (batch_size, seq_len, vocab_size)\n",
    "        return output\n",
    "\n",
    "    def _create_positional_encoding(self, embed_size, max_len):\n",
    "        \"\"\"Generate positional encoding.\"\"\"\n",
    "        pos_enc = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_enc.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        \"\"\"Generate a subsequent mask for the decoder to prevent attention to future tokens.\"\"\"\n",
    "        mask = torch.triu(torch.ones(size, size)) == 1\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _generate_padding_mask(self, seq):\n",
    "        \"\"\"Create a padding mask for sequences.\"\"\"\n",
    "        return seq == self.pad_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(src, tgt_input)\n",
    "\n",
    "            # Compute loss\n",
    "            logits = logits.reshape(-1, logits.size(-1))\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(logits, tgt_output)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(dataloader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
