{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df355e2",
   "metadata": {},
   "source": [
    "# Homework Sketch\n",
    "\n",
    "* Develop llm_scorer for scoring text summaries and apply it xsum and/or SAMSUM\n",
    "* Develop llm_summarizer for summarization and apply it to smaller subset of xsum or SAMSUM, score with ROUGE, BERTSCore, and llm_scorer.  Use local or API-based model.\n",
    "* Add llm_score to compute all metrics and apply it to the sentence pairs in the lesson.  In each case, comment on the llm similarity scores.\n",
    "* Load another fine-tuned summarizer for xsum and/or SAMSUM and evaluate with same metrics.  Use T5, Pegasus, or ???\n",
    "* Don't bother with fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2cf61",
   "metadata": {},
   "source": [
    "# üìù Exercise: Domain-Specific Summarization Using Fine-Tuned Models vs. General-Purpose LLMs\n",
    "\n",
    "## Overview\n",
    "In this exercise, you will fine-tune a specialized summarization model, `facebook/bart-large`, on the **PubMed** dataset to perform domain-specific summarization. You will then compare its performance to a general-purpose LLM (`gpt-4` or another open LLM like `LLaMA-2`) on the same task.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the process of fine-tuning a pre-trained transformer model for domain-specific summarization.\n",
    "2. Compare performance of fine-tuned models to general-purpose LLMs.\n",
    "3. Use standard summarization metrics (e.g., ROUGE) to evaluate performance.\n",
    "4. Explore trade-offs between specialized and general-purpose models for summarization.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Fine-Tuning `facebook/bart-large` on PubMed Dataset\n",
    "\n",
    "### Setup\n",
    "Install necessary libraries:\n",
    "```bash\n",
    "pip install transformers datasets accelerate\n",
    "```\n",
    "\n",
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f59538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"scientific_papers\", \"pubmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d2a56",
   "metadata": {},
   "source": [
    "### Fine-Tuning Code\n",
    "Follow the provided code snippet to fine-tune `facebook/bart-large` on a subset of the PubMed dataset.\n",
    "\n",
    "- Train on a subset (2000 samples).\n",
    "- Validate on a smaller subset (500 samples).\n",
    "- Use `fp16=True` for efficient training.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Evaluation with ROUGE Metrics\n",
    "\n",
    "Add the following to your code after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74571b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "outputs = trainer.predict(tokenized_val)\n",
    "\n",
    "preds = tokenizer.batch_decode(outputs.predictions, skip_special_tokens=True)\n",
    "refs = [example['abstract'] for example in val_data]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667fec3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Comparison with General-Purpose LLM\n",
    "\n",
    "### Instructions\n",
    "1. Use an LLM (e.g., GPT-4 via API or LLaMA-2 locally) to generate summaries for the same validation set.\n",
    "2. Compare outputs using the same ROUGE metrics.\n",
    "\n",
    "### Prompt Example:\n",
    "\"Summarize the following scientific paper: \\n\\n{Article}\"\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Analysis & Discussion\n",
    "\n",
    "Answer the following questions:\n",
    "1. How do the ROUGE scores of the fine-tuned BART model compare to those of the general-purpose LLM?\n",
    "2. Are there qualitative differences in the summaries (e.g., coherence, conciseness, relevance)?\n",
    "3. What are the strengths and weaknesses of each approach?\n",
    "4. In what scenarios would you prefer using a fine-tuned model over a general-purpose LLM?\n",
    "\n",
    "---\n",
    "\n",
    "## Extension (Optional)\n",
    "- Fine-tune the model on the full PubMed dataset or another domain-specific dataset.\n",
    "- Compare the performance of BART, PEGASUS, and T5 for this task.\n",
    "- Explore different evaluation metrics (e.g., BERTScore).\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "Submit a Jupyter notebook containing:\n",
    "- Your fine-tuning code.\n",
    "- ROUGE metric outputs for both models.\n",
    "- Your analysis and discussion.\n",
    "\n",
    "---\n",
    "\n",
    "Good luck and have fun! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbdc8a2",
   "metadata": {},
   "source": [
    "# Assignment: Comparing General and Fine-Tuned BART Models\n",
    "\n",
    "## Objective\n",
    "In this assignment, you will compare the performance of two pretrained BART models on the task of scientific text summarization:\n",
    "- A general-domain BART model: `facebook/bart-large` (trained on CNN/DailyMail)\n",
    "- A domain-specific BART model: 'ccdv/lsg-bart-base-4096-pubmed' (fine-tuned on PubMed)\n",
    "\n",
    "You will evaluate the models both qualitatively (by examining the generated summaries) and quantitatively (using ROUGE scores).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "### Part 1: Setup\n",
    "1. **Install the required packages**\n",
    "```bash\n",
    "%pip install transformers datasets rouge_score\n",
    "```\n",
    "\n",
    "2. **Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, pipeline\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1509e8e",
   "metadata": {},
   "source": [
    "### Part 2: Load Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general BART model\n",
    "general_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "general_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "general_summarizer = pipeline(\"summarization\", model=general_model, tokenizer=general_tokenizer)\n",
    "\n",
    "# Load domain-specific BART model\n",
    "pubmed_tokenizer = BartTokenizer.from_pretrained(\"ccdv/lsg-bart-base-4096-pubmed\")\n",
    "pubmed_model = BartForConditionalGeneration.from_pretrained(\"ccdv/lsg-bart-base-4096-pubmed\")\n",
    "\n",
    "domain_summarizer = pipeline(\"summarization\", model=pubmed_model, tokenizer=pubmed_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd2694",
   "metadata": {},
   "source": [
    "### Part 3: Provide Scientific Text for Summarization\n",
    "Paste a sample scientific text below. It should be at least a few paragraphs long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Your scientific text goes here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6828e8",
   "metadata": {},
   "source": [
    "### Part 4: Generate Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb311ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries using both models\n",
    "general_summary = general_summarizer(sample_text, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "domain_summary = domain_summarizer(sample_text, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "\n",
    "print(\"General Model Summary:\\n\", general_summary)\n",
    "print(\"\\nDomain-Finetuned Model Summary:\\n\", domain_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424d13c",
   "metadata": {},
   "source": [
    "### Part 5: Evaluate using ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Provide a reference summary for comparison\n",
    "reference_summary = \"\"\"\n",
    "Your reference summary goes here.\n",
    "\"\"\"\n",
    "\n",
    "# Compute ROUGE scores for each model\n",
    "scores_general = scorer.score(reference_summary, general_summary)\n",
    "scores_domain = scorer.score(reference_summary, domain_summary)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGeneral Model ROUGE Scores:\")\n",
    "for metric, score in scores_general.items():\n",
    "    print(f\"{metric}: {score}\")\n",
    "\n",
    "print(\"\\nDomain Model ROUGE Scores:\")\n",
    "for metric, score in scores_domain.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489495e3",
   "metadata": {},
   "source": [
    "### Part 6: Analysis\n",
    "1. **Qualitative Comparison**\n",
    "    - Compare the outputs of the general and domain-specific models. What differences do you notice? Which model produces more coherent, relevant, or precise summaries?\n",
    "\n",
    "2. **Quantitative Comparison**\n",
    "    - Compare the ROUGE scores from both models. Which model achieves higher scores? Does this align with your qualitative observations?\n",
    "\n",
    "3. **Discussion**\n",
    "    - Discuss why the domain-specific model might perform better on PubMed data. What trade-offs might exist between using a general-purpose model vs. a fine-tuned model?\n",
    "\n",
    "### Submission\n",
    "Save your notebook and submit it according to your instructor‚Äôs guidelines."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
