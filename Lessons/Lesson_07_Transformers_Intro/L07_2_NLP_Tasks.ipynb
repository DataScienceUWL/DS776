{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\bagge\\my drive\\python_projects\\ds776_develop_project\\ds776\\lessons\\course_tools\\introdl\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: IPython in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (8.28.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (8.1.5)\n",
      "Requirement already satisfied: ipycanvas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.13.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.14.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.13.2)\n",
      "Requirement already satisfied: torch in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (4.66.5)\n",
      "Requirement already satisfied: pillow>=6.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipycanvas->introdl==1.0) (10.4.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from pandas->introdl==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from pandas->introdl==1.0) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (2024.6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jedi>=0.16->IPython->introdl==1.0) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->introdl==1.0) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->introdl==1.0) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jinja2->torch->introdl==1.0) (3.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from sympy->torch->introdl==1.0) (1.3.0)\n",
      "Building wheels for collected packages: introdl\n",
      "  Building wheel for introdl (pyproject.toml): started\n",
      "  Building wheel for introdl (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for introdl: filename=introdl-1.0-py3-none-any.whl size=41919 sha256=4beaee997aea7973d000a940ec9b740503ccb44f2a544a38bffdefa5a3c7ea59\n",
      "  Stored in directory: C:\\Users\\bagge\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-2zp1fgt3\\wheels\\f5\\d5\\0f\\11f1d5af64d00defb23fa33cf51b2946a0899888d73571e687\n",
      "Successfully built introdl\n",
      "Installing collected packages: introdl\n",
      "  Attempting uninstall: introdl\n",
      "    Found existing installation: introdl 1.0\n",
      "    Uninstalling introdl-1.0:\n",
      "      Successfully uninstalled introdl-1.0\n",
      "Successfully installed introdl-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ../Course_Tools/introdl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from introdl.utils import get_device, wrap_print_text, config_paths_keys\n",
    "from introdl.nlp import llm_configure, llm_generate, clear_pipeline, print_pipeline_info\n",
    "\n",
    "# overload print to wrap text\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "paths = config_paths_keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Tasks with Transformer Models\n",
    "\n",
    "In this notebook we'll demonstrate soutions to some common Natural Langauge Processing (NLP) tasks taht use transformer models.  This material is nearly identical to the examples in the textbook chapter \"XXX\".  We'll add a little background about the underying models.  We'll also demonstrate how these same tasks come be done using a large language model.  \n",
    "\n",
    "## Using Large Language Models from Python\n",
    "\n",
    "For working with large language models we've introduced a couple of helper functions `llm_configure` and `llm_generate` that be used to download and prompt generative models from HuggingFace.  The same functions can also be used to interact with OpenAI's ChatGPT models and Google's Gemini models through their APIs (Application Program Interface). We'll demonstrate the use of these functions in this notebook.  In another notebook in this lesson we'll talk about using the APIs and in Lesson XXX we'll discuss text generation models.\n",
    "\n",
    "To use the Google and OpenAI APIs you'll need to get api keys.  \n",
    "\n",
    "### Google Gemini API\n",
    "\n",
    "Google's Gemini API has a free tier for testing and that may be all you need.  There is also a paid tier and their models are very inexpensive to use for the kind of things we'll do, but you may not need the paid tier at all.  We'll say more about pricing and how the API works in another notebook, but for now you'll want to obtain an API key (did I mention it's free to use for testing...).\n",
    "\n",
    "Go to [Google's AI Studio](https://aistudio.google.com/app/apikey).  Login with a Google account and create an API key.  Copy the API key and paste it into the Lessons/Course_Tools/api_keys.env in place of 'abcdefg' for the GEMINI_API_KEY.  You don't need quotes.  Restart the kernel for this notebook and execute the imports cell again.  The config_paths_keys function should read your api key and set it as an environment variable.  If you know how, you could instead set the GEMINI_API_KEY environment variable on your own machine or in CoCalc.  Generally you don't want to store api keys in notebooks that might be shared with me or others.  Once you have the api key set as an environment variable you can run the code below to interact with Gemini models through the python API.\n",
    "\n",
    "### Using `llm_configure` and `llm_generate` to interact with an LLM.\n",
    "\n",
    "We'll first use one of Meta's Llama LLM models called Llama-3.1-8B-Instruct.  This is the smallest of the [Llama 3.1 models](https://www.ultralytics.com/blog/getting-to-know-llama-3-1-meta-latest-open-source-model-family) introduced in July 2024.  The name includes '8B' because it has about 8 billion parameters and \"Intstruct\" because the model has been fine-tuned on a dataset to help it learn to follow instructions.  We choose a quantized (reduced precision weights) model with 8 billion parameters because it will fit in the RAM on many consumer GPUs.  We'll have much more to say about models like this later, but for now let's see how we can load it up and use it right from out notebook.  \n",
    "\n",
    "You should run on a CoCalc compute server with GPU for this to work well (or if you're running locally you'll need a capable Nvidia GPU). It might work without a GPU, but it could be pretty slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading model: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit (this may take a while)...\n",
      "🟢 Model unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit loaded successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first configure and load the model\n",
    "local_llm = llm_configure(\"llama-3p1-8B\") # llama-3p1-8B is shortcut for the Llama-3.1-8B-Instruct model\n",
    "\n",
    "# don't worry about the warning, it's just a warning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# now give a prompt to the model and wait a few seconds for the response\n",
    "response = llm_generate(local_llm, \"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three interesting facts about space:\n",
      "1. **The Universe is Still Expanding**: Scientists have observed that the\n",
      "universe has been expanding since its inception, and this expansion continues to\n",
      "accelerate due to dark energy.\n",
      "2. **There's a Giant Storm on Jupiter**: The Great Red Spot, a persistent\n",
      "anticyclonic storm on Jupiter, has raged for at least 187 years and may be as\n",
      "large as two Earths in diameter.\n",
      "3. **Space Smells Like Seared Steak**: In 2019, astronauts reported smelling\n",
      "seared steak when they encountered the interstellar medium (the material filling\n",
      "the vast spaces between stars). This scent was caused by charged particles\n",
      "reacting with their spacecraft's surfaces.\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(local_llm, \"Tell me three interesting facts about space.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've got one that's \"punderful\": Why did the mushroom go to the party? Because\n",
      "he was a fun-gi! (get it?)\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(local_llm, \"Tell me a dad joke.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also interact with the Google's Gemini models through their Python API.  We'll explain more about how the API works in another notebook, but we've made it possible to interact with it through `llm_generate`.  Once you have an API key you can make up to 30 requests per minute to the Gemini 2 Flash Lite model on the free tier!  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_config = llm_configure(\"gemini-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"What is the capital of France?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three interesting facts about space:\n",
      "*   **Space is completely silent:** Sound waves need a medium to travel, and\n",
      "since space is a vacuum, there's no air to transmit sound.\n",
      "*   **There are more stars than grains of sand on Earth:** Astronomers estimate\n",
      "there are over 100 billion stars in the Milky Way galaxy alone, and billions of\n",
      "other galaxies.\n",
      "*   **A year on Venus is shorter than a day:** Venus rotates very slowly, taking\n",
      "longer to complete a day than to orbit the sun.\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"Tell me three interesting facts about space.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"Tell me a dad joke.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, \n",
    "but after three days, I hadn’t even received a shipping update. After waiting 45 minutes on hold, \n",
    "customer service told me there was a stock issue—yet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. \n",
    "The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. \n",
    "Tech Haven, you need to do better! Sincerely, Jamie.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, \n",
    "but after three days, I hadn’t even received a shipping update. After waiting 45 minutes on hold, \n",
    "customer service told me there was a stock issue—yet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. \n",
    "The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. \n",
    "Tech Haven, you need to do better! Sincerely, Jamie.\n",
    "\n",
    "To add insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. \n",
    "I had to explain my situation multiple times before they even acknowledged the mistake. \n",
    "The entire experience has been incredibly disappointing and has left me questioning whether I should ever shop with Tech Haven again. \n",
    "It's baffling how a company can operate with such a lack of transparency and efficiency. \n",
    "I hope this feedback reaches someone who can make a difference, as no customer should have to go through what I did.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english, Size: 66,955,010 parameters\n",
      "[{'label': 'NEGATIVE', 'score': 0.9990748167037964}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "print_pipeline_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Models and Text Classification\n",
    "\n",
    "## BERT Models\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model developed by Google. It is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows BERT to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks.\n",
    "\n",
    "### Key Features of BERT:\n",
    "- **Bidirectional Contextual Understanding:** Unlike traditional models that read text sequentially, BERT reads text in both directions, capturing context from both sides.\n",
    "- **Pre-training and Fine-tuning:** BERT is pre-trained on a large corpus of text and can be fine-tuned on specific tasks with relatively small datasets.\n",
    "- **Versatility:** BERT can be used for a wide range of NLP tasks, including question answering, named entity recognition, and text classification.\n",
    "\n",
    "## Text Classification with BERT\n",
    "\n",
    "Text classification is the process of assigning predefined categories to text. BERT has significantly improved the performance of text classification tasks due to its deep understanding of context.\n",
    "\n",
    "### Steps for Text Classification with BERT:\n",
    "1. **Pre-training:** BERT is pre-trained on a large corpus using unsupervised learning tasks like masked language modeling and next sentence prediction.\n",
    "2. **Fine-tuning:** The pre-trained BERT model is fine-tuned on a labeled dataset specific to the text classification task.\n",
    "3. **Prediction:** The fine-tuned model is used to predict the category of new, unseen text.\n",
    "\n",
    "### Example:\n",
    "Using a pre-trained BERT model for sentiment analysis (a type of text classification) involves fine-tuning the model on a labeled dataset of text with sentiment labels (positive, negative, neutral). The fine-tuned model can then classify the sentiment of new text inputs accurately.\n",
    "\n",
    "BERT's ability to understand context and nuances in text makes it a powerful tool for text classification and other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 268.43 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(sentiment_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Named Entity Recognition**\n",
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889\n",
      "parameters\n",
      "[{'entity_group': 'MISC', 'score': 0.9910073, 'word': 'Samsung Galaxy S24\n",
      "Ultra', 'start': 14, 'end': 38}, {'entity_group': 'ORG', 'score': 0.99012053,\n",
      "'word': 'Tech Haven', 'start': 44, 'end': 54}, {'entity_group': 'MISC', 'score':\n",
      "0.992503, 'word': 'Google Pixel 8 Pro', 'start': 325, 'end': 343},\n",
      "{'entity_group': 'ORG', 'score': 0.96618426, 'word': 'Tech Haven', 'start': 551,\n",
      "'end': 561}, {'entity_group': 'PER', 'score': 0.98039377, 'word': 'Jamie',\n",
      "'start': 597, 'end': 602}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print_pipeline_info(ner_pipeline)\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 1330.93 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(ner_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Question Answering**\n",
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450\n",
      "parameters\n",
      "{'score': 0.11689740419387817, 'start': 222, 'end': 233, 'answer': 'stock\n",
      "issue'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\")\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "print_pipeline_info(qa_pipeline)\n",
    "question = \"What is the defect?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 260.77 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(qa_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Translation**\n",
      "Model: Helsinki-NLP/opus-mt-en-es, Size: 77,943,296 parameters\n",
      "[{'translation_text': 'Pedí el Samsung Galaxy S24 Ultra de Tech Haven, esperando\n",
      "la entrega del día siguiente, pero después de tres días, ni siquiera había\n",
      "recibido una actualización de envío. Después de esperar 45 minutos en espera, el\n",
      "servicio al cliente me dijo que había un problema de existencias — sin embargo,\n",
      "nadie me había informado! Cuando el paquete finalmente llegó una semana tarde,\n",
      "que contenía un Google Pixel 8 Pro en su lugar. El representante de apoyo era\n",
      "apologético, pero dijo que un intercambio tomaría otras dos semanas. Pagué\n",
      "$1,200 por el teléfono equivocado, trató con retrasos y mala comunicación, y\n",
      "ahora tienen que esperar aún más. Tech Haven, usted necesita hacer mejor!\n",
      "Sinceramente, Jamie.'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "print_pipeline_info(translation_pipeline)\n",
    "translation_result = translation_pipeline(text, max_length=200)\n",
    "print(translation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 312.03 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(translation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Text Generation**\n",
      "Model: openai-community/gpt2, Size: 124,439,808 parameters\n",
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day\n",
      "delivery,\n",
      "but after three days, I hadn’t even received a shipping update. After waiting 45\n",
      "minutes on hold,\n",
      "customer service told me there was a stock issue—yet no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead.\n",
      "The support rep was apologetic but said an exchange would take another two\n",
      "weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer.\n",
      "Tech Haven, you need to do better! Sincerely, Jamie.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up.\n",
      "\n",
      "However, due to the delay, the issue was resolved within 2 weeks, and the order\n",
      "has been delivered to its original address.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up.\n",
      "\n",
      "However, due to the delay, the issue was resolved within 2 weeks, and the order\n",
      "has been delivered to its original address.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n**Text Generation**\")\n",
    "generator_pipeline = pipeline(\"text-generation\", device=device)\n",
    "print_pipeline_info(generator_pipeline)\n",
    "response = \"Dear Jamie, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator_pipeline(prompt, max_length=200)\n",
    "generated_text = outputs[0]['generated_text']\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 511.15 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(generator_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Summarization**\n",
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n",
      "[{'summary_text': ' Tech Haven sent a Samsung Galaxy S24 Ultra to Tech Haven,\n",
      "expecting next-day delivery . The package arrived a week late, it contained a\n",
      "Google Pixel 8 Pro instead . An exchange would take another two weeks .'}]\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\", device=device)\n",
    "print_pipeline_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 1222.24 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(summarization_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 Unloading model: openai-community/gpt2 from GPU...\n",
      "✅ Model openai-community/gpt2 has been fully unloaded.\n",
      "🚀 Loading model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit (this may take a while)...\n",
      "🟢 Model unsloth/mistral-7b-instruct-v0.3-bnb-4bit loaded successfully.\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from introdl.nlp import llm_configure, llm_generate\n",
    "from introdl.utils import wrap_print_text, get_device\n",
    "\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "mistral_config = llm_configure(\"mistral\")\n",
    "response = llm_generate(mistral_config, \"What is the capital of France?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, \n",
    "but after three days, I hadn’t even received a shipping update. After waiting 45 minutes on hold, \n",
    "customer service told me there was a stock issue—yet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. \n",
    "The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. \n",
    "Tech Haven, you need to do better! Sincerely, Jamie.\"\"\"\n",
    "\n",
    "\n",
    "system_prompt_sentiment = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "response = llm_generate(mistral_config, user_prompt, system_prompt=system_prompt_sentiment)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy S24 Ultra\",\n",
      "      \"type\": \"Product\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Tech Haven\",\n",
      "      \"type\": \"Organization\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"$1,200\",\n",
      "      \"type\": \"CurrencyAmount\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Google Pixel 8 Pro\",\n",
      "      \"type\": \"Product\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_prompt_ner = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt_ner = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "response_ner = llm_generate(mistral_config, user_prompt_ner, system_prompt=system_prompt_ner)\n",
    "print(response_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue involves incorrect order delivery (Google Pixel 8 Pro instead of\n",
      "Samsung Galaxy S24 Ultra), delay in shipment, lack of timely communication about\n",
      "the stock issue, and further prolonged time for an exchange.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the issue?\"\n",
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. Be succinct\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(mistral_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimado Tech Haven, ordené el Samsung Galaxy S24 Ultra esperando entrega en un\n",
      "día,\n",
      "sin embargo, después de tres días, aún no había recibido una actualización de\n",
      "envío. Después de pasar cuarenta y cinco minutos en llamada por línea\n",
      "telefónica,\n",
      "el servicio al cliente me informó que existía un problema de stock — sin\n",
      "embargo, nadie me lo había comunicado previamente!\n",
      "Cuando finalmente llegó la paquetería retrasada por una semana, contenía un\n",
      "Google Pixel 8 Pro en su lugar. El representante de soporte se disculpó pero\n",
      "dijo que para realizar una intercambio necesitarían otras dos semanas.\n",
      "Pague $1,200 por el teléfono equivocado, tuve que lidiar con retras\n"
     ]
    }
   ],
   "source": [
    "system_prompt_translation = \"\"\"You are an expert translation model. Translate the following text from English to Spanish.\"\"\"\n",
    "user_prompt_translation = f\"Text: {text}\\nTranslation:\"\n",
    "\n",
    "response_translation = llm_generate(mistral_config, user_prompt_translation, system_prompt=system_prompt_translation)\n",
    "print(response_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Jamie experienced delayed delivery of the Samsung Galaxy S24 Ulta from\n",
      "Tech Haven, receiving a Google Pixel 8 Pro instead due to a stock issue that\n",
      "wasn't communicated. The customer service representative offered an exchange,\n",
      "but this process will also take additional time. Overall, Jamie is dissatisfied\n",
      "with the service provided by Tech Haven, citing issues with communication,\n",
      "delays, and incorrect product shipment.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_summarization = \"\"\"You are an expert summarization model. Summarize the following text in a concise manner.\"\"\"\n",
    "user_prompt_summarization = f\"Text: {text}\\nSummary:\"\n",
    "\n",
    "response_summarization = llm_generate(mistral_config, user_prompt_summarization, system_prompt=system_prompt_summarization)\n",
    "print(response_summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Jamie,\n",
      "Thank you for taking the time to share your experience with us at Tech Haven. We\n",
      "deeply regret the inconvenience you've encountered during your recent purchase\n",
      "of the Samsung Galaxy S24 Ultra.\n",
      "We understand that prompt delivery and clear communication are crucial aspects\n",
      "in maintaining trust with our customers, and we sincerely apologize for falling\n",
      "short of these expectations. The mix-up with the order is unacceptable, and we\n",
      "should have kept you updated about the stock issues earlier.\n",
      "To address this matter, we will immediately process a full refund for the\n",
      "incorrectly shipped device as well as cover any additional costs related to\n",
      "returning the Google Pixel 8 Pro back to us. Furthermore, we will prioritize\n",
      "sending out the correct product (Samsung Galaxy S24 Ultra) without delay, along\n",
      "with expedited shipping to ensure timely arrival.\n",
      "As a token of appreciation for your patience throughout this ordeal, we will\n"
     ]
    }
   ],
   "source": [
    "system_prompt_generation = \"\"\"You are an expert text generation model. Generate a customer service response based on the provided context.\"\"\"\n",
    "user_prompt_generation = f\"Context: {text}\\n\\nCustomer service response:\\n{response}\"\n",
    "\n",
    "response_generation = llm_generate(mistral_config, user_prompt_generation, system_prompt=system_prompt_generation)\n",
    "print(response_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\bagge\\my drive\\python_projects\\ds776_develop_project\\ds776\\lessons\\course_tools\\introdl\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: IPython in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (8.28.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (8.1.5)\n",
      "Requirement already satisfied: ipycanvas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.13.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.14.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.13.2)\n",
      "Requirement already satisfied: torch in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (4.66.5)\n",
      "Requirement already satisfied: pillow>=6.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipycanvas->introdl==1.0) (10.4.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from pandas->introdl==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from pandas->introdl==1.0) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (2024.6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jedi>=0.16->IPython->introdl==1.0) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->introdl==1.0) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->introdl==1.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jinja2->torch->introdl==1.0) (3.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from sympy->torch->introdl==1.0) (1.3.0)\n",
      "Building wheels for collected packages: introdl\n",
      "  Building wheel for introdl (pyproject.toml): started\n",
      "  Building wheel for introdl (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for introdl: filename=introdl-1.0-py3-none-any.whl size=41669 sha256=ef0876a17990433a5c0cf7239eec100df7f67d9a9e3cbccbe83cea9d2b439e2b\n",
      "  Stored in directory: C:\\Users\\bagge\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-2_41wy4l\\wheels\\f5\\d5\\0f\\11f1d5af64d00defb23fa33cf51b2946a0899888d73571e687\n",
      "Successfully built introdl\n",
      "Installing collected packages: introdl\n",
      "  Attempting uninstall: introdl\n",
      "    Found existing installation: introdl 1.0\n",
      "    Uninstalling introdl-1.0:\n",
      "      Successfully uninstalled introdl-1.0\n",
      "Successfully installed introdl-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ../Course_Tools/introdl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_config = llm_configure(\"gemini-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"Tell me a dad joke.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three interesting facts about space:\n",
      "1.  **There's a planet made of diamond:** Astronomers discovered a planet called\n",
      "55 Cancri e that is twice the size of Earth and made almost entirely of diamond.\n",
      "2.  **Space is completely silent:** Sound waves need a medium to travel, and\n",
      "space is a vacuum.\n",
      "3.  **The universe is still expanding:** Scientists have observed that the\n",
      "universe is not only expanding but that the expansion is accelerating.\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"I am testing the audio transcription abilities of Gemini. Please tell me three interesting facts about space.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am testing the audio transcription abilities of Gemini.\n",
      "Please tell me three interesting facts about space.\n"
     ]
    }
   ],
   "source": [
    "gemini_config = llm_configure(\"gemini-flash-lite\")\n",
    "\n",
    "system_prompt = \"\"\"You are an AI assistant that refines transcriptions.  \n",
    "The input is the transcription of an audio file.  \n",
    "Do not answer questions in the input or add any content beyond the original input.  \n",
    "Do edit the input transcript for clarity but keep topics in roughly the same order.  \n",
    "Use bullets if appropriate.\n",
    "\"\"\"\n",
    "input_prompt = \"\"\"I am testing the audio transcription abilities of Gemini. \n",
    "Please tell me three interesting facts about space.\n",
    "\"\"\"\n",
    "response = llm_generate(gemini_config, input_prompt, system_prompt=system_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
