{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS776 Auto-Update (runs in ~2 seconds, only updates when needed)\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "# Homework 11: Text Generation and Decoding Strategies\n\n**Name:** [Your Name Here]  \n**Total Points: 40**\n\n## Submission Checklist\n- [ ] All code cells executed with output saved\n- [ ] All questions answered in markdown cells\n- [ ] Used `DATA_PATH` and `MODELS_PATH` variables (no hardcoded paths)\n- [ ] Decoding strategies compared with visualizations\n- [ ] Creative application implemented\n- [ ] Reflection questions answered\n- [ ] Notebook exported to HTML\n- [ ] Canvas filename includes `_GRADE_THIS_ONE`\n- [ ] Files uploaded to Canvas\n\n---\n\n**Point Breakdown:**\n- Part 1 (Decoding Strategies): 10 pts\n- Part 2 (API Helper Functions): 8 pts\n- Part 3 (Model Size Comparison): 8 pts\n- Part 4 (Creative Application): 8 pts\n- Part 5 (Analysis and Comparison): 4 pts\n- Part 6 (Reflection): 2 pts"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1 - Decoding Strategies (10 pts)\n\nIn this part, you'll experiment with different decoding strategies to see how they affect generated text quality and diversity.\n\n**Model Suggestions for A6000 GPUs (48GB VRAM):**\n\n**Small models (3-4B parameters, ~2-3GB VRAM):**\n- `\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"` - Fast, good for testing\n- `\"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"` - Alternative 3B option\n\n**Medium models (7-8B parameters, ~5-6GB VRAM):**\n- `\"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\"` - Balanced quality/speed\n- `\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"` - Base 8B variant\n- `\"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"` - Strong 7B model\n\n**Large models (14B parameters, ~8-10GB VRAM):**\n- `\"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"` - High quality generation\n\n**Very large models (70B+ parameters, ~40-45GB VRAM):**\n- `\"unsloth/Meta-Llama-3.1-70B-bnb-4bit\"` - Maximum quality (use if you have exclusive GPU access)\n\n**Note:** All these models use 4-bit quantization (bnb-4bit) which reduces memory by ~4x compared to full precision. Start with a 3B or 8B model for faster experimentation, then try larger models if you want to compare quality.\n\n### 1.1 Understanding Model Outputs (2 pts)\n\nBefore implementing decoding strategies, you need to understand how to extract and display token probabilities from the model.\n\n**Instructions:**\n\n1. Load a text generation model (use one of the quantized models suggested above)\n2. Define a prompt and get the model's output for the next token prediction\n3. Complete the provided `get_top_tokens_with_probs()` function to:\n   - Extract logits from model output\n   - Convert logits to probabilities using softmax\n   - Return the top-k tokens with their probabilities\n4. Create a visualization showing the probability distribution\n5. Answer the analysis questions below\n\n**Hints:**\n- Review Section 6 in the lesson for similar examples\n- Use `torch.softmax()` to convert logits to probabilities\n- Use `torch.topk()` to get the top tokens\n- Probabilities should sum to approximately 1.0"
  },
  {
   "cell_type": "markdown",
   "id": "axw0bqrbeg",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_11_Models/` | `Homework_11_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` ‚Äî never hardcode paths like `'Homework_11_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # or another model from suggestions\n",
    "\n",
    "# YOUR CODE: Load tokenizer and model\n",
    "\n",
    "\n",
    "# YOUR CODE: Define a prompt and tokenize it\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "\n",
    "def get_top_tokens_with_probs(model, input_ids, k=10):\n",
    "    \"\"\"\n",
    "    Extract top-k tokens and their probabilities for next token prediction.\n",
    "\n",
    "    Args:\n",
    "        model: The language model\n",
    "        input_ids: Tokenized input (tensor)\n",
    "        k: Number of top tokens to return\n",
    "\n",
    "    Returns:\n",
    "        List of (token_string, probability) tuples\n",
    "\n",
    "    Hints:\n",
    "    - Get model output: output = model(input_ids)\n",
    "    - Extract logits for last position: logits = output.logits[0, -1, :]\n",
    "    - Convert to probabilities: probs = torch.softmax(logits, dim=-1)\n",
    "    - Get top-k: top_probs, top_ids = torch.topk(probs, k=k)\n",
    "    - Decode tokens: tokenizer.decode(token_id)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_probability_distribution(probabilities, title=\"Token Probabilities\"):\n",
    "    \"\"\"\n",
    "    Plot probability distribution for top 50 tokens and cumulative probability.\n",
    "\n",
    "    Hints:\n",
    "    - Sort probabilities: sorted_probs, _ = torch.sort(probs, descending=True)\n",
    "    - Use plt.bar() for histogram\n",
    "    - Use torch.cumsum() for cumulative probability\n",
    "    - Create a 2-subplot figure\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# YOUR CODE: Call the functions and display results\n",
    "# 1. Get top 10 tokens with probabilities\n",
    "# 2. Print them in a formatted table\n",
    "# 3. Create probability distribution plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Implement Three Decoding Strategies (5 pts)\n\nNow implement three core decoding strategies and compare their behavior. Use the **same prompt** for all three strategies so you can compare the results fairly.\n\n**Strategy 1: Greedy Search**\n- `do_sample=False` (no sampling parameter needed)\n- Always picks the token with highest probability\n- Deterministic - same output every time\n\n**Strategy 2: Beam Search with No-Repeat N-Gram**\n- `num_beams=5`\n- `no_repeat_ngram_size=2` (prevents repetitive phrases)\n- `do_sample=False`\n- Explores multiple paths simultaneously\n\n**Strategy 3: Nucleus (Top-P) Sampling**\n- `do_sample=True`\n- `top_p=0.9` (sample from tokens covering 90% probability mass)\n- `temperature=0.7` (slightly lower than default for more focused generation)\n- Stochastic - different output each time\n\n**For each strategy:**\n1. Generate text (max_length=100 or max_new_tokens=50)\n2. Display the generated text\n3. Show the top 5 token probabilities at **three key timesteps**:\n   - Beginning (token 1 or 2 after prompt)\n   - Middle (around token 25)\n   - End (last few tokens)\n4. Create a comparison visualization\n5. Comment on the text quality and probability patterns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Strategy 1: Greedy Search\n",
    "\n",
    "prompt = \"In a shocking scientific discovery, researchers found\"  # Use your own prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY 1: GREEDY SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate with greedy search\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Display generated text\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Show probability distributions at key timesteps\n",
    "# Hint: You'll need to call the model again with the generated sequence\n",
    "#       to get probabilities at different positions\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Strategy 2: Beam Search with No-Repeat N-Gram\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STRATEGY 2: BEAM SEARCH (num_beams=5, no_repeat_ngram_size=2)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ou0romoktd",
   "source": "# YOUR CODE HERE\n# Strategy 3: Nucleus (Top-P) Sampling\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"STRATEGY 3: NUCLEUS SAMPLING (top_p=0.9, temperature=0.7)\")\nprint(\"=\" * 70)\n\n# YOUR CODE HERE",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 1.3 Comparison and Analysis (3 pts)\n\nWrite 2-3 paragraphs comparing the three decoding strategies. Address:\n\n**Text Quality:**\n- Which strategy produced the most coherent text?\n- Did any strategy produce repetitive text?\n- Which was most creative/diverse?\n\n**Probability Patterns:**\n- How did the probability distributions differ between strategies?\n- Did greedy search show higher confidence (more peaked distributions)?\n- How did sampling affect the probability spread?\n- Did you notice the distributions changing from beginning to end of generation?\n\n**Use Case Recommendations:**\n- When would you use greedy search?\n- When would beam search be preferred?\n- When would you choose nucleus sampling?\n- What trade-offs exist between coherence and diversity?\n\nüìù **YOUR ANALYSIS HERE:**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2 - API Helper Functions (8 pts)\n\nIn Lesson 11, Section 5.5, you learned about the `Conversation` class for building chatbots with APIs. In Section 7.5, you learned about decoding parameters (temperature, top_p, max_tokens) that control text generation behavior.\n\nIn this part, you'll combine these concepts by extending the `Conversation` class to support decoding parameter control, allowing you to fine-tune the chatbot's behavior for different use cases.\n\n**Learning Objectives:**\n- Practice working with chat roles (system/user/assistant) and conversation history\n- Understand how decoding parameters affect model behavior\n- Apply object-oriented design patterns to build maintainable APIs\n- Test different parameter combinations for various tasks"
  },
  {
   "cell_type": "code",
   "id": "2znxhkpqnb5",
   "source": "# YOUR CODE HERE\n# Import necessary libraries\nimport os\nfrom openai import OpenAI\n\n# Copy and extend the Conversation class\nclass Conversation:\n    \"\"\"\n    Extended chatbot class with decoding parameter control.\n\n    Args:\n        api_key (str): OpenRouter API key\n        model (str): Model name (e.g., 'google/gemini-2.5-flash-lite')\n        system_prompt (str, optional): Initial instructions for the model\n        temperature (float): Sampling temperature (default: 0.7)\n        top_p (float): Nucleus sampling parameter (default: 0.9)\n        max_tokens (int): Maximum tokens to generate (default: 256)\n    \"\"\"\n\n    def __init__(self, api_key, model=\"google/gemini-2.5-flash-lite\",\n                 system_prompt=None, temperature=0.7, top_p=0.9, max_tokens=256):\n        # YOUR CODE HERE\n        pass\n\n    def send(self, user_message, temperature=None, top_p=None, max_tokens=None):\n        \"\"\"\n        Send a message and get a response.\n\n        Args:\n            user_message (str): The user's input\n            temperature (float, optional): Override default temperature\n            top_p (float, optional): Override default top_p\n            max_tokens (int, optional): Override default max_tokens\n\n        Returns:\n            str: The assistant's response\n        \"\"\"\n        # YOUR CODE HERE\n        pass\n\n    def display(self, show_recent_only=True):\n        \"\"\"Show conversation history (you can use a simple print for now)\"\"\"\n        # YOUR CODE HERE\n        pass\n\n    def reset(self, system_prompt=None):\n        \"\"\"Reset conversation history\"\"\"\n        # YOUR CODE HERE\n        pass\n\nprint(\"‚úì Extended Conversation class defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uuied4a4kb",
   "source": "# YOUR CODE HERE\n# Recipe 1: Factual Question Answering\n\nprint(\"=\" * 70)\nprint(\"RECIPE 1: FACTUAL QUESTION ANSWERING\")\nprint(\"(temperature=0, top_p=1.0, max_tokens=150)\")\nprint(\"=\" * 70)\n\n# Create chatbot and test with multi-turn conversation\n# YOUR CODE HERE",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ywh29kq8pos",
   "source": "## Part 3 - Model Size Comparison (8 pts)\n\nDifferent model sizes offer different trade-offs between quality and computational cost. In this part, you'll compare text generation across different model sizes.\n\n**Tasks:**\n1. Load at least two different-sized models (e.g., 3B and 8B parameter models). Suggested options:\n   - \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" (3B parameters, 4-bit quantized)\n   - \"unsloth/Llama-3.1-8B-Instruct-bnb-4bit\" (8B parameters, 4-bit quantized)\n   \n2. Create 3 diverse prompts that test different capabilities:\n   - A factual knowledge question\n   - A creative writing task\n   - A reasoning/problem-solving task\n   \n3. Generate responses from each model for each prompt using the same decoding parameters (e.g., temperature=0.7, top_p=0.9)\n\n4. For each generation, measure and record:\n   - Generation time\n   - Memory usage (if possible)\n   - Response length\n   - Qualitative assessment of quality\n   \n5. Create visualizations:\n   - Bar chart comparing generation times\n   - Any other relevant comparisons\n   \n6. Write an analysis (2-3 paragraphs) discussing:\n   - Performance differences between model sizes\n   - Quality differences in outputs\n   - When you would choose each model size\n   - Trade-offs between speed and quality",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "r421o9vraid",
   "source": "# YOUR CODE HERE\n# Part 3: Model Size Comparison\n\n# Load and compare different model sizes\n# Suggested: Compare a 3B model vs an 8B model\n\nimport time\n\n# Define your test prompts\nprompts = [\n    \"Explain how photosynthesis works in plants.\",  # Factual\n    \"Write a short poem about the ocean at sunset.\",  # Creative\n    \"If a train travels 60 mph for 2.5 hours, how far does it go? Show your reasoning.\"  # Reasoning\n]\n\n# YOUR CODE: Load models, generate responses, measure performance\n# Track: generation time, response length, quality assessment",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3qa2pehzpkm",
   "source": "üìù **YOUR PART 3 ANALYSIS HERE:**\n\nDiscuss your findings on model size comparison (2-3 paragraphs):\n- Performance differences between model sizes\n- Quality differences in outputs\n- When you would choose each model size\n- Trade-offs between speed and quality",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "aq4hy50dr26",
   "source": "## Part 4 - Creative Application (8 pts)\n\nDesign and implement a creative text generation application that combines the techniques you've learned. Choose ONE of the following options or propose your own:\n\n**Option A: Interactive Story Generator**\n- Create a choose-your-own-adventure style story\n- Use different temperature settings for different story branches\n- Allow user choices to influence the narrative direction\n\n**Option B: Writing Style Mimicry**\n- Prompt the model to write in different author styles (e.g., Shakespeare, Hemingway, technical documentation)\n- Compare outputs across different styles\n- Analyze how well the model captures stylistic elements\n\n**Option C: Multi-Agent Conversation**\n- Create multiple chatbot personas with different system prompts\n- Have them \"discuss\" a topic with each other\n- Experiment with different temperature settings for different personalities\n\n**Option D: Custom Application**\n- Propose your own creative application\n- Must demonstrate understanding of decoding strategies and/or API-based generation\n- Should be something you find interesting or useful\n\n**Requirements:**\n1. Implement your chosen application with working code\n2. Use appropriate decoding parameters for your use case\n3. Generate at least 3-5 example outputs\n4. Write a brief explanation (1-2 paragraphs) of:\n   - What you built and why\n   - What parameters you chose and why\n   - What worked well and what could be improved",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7xtkp2twbnv",
   "source": "# YOUR CODE HERE\n# Part 4: Creative Application\n\n# Implement your chosen option (A, B, C, or D)\n# Remember to:\n# 1. Use appropriate decoding parameters\n# 2. Generate 3-5 example outputs\n# 3. Document your parameter choices",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "üìù **YOUR PART 4 EXPLANATION HERE:**\n\n- What did you build and why?\n- What parameters did you choose and why?\n- What worked well and what could be improved?"
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": "## Part 5 - Analysis and Comparison (4 pts)\n\nSynthesize your findings from all the previous parts into a comprehensive analysis.\n\n**Write 2-3 paragraphs addressing:**\n\n1. **Local vs. API-Based Generation:**\n   - Compare the experience of using local models vs. API-based models\n   - Discuss trade-offs in terms of control, cost, latency, and quality\n   - When would you choose each approach for a production application?\n\n2. **Decoding Strategy Selection:**\n   - Which decoding strategies performed best for different types of tasks?\n   - How did you balance coherence and diversity in your experiments?\n   - What general principles would you follow when choosing decoding parameters?\n\n3. **Practical Applications:**\n   - What real-world applications would benefit from the techniques you explored?\n   - What challenges did you encounter that would need to be addressed for production use?\n   - What additional features or improvements would make your implementations more robust?\n\nüìù **YOUR ANALYSIS HERE:**"
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": "## Part 6 - Reflection (2 pts)\n\n* What, if anything, did you find difficult to understand for the lesson? Why?\n\nüìù **YOUR ANSWER HERE:**\n\n* What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n\nüìù **YOUR ANSWER HERE:**"
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n",
    "\n",
    "Uncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n",
    "# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}