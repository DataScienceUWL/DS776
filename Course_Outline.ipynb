{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55beec3a",
   "metadata": {},
   "source": [
    "# Deep Learning Course Syllabus\n",
    "\n",
    "## Course Outcomes:\n",
    "1. **Understand Core Neural Network Architectures**: Describe and differentiate key neural network architectures, including convolutional neural networks (CNNs) and transformers, and their applications in computer vision and NLP.\n",
    "2. **Develop and Train Models in PyTorch**: Use PyTorch to build, train, and fine-tune neural network models for both computer vision and NLP tasks.\n",
    "3. **Apply Deep Learning to Computer Vision**: Implement CNNs and advanced architectures for computer vision applications, such as object detection, image classification, and segmentation.\n",
    "4. **Implement Transformer-Based Models for NLP**: Utilize transformer models to perform NLP tasks such as text classification, named entity recognition, text generation, and summarization.\n",
    "5. **Leverage the Hugging Face Ecosystem for NLP**: Use Hugging Face’s models and tools to explore, fine-tune, and experiment with transformer-based NLP models.\n",
    "6. **Evaluate and Optimize Model Performance**: Assess model performance for both computer vision and NLP applications, using appropriate metrics to interpret results and improve accuracy.\n",
    "\n",
    "## Textbooks and Other Resources\n",
    "\n",
    "### Required Books\n",
    "- [\"Inside Deep Learning: Math, Algorithms, Models\" by Edward Raff](https://www.amazon.com/Inside-Deep-Learning-Algorithms-Models/dp/1617298638/ref=sr_1_1?crid=32FZFJXN81JCC&dib=eyJ2IjoiMSJ9.pVb-gd6L20N_JNl6QLReqW_Ru_I-npjeZRSDzDujZg8qkJUpi9vRQ11tK-Jgv8lJ_9lJ1BDFVrq5KsdvYr5tTp3P4Hynb4DU_dMAgDRBpaU.ZXz9Dxm_mU9LEpRx8Jzgjul9xWXc51ZWqoDiGCQh-gg&dib_tag=se&keywords=inside+deep+learning+math%2C+algorithms%2C+models&qid=1729870668&sprefix=inside+deep+lea%2Caps%2C137&sr=8-1)\n",
    "- [\"Natural Language Processing with Transformers, Revised Edition\" by Tunstall, et al](https://www.amazon.com/Natural-Language-Processing-Transformers-Revised/dp/1098136799/ref=sr_1_1?crid=3CVB0BRZHS7GI&dib=eyJ2IjoiMSJ9.n87OC6jhL6N-C2KDf_e-0bXcjttnGT8821PDe5oHt6z9QpOBt1y-wdQTqxUx94wbrYfbvOcm14q2FYDNw9bD0oGmvMbTLDE8v4P0Zb0WAp08rxbUkdsp0uVxm55I-M4Z3Vk85S2-F0nzD07rym-NJKNx6BcIMyZNkUn6WoL7AJ9HGN8Wr_iOxRbgyu1lSgCj3IbiFEYQaH36C4bo0GDjMBvdttOga5FQxOUvKyDSSCc.QEAnSA-iAS3aC3D2zzqjB8j9f98PPvlrJS5EHlhjcmU&dib_tag=se&keywords=natural+language+processing+with+transformers&qid=1729870809&sprefix=natural+langua%2Caps%2C136&sr=8-1)\n",
    "\n",
    "###  Free Online Books\n",
    "- [Dive Into Deep Learning](https://d2l.ai/)  Shows how to code in multiple frameworks, but you'll need to use their packages to get things up and running.  \n",
    "- [Understanding Deep Learning](https://udlbook.github.io/udlbook/)  Explains a lot of the mathematical details about various deep learning models.  Great diagrams, but not light reading.\n",
    "- [Deep Learning](https://www.deeplearningbook.org/)  From MIT.  Good book for the first half of the class.\n",
    "\n",
    "### Other Resources:\n",
    "- **Documentation:**\n",
    "    - **Hugging Face Documentation:** [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "    - **PyTorch Documentation:** [PyTorch](https://pytorch.org/docs/stable/index.html)\n",
    "- **Tutorials:**\n",
    "    - **PyTorch Tutorials:** The [official PyTorch tutorials](https://pytorch.org/tutorials/) are quite good.\n",
    "\n",
    "- **Classes:**\n",
    "    - [Math for Machine Learning and Data Science](https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science?) Coursera three course specialization: linear algebra, calculus, and probability and statistics.  This 3-month specialization is highly recommended for a deeper dive.\n",
    "    - **Introduction to Deep Learning from MIT**  [Free public course](http://introtodeeplearning.com/) updated annually.  The beginning lecture is very accessible.\n",
    "    - **Neural Networks and Deep Learning.** Andrew Ng is a big name in AI and I love listening to him.  This [playlist](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0) is all the videos from the first course of his Deep Learning Specialization on Coursera.  This is a really great resource for a deep learning introduction.\n",
    "\n",
    "- **Effective AI Use:**\n",
    "    * [OpenAI’s Guide on Prompt Engineering](https://platform.openai.com/docs/guides/completion) This guide provides strategies for crafting prompts to achieve the best results when interacting with LLMs, explaining concepts like clarity, specificity, and context.\n",
    "    Hugging Face’s Prompting Techniques Blog Post:\n",
    "    * [Hugging Face’s blog post](https://platform.openai.com/docs/guides/completion) covers prompt design for various NLP tasks, with examples on how different prompt structures affect model outputs.\n",
    "    * [DeepLearning.AI’s \"ChatGPT Prompt Engineering for Developers\" (Free Course)](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) This short course, created in partnership with OpenAI, introduces prompt engineering techniques for using ChatGPT effectively. It includes practical exercises on structuring prompts and handling more complex queries. Available here.\n",
    "    * [Using Github Copilot in Visual Studio Code](https://docs.github.com/en/copilot/getting-started-with-github-copilot/getting-started-with-github-copilot-in-visual-studio-code) This official guide from GitHub provides detailed instructions on setting up and using GitHub Copilot in VS Code. Copilot is a powerful AI-driven code completion tool that suggests context-aware code snippets as you type.\n",
    "    * [Sourcegraph's Cody Extension for VS Code](https://sourcegraph.com/cody)  An extension for VS Code that let's you choose which large language model to use.  Some reviews suggest using Claude 3.5 Sonnet works better than OpenAI's models (which powers Github Copilot).  The [Sourcegraph blog](https://sourcegraph.com/blog) is a good source for updates and case studies using Cody."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9709315",
   "metadata": {},
   "source": [
    "## Weekly Topics\n",
    "\n",
    "- Lesson 1: Introduction to Deep Learning and Fully Connected Networks\n",
    "- Lesson 2: Convolutional Neural Networks\n",
    "- Lesson 3: Better Network Training\n",
    "- Lesson 4: Improved Network Architectures\n",
    "- Lesson 5: Transfer Learning\n",
    "- Lesson 6: Object Detection and Segmentation\n",
    "- Lesson 7: Introduction to Transformers, Hugging Face, and Using LLMs Effectively\n",
    "- Lesson 8: Transformer Internals - Self-Attention and Positional Encoding\n",
    "- Lesson 9: Text Classification with Transformers\n",
    "- Lesson 10: Named Entity Recognition (NER) and Tokenization\n",
    "- Lesson 11: Text Generation and Decoding Strategies\n",
    "- Lesson 12: Summarization with Transformers\n",
    "- Lesson 13-14: Final Project (unpolished)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a494b8a",
   "metadata": {},
   "source": [
    "## Lesson 1: Introduction to Deep Learning and Fully Connected Networks\n",
    "\n",
    "### Topics\n",
    "* Introduction to PyTorch\n",
    "* Fully-connected neural networks for regression and classification\n",
    "* Modeling process: prepare data, prepare model, train model, evaluate model, make predictions\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand deep learning fundamentals**, including the structure of neural networks.\n",
    "2. **Utilize PyTorch datasets and data loaders** to efficiently handle and preprocess data for training deep learning models.\n",
    "3. **Develop and train basic fully connected neural networks** in PyTorch, applying optimization techniques and appropriate loss functions.\n",
    "4. **Evaluate model performance** by plotting loss functions and other metrics.\n",
    "5. **Make predictions for new data** by applying a trained neural network to new inputs.\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* **Course Intro Notebook / Video** (Still to come)\n",
    "\n",
    "* **(Optional) Review Neural Networks from DS740**\n",
    "\n",
    "    * You might want to review the first 14 slides of the [Lesson on Neural networks in DS740](https://media.uwex.edu/content/ds/ds740_r23/ds740_artificial-neural-networks.sbproj/).  We're covering similar material this week.  Don't review the material about neural networks in R since we'll be using Python.\n",
    "\n",
    "* **Readings from Inside Deep Learning (IDL)**\n",
    "    * Read Chapter 1, pay attention to 1.2, 1.4, and 1.5 and skim the rest.\n",
    "    * Read Chapter 2, don't worry about backpropagation or automatic differentiation.  Don't worry too much about code details since you'll see more examples in our notebooks.\n",
    "\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included in the lesson folder and watch the embedded video.  You can read along and work through the code examples as needed.  The notebooks for this lesson are in the Lesson_01 directory.  The notebooks are numbered in the order they should be used.\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "### Auxiliary Materials\n",
    "\n",
    "- **Background Mathematics from Dr. Anne Hsu:**\n",
    "    * [Matrices and Vectors](https://www.youtube.com/watch?v=sM2Mm6aT_HI)\n",
    "    * [Derivatives and Gradients 1](https://www.youtube.com/watch?v=Fiw0_w4AykA)\n",
    "    * [Derivatives and Gradients 2](https://www.youtube.com/watch?v=qORZmKCB0g8)\n",
    "    * [Playlist for entire Intro to Deep Learning](https://www.youtube.com/@drannehsu/playlists)\n",
    "- **Object-Oriented Programming** It helps to have a basic familiarity with classes, inheritance, and methods for finding your way around in PyTorch.  [Real Python has a great tutorial](https://realpython.com/python3-object-oriented-programming/) on the basics of OOP with many code examples that you can either read or watch (40 minutes).\n",
    "- **Deep Learning Basics: Introduction and Overview**  Lex Fridman is a well known podcast host for AI and Data Science.  Here he gives an [introductory lecture](https://youtu.be/O5xeyoRL95U?si=SrM7RLWB_iBPMiK4) for MIT's public deep learning class.  Since it was recorded in 2019 it doesn't include the latest on transformer architectures that are driving the current boom in AI (ChatGPT, etc.), it's still a great introduction that discusses many applications of deep learning.  Watch this if you want a good overview.  I also recommend his podcast.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1b266",
   "metadata": {},
   "source": [
    "## Lesson 2: Convolutional Neural Networks\n",
    "\n",
    "### Topics\n",
    "* Image data\n",
    "* Convolutional Layers\n",
    "* Pooling Layers\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand the structure and role of Convolutional Neural Networks (CNNs)** in processing spatial data like images.\n",
    "2. **Design and implement basic CNN architectures** in PyTorch, including convolutional layers, pooling, and activation functions.\n",
    "3. **Understand how padding, stride, kernel size, and the number of output channels** interact to determine the dimensionality of the output in each convolutional layer.\n",
    "### Readings and Videos\n",
    "\n",
    "* Read Chapter 3, through Section 3.5, in Inside Deep Learning.  You can read 3.6 if you wish, we'll get into that material in the next lesson.\n",
    "* [Andrew Ng on Convolution over Volumes](https://www.youtube.com/watch?v=KTB_OFoAQcc&ab_channel=DeepLearningAI) This is one of many videos Andrew Ng has made to support his deep learning course.  Watch this to solidify your understanding convolutions after doing the reading.  About 11 minutes.\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_02 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9a1de",
   "metadata": {},
   "source": [
    "## Lesson 3: Better Network Training\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Apply data augmentation techniques** to improve model generalization, especially with small datasets.\n",
    "2. **Understand and implement learning rate schedules**, including exponential decay, step drop, and cosine annealing.\n",
    "3. **Optimize training** using modern techniques like SGD with momentum, Adam, and gradient clipping.\n",
    "4. **Implement early stopping** to prevent overfitting and improve training efficiency by halting training when performance plateaus.\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* Read Sections 3.6, 5.1-5.3 in Inside Deep Learning.\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_03 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007c0ad",
   "metadata": {},
   "source": [
    "## Lesson 4: Improved Network Architectures\n",
    "\n",
    "### Outcomes\n",
    "* **Understand and apply ReLU and LeakyReLU activations** to address vanishing gradient problems and enhance network convergence.\n",
    "* **Implement batch and layer normalization** to stabilize training and improve network performance.\n",
    "* **Analyze and utilize residual connections** to enable deeper network architectures by mitigating vanishing gradient issues.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read Sections 6.1-6.4 from Inside Deep Learning\n",
    "* **Course Notebooks with Videos** Open each of the notebooks included the lesson folder and watch the embedded video. You can read along and work through the code examples as you want. The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "* Complete the reading quiz in Canvas (10 points).\n",
    "* Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e86c4",
   "metadata": {},
   "source": [
    "## Lesson 5: Transfer Learning\n",
    "\n",
    "### Topics\n",
    "* Sources for pre-trained models:  Torch Hub, TIMM, Hugging Face, and more\n",
    "* Identifying the layers that need tweaking\n",
    "* Freezing and unfreezing Layers\n",
    "* The small data problem\n",
    "* Fine-tuning vs full Training\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Explain Transfer Learning**: Describe the concept and benefits of transfer learning for leveraging pre-trained models on new tasks.\n",
    "\n",
    "2. **Implement Model Parameter Transfer**: Apply a pre-trained model’s weights to a new problem by modifying only specific layers.\n",
    "\n",
    "3. **Optimize Training with Limited Data**: Use transfer learning techniques to improve model performance with smaller labeled datasets.\n",
    "\n",
    "4. **Adapt CNNs for New Tasks**: Fine-tune models for target datasets by adjusting layers and applying warm or frozen weights.\n",
    "\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* Read 13.1-13.3 in Inside Deep Learning\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_02 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aeb8b7",
   "metadata": {},
   "source": [
    "## Lesson 6: Object Detection and Segmentation\n",
    "\n",
    "### Topics\n",
    "* Transposed Convolutions\n",
    "* U-Net for Segmentation\n",
    "* R-CNN for Object Detection\n",
    "* Other architectures\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Differentiate Image Segmentation and Object Detection**: Explain the roles of pixel-level segmentation and object detection with bounding boxes.\n",
    "   \n",
    "2. **Build an Image Segmentation Model**: Implement segmentation using transposed convolutions and U-Net architecture.\n",
    "\n",
    "3. **Apply Bounding Box Detection**: Use Faster R-CNN for object detection with bounding boxes and assess precision trade-offs.\n",
    "\n",
    "4. **Reduce False Positives**: Explore filtering methods to improve detection accuracy by minimizing overlapping boxes.\n",
    "\n",
    "### Readings and Videos\n",
    "\n",
    "* Read Chapter 8 in Inside Deep Learning\n",
    "* **Course Notebooks with Videos**  Open each of the notebooks included the lesson folder and watch the embedded video.  You can read along and work through the code examples as you want.  The notebooks for this lesson are in the Lesson_02 directory.  The notebooks are numbered in the order they should be used.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "1.  Complete the reading quiz in Canvas (10 points).\n",
    "2.  Complete the exercises in your the homework notebook in CoCalc (40 points).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961cdc19",
   "metadata": {},
   "source": [
    "## Lesson 7: Introduction to Transformers, Hugging Face, and Using LLMs Effectively\n",
    "\n",
    "### Topics\n",
    "* Transformer architecture overview\n",
    "* Self-attention mechanism\n",
    "* Hugging Face library introduction\n",
    "* Effective use of large language models (LLMs) and prompt engineering basics\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Describe Transformer Architecture**: Identify the core components of transformers, including the encoder-decoder structure and the role of self-attention.\n",
    "   \n",
    "2. **Explain Self-Attention Basics**: Outline how self-attention works, including the function of queries, keys, and values, and the advantages it provides for handling context.\n",
    "\n",
    "3. **Use Pre-trained Models**: Load and use a pre-trained Hugging Face model for a simple NLP task to gain familiarity with the Hugging Face transformers library.\n",
    "\n",
    "4. **Improve Prompting Techniques**: Experiment with prompt engineering to enhance response quality when interacting with LLMs, learning to refine prompts for clarity and relevance.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 1: Hello Transformers* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each of the notebooks included in the Lesson_07 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5966839",
   "metadata": {},
   "source": [
    "## Lesson 8: Transformer Internals - Self-Attention and Positional Encoding\n",
    "\n",
    "### Topics\n",
    "* In-depth self-attention mechanics\n",
    "* Multi-headed attention and context capture\n",
    "* Positional encoding and sequence structure\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand Self-Attention Calculations**: Demonstrate how self-attention works by calculating attention scores for a simple sequence, showing the process of computing queries, keys, and values.\n",
    "   \n",
    "2. **Explain Multi-Headed Attention**: Describe the purpose of multi-headed attention in transformers and explain how multiple attention heads provide richer contextual understanding.\n",
    "\n",
    "3. **Discuss Positional Encoding**: Explain the role of positional encoding in maintaining sequence order within transformers and understand its mathematical formulation.\n",
    "\n",
    "4. **Experiment with Attention Mechanisms**: Use Hugging Face models to observe how multi-headed attention and positional encoding affect outputs.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 3: Transformer Anatomy* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each of the notebooks in the Lesson_08 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a067d",
   "metadata": {},
   "source": [
    "## Lesson 9: Text Classification with Transformers\n",
    "\n",
    "### Topics\n",
    "* Fine-tuning transformers for text classification\n",
    "* Tokenization and data preprocessing for NLP tasks\n",
    "* Using the Hugging Face Trainer API for efficient model training\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Explain the Fine-Tuning Process**: Describe how transformers are fine-tuned for text classification tasks, focusing on modifying specific layers and adjusting model parameters.\n",
    "   \n",
    "2. **Use Tokenization for Classification Tasks**: Use Hugging Face’s `AutoTokenizer` to tokenize and preprocess input text for classification, understanding the effect of different tokenization strategies on model input.\n",
    "\n",
    "3. **Fine-Tune a Transformer Model**: Fine-tune a BERT model (or similar) on a classification dataset, adjusting hyperparameters to improve model performance.\n",
    "\n",
    "4. **Evaluate Model Performance**: Analyze the model’s accuracy on the validation set, learning to interpret common metrics (accuracy, F1) and assess model quality.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 2: Text Classification* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_09 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c55729",
   "metadata": {},
   "source": [
    "## Lesson 10: Named Entity Recognition (NER) and Tokenization\n",
    "\n",
    "### Topics\n",
    "* Overview of Named Entity Recognition (NER)\n",
    "* Subword tokenization techniques (e.g., BPE, WordPiece)\n",
    "* Multilingual considerations in tokenization and NER\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Explain Named Entity Recognition (NER)**: Define NER and its applications, identifying how it is used to label specific entities (e.g., names, locations) in text.\n",
    "   \n",
    "2. **Differentiate Tokenization Methods**: Describe different tokenization techniques (e.g., Byte-Pair Encoding, WordPiece) and their relevance in NER and multilingual settings.\n",
    "\n",
    "3. **Apply NER with Transformers**: Fine-tune a transformer model for NER tasks, learning how token-level classification works in transformers.\n",
    "\n",
    "4. **Discuss Multilingual Challenges**: Explain tokenization and NER challenges in multilingual contexts, including handling multiple languages and out-of-vocabulary (OOV) words.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 4: Multilingual Named Entity Recognition* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_10 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890e094",
   "metadata": {},
   "source": [
    "## Lesson 11: Text Generation and Decoding Strategies\n",
    "\n",
    "### Topics\n",
    "* Overview of transformer-based text generation\n",
    "* Decoding strategies: Greedy search, beam search, top-k sampling, and nucleus sampling\n",
    "* Applications of text generation in NLP\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Explain Text Generation Basics**: Describe how transformer models like GPT-2 generate text, and identify common applications of text generation, such as chatbots, content creation, and automated summarization.\n",
    "   \n",
    "2. **Use Decoding Strategies**: Implement and compare different decoding methods (e.g., greedy search, beam search, top-k sampling, nucleus sampling) to observe how each affects text generation quality.\n",
    "\n",
    "3. **Evaluate Generated Text**: Assess the quality of generated text, discussing trade-offs in coherence, creativity, and relevance with different decoding strategies.\n",
    "\n",
    "4. **Identify Real-World Applications**: Explain practical uses of text generation in industries like customer service, media, and content creation, understanding the strengths and limitations of transformer-based text generation.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 5: Text Generation* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_11 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d6867",
   "metadata": {},
   "source": [
    "## Lesson 12: Summarization with Transformers\n",
    "\n",
    "### Topics\n",
    "* Extractive vs. abstractive summarization\n",
    "* Transformer models for summarization (e.g., BART, T5)\n",
    "* Evaluation metrics for summarization (e.g., ROUGE scores)\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "1. **Understand Summarization Approaches**: Differentiate between extractive and abstractive summarization, identifying the benefits and limitations of each approach.\n",
    "   \n",
    "2. **Fine-Tune a Summarization Model**: Use a transformer model, such as BART or T5, to perform summarization on a dataset, focusing on fine-tuning techniques for high-quality summary generation.\n",
    "\n",
    "3. **Evaluate Summaries**: Apply evaluation metrics like ROUGE to assess the relevance, coherence, and completeness of generated summaries.\n",
    "\n",
    "4. **Discuss Summarization Applications**: Identify practical applications of summarization in fields like news, research, customer service, and document management.\n",
    "\n",
    "### Readings and Videos\n",
    "* Read *Chapter 6: Summarization* in *Natural Language Processing with Transformers*\n",
    "* **Course Notebooks with Videos**: Open each notebook in the Lesson_12 directory and watch the embedded videos in the recommended order.\n",
    "\n",
    "### Assessments\n",
    "1. Complete the reading quiz in Canvas (10 points).\n",
    "2. Complete the exercises in your homework notebook in CoCalc (40 points).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f3d2b",
   "metadata": {},
   "source": [
    "## Lesson 13-14: Final Project (unpolished)\n",
    "\n",
    "In the final two weeks of the course you will either:\n",
    "\n",
    "* Investigate a new topic from one of our textbooks on your own and produce a notebook that introduces the topic, explains a bit about how it works, and demonstrates one or more computational experiments related topic.  Your goal is to produce an \"educational exposition\" that highlights the topic so that it could be read by one of your peers for them to supplement their own study of the corresponding textbook chapter.  It would be similar to any of the notebooks I've provided for the class.\n",
    "\n",
    "* Apply one of the topics we've covered to a new dataset or in a new way.  For example, ...\n",
    "\n",
    "### Assessments\n",
    "* At the end Week 13 you must have a very rough draft of your notebook in CoCalc.  It should outline your topic and have the beginnings of the code you will use. (20 pts)\n",
    "* At the end of Week 14 you must submit your final notebook in the Final Project folder in CoCalc.  (80 pts)\n",
    "\n",
    "#### Rubric:\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
