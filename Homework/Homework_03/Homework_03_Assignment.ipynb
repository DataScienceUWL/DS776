{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 03 Assignment\n",
    "**Name:** [Student Name Here]  \n",
    "**Total Points:** 50\n",
    "\n",
    "## Submission Checklist\n",
    "- [ ] All code cells executed with output saved\n",
    "- [ ] All questions answered\n",
    "- [ ] Notebook converted to HTML (use the Homework_03_Utilities notebook)\n",
    "- [ ] Canvas notebook filename includes `_GRADE_THIS_ONE`\n",
    "- [ ] Files uploaded to Canvas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Training Techniques\n",
    "\n",
    "In this assignment you will build a deeper CNN model to improve the classification performance on the FashionMNIST dataset. Deeper models can be more difficult to train so you'll employ some of the techniques from Lesson 3 to improve the training. You'll also use data augmentation to improve the performance of the model while reducing overfitting. Along the way you'll see how to downsample a dataset to make for more efficient experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR IMPORTS HERE ===\n",
    "# Add any additional imports you need below this line\n",
    "\n",
    "\n",
    "from introdl.utils import config_paths_keys\n",
    "\n",
    "# Configure paths\n",
    "paths = config_paths_keys()\n",
    "DATA_PATH = paths['DATA_PATH']\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "# === END YOUR IMPORTS ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pts] Build the Model\n",
    "\n",
    "Implement a PyTorch model of class `nn.Module` to reproduce a model with the structure shown in the assignment. The model should have:\n",
    "- Three blocks of convolutional layers\n",
    "- Each block contains 3 Conv2d layers with ReLU activations\n",
    "- MaxPool2d after blocks 1 and 2\n",
    "- A single Linear layer as the classifier\n",
    "- Total parameters should be around 542,922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Define the FashionMNISTModel class\n",
    "# - Block 1: 1->32 channels, 3 conv layers, MaxPool2d\n",
    "# - Block 2: 32->64 channels, 3 conv layers, MaxPool2d\n",
    "# - Block 3: 64->128 channels, 3 conv layers, no pooling\n",
    "# - Classifier: Flatten + Linear(6272, 10)\n",
    "\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Your implementation\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Your implementation\n",
    "        pass\n",
    "\n",
    "# Create model and show summary\n",
    "# model = FashionMNISTModel()\n",
    "# summary(model, input_size=(64, 1, 28, 28))\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pts] Setup the Data\n",
    "\n",
    "Load the FashionMNIST dataset. Normalize with mean 0.2860 and standard deviation 0.3530. Downsample the train dataset to 10% of its original size to make experimentation quick.\n",
    "\n",
    "Use the FashionMNIST test dataset for your `valid_dataset`. For the DataLoaders try batch size 64 to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Load FashionMNIST with normalization\n",
    "# TODO: Downsample training set to 10% using provided code\n",
    "# TODO: Create DataLoaders\n",
    "\n",
    "# Use this code for downsampling:\n",
    "# from torch.utils.data import Subset\n",
    "# np.random.seed(42)  # use this seed for reproducibility\n",
    "# subset_indices = np.random.choice(len(train_dataset), size=int(0.1 * len(train_dataset)), replace=False)\n",
    "# train_dataset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pts] Training with SGD\n",
    "\n",
    "Train your model with Stochastic Gradient Descent. Track the accuracy metric. You'll likely need to increase both the learning rate and the number of epochs to see the validation accuracy plateau.\n",
    "\n",
    "Make sure to instantiate a fresh model to see complete training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create fresh model instance\n",
    "# TODO: Setup SGD optimizer\n",
    "# TODO: Train with appropriate learning rate and epochs\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Load checkpoint and plot metrics\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pts] Training with AdamW\n",
    "\n",
    "Now repeat the previous training using AdamW. You should be able to use the default learning rate of 0.001 and fewer epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create fresh model instance\n",
    "# TODO: Setup AdamW optimizer\n",
    "# TODO: Train with default learning rate\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Load checkpoint and plot metrics\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SGD and AdamW Training Performance\n",
    "\n",
    "Make plots of validation loss and accuracy for both SGD and AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create comparison plots\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pts] Data Augmentation\n",
    "\n",
    "Now use data augmentation. Build a transform_train pipeline that includes:\n",
    "* Random horizontal flips\n",
    "* Random crops of size 28, padding = 4\n",
    "* Random rotations up to 10 degrees\n",
    "\n",
    "Use the same seed to downsample the train_dataset to 10% of its size.\n",
    "\n",
    "In the next cell, set up the data and augmentation transforms (don't augment the validation data). Build the DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create augmentation transforms\n",
    "# TODO: Setup datasets with augmentation\n",
    "# TODO: Create DataLoaders\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new instance of your model with the new DataLoaders and AdamW. Training will take more epochs so you may have to experiment a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Train model with augmentation\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Load checkpoint and plot metrics\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare validation loss and accuracy for the three different approaches so far: SGD, AdamW, and AdamW with augmentation. Make appropriate graphs and comment on the three training strategies in terms of their performance on metrics and overfitting.\n",
    "\n",
    "üìù **YOUR ANALYSIS HERE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create comparison plots for all three approaches\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pts] Early Stopping\n",
    "\n",
    "Early stopping isn't really necessary unless the metrics on the validation or test set start to degrade. Try it anyway just to reinforce how it works. In this section implement early stopping based on the validation loss. Use AdamW and data augmentation. Add a comparison plot of the two methods. Comment on the performance with and without early stopping. Do you get comparable performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Implement training with early stopping\n",
    "# TODO: Compare with regular training\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR ANALYSIS OF EARLY STOPPING:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pts] OneCycleLR\n",
    "\n",
    "Create a new instance of the model. Implement a OneCycleLR learning rate scheduler and add it to your AdamW approach with data augmentation. You should be able to use a larger max learning rate of 0.003 or so. Experiment a little to see if you can get similar results to the above with fewer epochs (you may not be able to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Implement OneCycleLR training\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Load checkpoint and plot metrics\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot comparing the validation losses and accuracies for all of the training approaches above (there should be 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Create comprehensive comparison plots\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which approach works best? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pts] Use Best Approach on Full Dataset\n",
    "\n",
    "Take your best approach and apply it to the full dataset (don't downsample).\n",
    "\n",
    "This will take a little more than a minute per epoch so run your experiments with the smaller dataset above, then run this once. You can use `resume_from_checkpoint = True` if you want to extend the training.\n",
    "\n",
    "How does this compare to the performance you achieved in HW 2? Import your best run from HW 2 and make a plot comparing the performance of your best approach from this assignment to the approach from the second assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YOUR CODE HERE ===\n",
    "# TODO: Train best model on full dataset\n",
    "# TODO: Compare with HW2 results\n",
    "\n",
    "\n",
    "# === END YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **YOUR COMPARISON WITH HW2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8 pts] Questions from Sections 3.6 and 5.1-5.3 Reading\n",
    "\n",
    "**Question 1 (3 pts):** Section 3.6 discusses data augmentation in CNNs. Based on your implementation above and the reading:\n",
    "- What is the fundamental trade-off that data augmentation helps address in deep learning?\n",
    "- Why is it important that augmentation transformations are applied randomly during training rather than creating a fixed larger dataset?\n",
    "- Looking at your results above, how did data augmentation affect the gap between training and validation accuracy? What does this indicate about overfitting? (The effect may be hard to see on this relatively easy dataset.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "**Question 2 (3 pts):** Sections 5.1-5.2 cover different learning rate schedules. The reading mentions exponential decay, step drop, and cosine annealing:\n",
    "- Explain why a fixed learning rate throughout training can be suboptimal, particularly in the later stages of training.\n",
    "- The OneCycleLR scheduler you used above combines increasing and decreasing phases. Based on the reading about learning rate schedules, why might starting with a lower learning rate and increasing it (warm-up) be beneficial?\n",
    "- Which type of schedule from the reading (exponential, step, or cosine) most closely resembles the OneCycleLR approach you implemented?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "**Question 3 (2 pts):** Section 5.3 discusses adaptive learning rate adjustment based on plateau detection:\n",
    "- What metric is typically monitored to detect a plateau, and why is this more reliable than monitoring training loss?\n",
    "- How does the early stopping technique you implemented above relate to plateau-based learning rate adjustment? Could they work together?\n",
    "- Note: We'll use plateau-based learning rate adjustment in Lesson 6 to train object detection models.\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2 pts] Reflection\n",
    "\n",
    "1. What, if anything, did you find difficult to understand for this lesson? Why?\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**\n",
    "\n",
    "2. What resources did you find supported your learning most and least for this lesson? (Be honest - I use your input to shape the course.)\n",
    "\n",
    "üìù **YOUR ANSWER HERE:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {},
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
