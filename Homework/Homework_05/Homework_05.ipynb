{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Transfer Learning\n",
    "\n",
    "In this homework you'll experiment with applying transfer learning for fine-grained classification using the Flowers102 dataset in torchvision.datasets.  Fine-grained classification is when you have many categories or classes that are similar like related series of flowers.  Or, for example, trying to distinguish breeds of dogs as opposed to cats, dogs, and foxes.\n",
    "\n",
    "Note: we were able to train all the models described in this homework in about 40 minutes on the T4 Compute Server.  The ConvNext model was the biggest and took the most time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Flowers102 dataset\n",
    "\n",
    "There are 102 classes of flowers each with between 40 and 258 images. The dataset is available in torchvision as `torchvision.datasets.Flowers102`.  You can find more information about the [dataset here](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/).  The labels for the classes are also [available here](https://gist.github.com/JosephKJ/94c7728ed1a8e0cd87fe6a029769cde1).  \n",
    "\n",
    "The dataset has three splits each of which can be accessed with code like this:\n",
    "\n",
    "```python\n",
    "train_dataset = Flowers102(root=DATA_PATH, split='train', download=True, transform = transform_train)\n",
    "```\n",
    "\n",
    "To get the validation and testing splits change split to 'valid' or 'test'.  \n",
    "\n",
    "### Data Exploration (5 pts)\n",
    "\n",
    "In this section you should explore the dataset a bit.  Plot a few examples and find at least two classes that have similar looking flowers.  Also how many images per class in the training and validation sets?  You may want to start with transforms that don't add any augmentation for the purposes of exploring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation and DataLoaders (5 pts)\n",
    "\n",
    "Build your transforms for training.  Remember that for testing and validation the transforms shouldn't add any augmentation.  The images should be $224 \\times 224$ when transformed since our pretrained models were trained on Imagenet with the same size images.  We used `batch_size = 32` on the T4 Compute Servers.  For normalization use the statistics from Imagenet since the pretrained models we are using expect that normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 (5 pts)\n",
    "\n",
    "The ResNet models establish good baselines for results.\n",
    "\n",
    "Build a custom model class for ResNet50 (AI may be helpful here) with an adjustable number of output classes.  It should have methods to freeze and unfreeze the backbone.  Apply transfer learning instantiating your model with the default Imagenet weights and training with for 5 epochs followed by training for a suitable number of epochs (you may need to experiment).  Include graphics or display dataframes to show how the model is converging (at least for the unfrozen training).\n",
    "\n",
    "Use the training and validation sets here.  The test set will be reserved for your final best model. \n",
    "\n",
    "What kind of validation accuracy are you able to achieve?  Is the model overfitting?\n",
    "\n",
    "Note: the training dataset is already pretty small so downsampling it to expedite experimentation isn't a good idea, but you could temporarily reduce the size of the images to say 128x128 in your tranforms to get things working, then go back to 224x224 to train your models.  All final results should be done with 224x224."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet V2 Small (5 pts)\n",
    "\n",
    "EfficientNet models are a modern upgrade to traditional convolutional neural networks, offering improved performance and efficiency.  Repeat what you did for ResNet50 for EfficientNet V2 Small.  Use AI to search for how to load it in torchvision and how to adapt in your custom model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNeXt Small (5 pts)\n",
    "\n",
    "ConvNeXt models are a family of convolutional neural networks that aim to modernize the design of traditional CNNs by incorporating elements from vision transformers. They provide a strong performance baseline for various computer vision tasks.  Use transfer learning to train a ConvNeXT Small (not Tiny) model on Flowers102."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Small (5 pts)\n",
    "\n",
    "Vision Transformers (ViTs) are a type of neural network architecture that leverages the transformer model, originally designed for natural language processing, to process image data. Unlike Convolutional Neural Networks (CNNs), which use convolutional layers to capture spatial hierarchies, ViTs divide images into patches and process them as sequences, allowing for global context understanding. ViTs typically require more data to train from scratch compared to CNNs, but they can be effectively used for transfer learning on smaller datasets if the images are similar to those in the Imagenet dataset.  We'll learn more about transformer models in the second half of the course.\n",
    "\n",
    "We'll use the timm library which doesn't seem to be installed in CoCalc.  \n",
    "To use ViT Small from the timm library, you can install timm with the following command:\n",
    "```python\n",
    "!pip install timm\n",
    "```\n",
    "Then, load the pre-trained ViT Small model with:\n",
    "```python\n",
    "import timm\n",
    "model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
    "```\n",
    "\n",
    "The ViT Small model is pretrained on Imagenet and expects the same size images and same normalization as other models.  Typically we fine tune the whole model and don't train with a frozen backbone.  The learning rates used are usually smaller, too.  Do the same kind of fine tuning as you've done above using OneCycleLR with max_lr = 0.0005.  We found that the number of epochs needed was similar to the total number of epochs used in the two-phase training used by our other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Best Model to Test Data and Evaluate (10 pts)\n",
    "\n",
    "Write a brief summary of your investigations above.  Include a graph comparing the training metrics from the fine-tuning phases on the validation data from above.\n",
    "\n",
    "Generate a classification report comparing the predictions of your best model to the ground truth labels on the test dataset.  Summarize the highlights of the report.  A confusion matrix display probably isn't helpful because there are so many classes (set `display_confusion=False` if use `evaluate_classifier` from `introdl.utils`.)  But you can look at slices of the confusion matrix.  Try to identify at least two classes which are being confused by your model and display examples, with proper labels, from those classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n"
     ]
    }
   ],
   "source": [
    "# imports and configuration\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import Flowers102\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from introdl.utils import get_device, load_results, load_model, config_paths_keys\n",
    "from introdl.idlmam import train_network\n",
    "from introdl.visul import plot_training_metrics, plot_transformed_images, create_image_grid, evaluate_classifier\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]  # Set the default figure size (width, height) in inches\n",
    "\n",
    "paths = config_paths_keys()\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "DATA_PATH = paths['DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use \n",
    "mean = [0.485, 0.456, 0.406]  # Imagenet\n",
    "std = [0.229, 0.224, 0.225]  # Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    T.Resize(224, max_size=None),  # Resize so the shortest edge is 224\n",
    "    T.CenterCrop(224),            # Center crop to 224x224\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.RandomCrop(224, padding=10),            # Random crop padded img to 224x224\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.RandomGrayscale(),\n",
    "    T.Normalize(mean=mean, std=std),\n",
    "    T.ToPureTensor()\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    T.Resize(224, max_size=None),  # Resize so the shortest edge is 224\n",
    "    T.CenterCrop(224),            # Center crop to 224x224\n",
    "    T.Normalize(mean=mean, std=std),\n",
    "    T.ToPureTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Flowers102(root=DATA_PATH, split='train', download=True, transform = transform_train)\n",
    "valid_dataset = Flowers102(root=DATA_PATH, split='val', download=True, transform = transform_val)\n",
    "test_dataset = Flowers102(root=DATA_PATH, split='test', download=True, transform = transform_val)\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Custom(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom ResNet18 model with a modified final layer for a specified number of output classes.\n",
    "\n",
    "    Args:\n",
    "        num_outputs (int): The number of output classes for the modified final layer.\n",
    "        weights (ResNet18_Weights or None): Pretrained weights to load for ResNet18. If None, the model is randomly initialized.\n",
    "\n",
    "    Methods:\n",
    "        freeze_backbone(): Freezes all layers of the backbone except the final classification head.\n",
    "        unfreeze_backbone(): Unfreezes all layers of the backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs: int, weights=None):\n",
    "        \"\"\"\n",
    "        Initializes the ResNet18Custom model.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): The number of output classes for the modified final layer.\n",
    "            weights (ResNet18_Weights or None): Pretrained weights for ResNet18. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(ResNet50Custom, self).__init__()\n",
    "        # Load ResNet18 with specified weights (pretrained or None)\n",
    "        self.model = models.resnet50(weights=weights)\n",
    "        \n",
    "        # Replace the final fully connected layer to fit the desired output size\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Freezes all layers of the backbone except the final classification head.\n",
    "        This is useful for transfer learning scenarios where only the head is fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ensure the final fully connected layer remains trainable\n",
    "        for param in self.model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Unfreezes all layers of the backbone, allowing the entire model to be fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "model = ResNet50Custom(num_outputs=102, weights=ResNet50_Weights.DEFAULT)\n",
    "model.freeze_backbone()\n",
    "loss_func = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.AdamW(model.parameters())  # Adam optimizer\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "ckpt_file = MODELS_PATH / 'HW05_resnet50_frozen_backbone.pt'\n",
    "epochs = 5\n",
    "\n",
    "score_funcs = {'ACC':accuracy_score}\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file)\n",
    "\n",
    "print('frozen backbone results')\n",
    "print(results)\n",
    "# load the model with the frozen backbone and unfreeze it\n",
    "model = load_model(ResNet50Custom(num_outputs=102), MODELS_PATH / 'HW05_resnet50_frozen_backbone.pt')\n",
    "model.unfreeze_backbone()\n",
    "\n",
    "# Configure Training for unfrozen model\n",
    "ckpt_file = MODELS_PATH / 'HW05_resnet50_unfrozen_backbone.pt'\n",
    "epochs = 10\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.001, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "# Train and save\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file,\n",
    "                        lr_schedule=scheduler,\n",
    "                        scheduler_step_per_batch=True,\n",
    "                        #early_stop_crit='max',\n",
    "                        #early_stop_metric='ACC',\n",
    "                        #patience=1,\n",
    "                        pretend_train=False)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 6149 samples.\n",
      "The model misclassified 754 samples.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      1.00      0.53        20\n",
      "           1       1.00      0.90      0.95        40\n",
      "           2       0.50      0.80      0.62        20\n",
      "           3       0.62      0.67      0.64        36\n",
      "           4       0.93      0.62      0.75        45\n",
      "           5       0.85      0.92      0.88        25\n",
      "           6       0.80      1.00      0.89        20\n",
      "           7       0.84      1.00      0.92        65\n",
      "           8       0.74      0.88      0.81        26\n",
      "           9       0.93      1.00      0.96        25\n",
      "          10       0.93      0.60      0.73        67\n",
      "          11       0.86      0.94      0.90        67\n",
      "          12       0.88      0.97      0.92        29\n",
      "          13       0.87      0.93      0.90        28\n",
      "          14       0.69      1.00      0.82        29\n",
      "          15       0.73      0.76      0.74        21\n",
      "          16       0.97      0.97      0.97        65\n",
      "          17       0.76      0.97      0.85        62\n",
      "          18       0.75      0.93      0.83        29\n",
      "          19       0.76      0.89      0.82        36\n",
      "          20       0.81      0.85      0.83        20\n",
      "          21       0.97      0.97      0.97        39\n",
      "          22       0.98      0.89      0.93        71\n",
      "          23       0.92      1.00      0.96        22\n",
      "          24       0.87      0.95      0.91        21\n",
      "          25       0.93      0.67      0.78        21\n",
      "          26       0.95      0.95      0.95        20\n",
      "          27       0.91      0.93      0.92        46\n",
      "          28       1.00      0.81      0.90        58\n",
      "          29       0.91      0.82      0.86        65\n",
      "          30       0.96      0.75      0.84        32\n",
      "          31       0.53      0.92      0.68        25\n",
      "          32       0.96      0.92      0.94        26\n",
      "          33       0.95      0.90      0.92        20\n",
      "          34       0.79      1.00      0.88        23\n",
      "          35       0.96      0.89      0.92        55\n",
      "          36       0.99      0.99      0.99        88\n",
      "          37       0.88      1.00      0.94        36\n",
      "          38       0.65      0.81      0.72        21\n",
      "          39       0.71      0.77      0.73        47\n",
      "          40       0.95      0.91      0.93       107\n",
      "          41       0.80      0.90      0.84        39\n",
      "          42       0.80      0.81      0.81       110\n",
      "          43       0.93      0.93      0.93        73\n",
      "          44       0.91      1.00      0.95        20\n",
      "          45       0.94      0.98      0.96       176\n",
      "          46       0.98      1.00      0.99        47\n",
      "          47       0.88      0.98      0.93        51\n",
      "          48       0.88      1.00      0.94        29\n",
      "          49       1.00      0.85      0.92        72\n",
      "          50       0.97      0.56      0.71       238\n",
      "          51       0.97      1.00      0.98        65\n",
      "          52       0.82      0.82      0.82        73\n",
      "          53       0.91      1.00      0.95        41\n",
      "          54       0.92      0.94      0.93        51\n",
      "          55       0.96      0.96      0.96        89\n",
      "          56       0.92      0.96      0.94        47\n",
      "          57       0.95      0.99      0.97        94\n",
      "          58       0.98      0.98      0.98        47\n",
      "          59       1.00      0.99      0.99        89\n",
      "          60       0.94      1.00      0.97        30\n",
      "          61       0.92      0.69      0.79        35\n",
      "          62       0.87      1.00      0.93        34\n",
      "          63       1.00      1.00      1.00        32\n",
      "          64       1.00      0.98      0.99        82\n",
      "          65       0.95      1.00      0.98        41\n",
      "          66       0.69      0.82      0.75        22\n",
      "          67       0.69      0.71      0.70        34\n",
      "          68       0.89      0.91      0.90        34\n",
      "          69       0.95      0.95      0.95        42\n",
      "          70       0.95      0.97      0.96        58\n",
      "          71       0.90      0.80      0.85        76\n",
      "          72       0.92      0.95      0.94       174\n",
      "          73       0.96      0.87      0.91       151\n",
      "          74       0.87      0.97      0.92       100\n",
      "          75       0.92      0.90      0.91        87\n",
      "          76       0.99      0.97      0.98       231\n",
      "          77       0.85      0.85      0.85       117\n",
      "          78       0.88      1.00      0.93        21\n",
      "          79       0.86      0.96      0.91        85\n",
      "          80       0.97      0.97      0.97       146\n",
      "          81       1.00      0.73      0.84        92\n",
      "          82       0.71      0.72      0.71       111\n",
      "          83       0.86      0.85      0.85        66\n",
      "          84       0.80      0.86      0.83        43\n",
      "          85       0.59      0.92      0.72        38\n",
      "          86       0.63      0.95      0.76        43\n",
      "          87       0.94      0.84      0.89       134\n",
      "          88       1.00      0.60      0.75       164\n",
      "          89       0.77      0.66      0.71        62\n",
      "          90       0.78      0.91      0.84        56\n",
      "          91       0.95      0.91      0.93        46\n",
      "          92       0.83      0.77      0.80        26\n",
      "          93       0.94      0.96      0.95       142\n",
      "          94       0.88      0.89      0.88       108\n",
      "          95       0.81      0.66      0.73        71\n",
      "          96       0.56      0.78      0.65        46\n",
      "          97       1.00      0.84      0.91        62\n",
      "          98       0.97      0.91      0.94        43\n",
      "          99       0.86      0.86      0.86        29\n",
      "         100       0.39      0.82      0.53        38\n",
      "         101       0.87      0.96      0.92        28\n",
      "\n",
      "    accuracy                           0.88      6149\n",
      "   macro avg       0.86      0.89      0.87      6149\n",
      "weighted avg       0.90      0.88      0.88      6149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model = load_model(ResNet50Custom(num_outputs=102), MODELS_PATH / 'HW05_resnet50_unfrozen_backbone.pt', device)\n",
    "conf_mat,report,missed_dataset=evaluate_classifier(model, test_dataset, device, display_confusion=False,img_size=(8,8),use_class_labels=False)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d01efc7651406ab5bc561e665bebcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from first frozen\n",
      "   epoch  total time  train loss  val loss  train ACC   val ACC\n",
      "0      0   11.369319    4.492456  4.012067   0.058824  0.309804\n",
      "1      1   22.745737    3.621311  3.471067   0.447059  0.483333\n",
      "2      2   34.182627    3.006349  3.023964   0.650000  0.565686\n",
      "3      3   46.747025    2.511653  2.690017   0.733333  0.595098\n",
      "4      4   58.774077    2.100312  2.434196   0.794118  0.621569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086f9499f88040d9a239243f3c86d8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from unfrozen\n",
      "   epoch  total time  train loss  val loss  train ACC   val ACC            lr\n",
      "0      0   12.209569    1.282725  1.169192   0.816667  0.761765  2.845967e-04\n",
      "1      1   25.082786    0.449274  0.609663   0.925490  0.859804  7.691054e-04\n",
      "2      2   38.376692    0.292723  0.775564   0.927451  0.810784  9.999508e-04\n",
      "3      3   50.758855    0.330240  0.632020   0.914706  0.819608  9.473978e-04\n",
      "4      4   62.625299    0.265910  0.613509   0.925490  0.839216  8.062326e-04\n",
      "5      5   74.513244    0.127086  0.533884   0.974510  0.876471  6.044147e-04\n",
      "6      6   86.665337    0.064149  0.413011   0.984314  0.895098  3.819165e-04\n",
      "7      7   98.682648    0.052109  0.360036   0.991176  0.907843  1.828066e-04\n",
      "8      8  110.750937    0.027482  0.344016   0.995098  0.913725  4.652118e-05\n",
      "9      9  122.982529    0.025984  0.338021   0.995098  0.917647  5.317392e-08\n"
     ]
    }
   ],
   "source": [
    "class EfficientNetV2SCustom(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom EfficientNetV2-S model with a modified final layer for a specified number of output classes.\n",
    "\n",
    "    Args:\n",
    "        num_outputs (int): The number of output classes for the modified final layer.\n",
    "        weights (EfficientNet_V2_S_Weights or None): Pretrained weights to load for EfficientNetV2-S. If None, the model is randomly initialized.\n",
    "\n",
    "    Methods:\n",
    "        freeze_backbone(): Freezes all layers of the backbone except the final classification head.\n",
    "        unfreeze_backbone(): Unfreezes all layers of the backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs: int, weights=None):\n",
    "        \"\"\"\n",
    "        Initializes the EfficientNetV2SCustom model.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): The number of output classes for the modified final layer.\n",
    "            weights (EfficientNet_V2_S_Weights or None): Pretrained weights for EfficientNetV2-S. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(EfficientNetV2SCustom, self).__init__()\n",
    "        # Load EfficientNetV2-S with specified weights (pretrained or None)\n",
    "        self.model = models.efficientnet_v2_s(weights=weights)\n",
    "        \n",
    "        # Replace the final fully connected layer to fit the desired output size\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "        self.model.classifier[1] = nn.Linear(in_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Freezes all layers of the backbone except the final classification head.\n",
    "        This is useful for transfer learning scenarios where only the head is fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ensure the final fully connected layer remains trainable\n",
    "        for param in self.model.classifier[1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Unfreezes all layers of the backbone, allowing the entire model to be fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "# Example usage\n",
    "model = EfficientNetV2SCustom(num_outputs=102, weights=EfficientNet_V2_S_Weights.DEFAULT)\n",
    "model.freeze_backbone()\n",
    "loss_func = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.AdamW(model.parameters())  # Adam optimizer\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "ckpt_file = MODELS_PATH / 'HW05_effnetv2s_frozen_backbone.pt'\n",
    "epochs = 5\n",
    "\n",
    "score_funcs = {'ACC':accuracy_score}\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file)\n",
    "print('results from first frozen')\n",
    "print(results)\n",
    "# load the model with the frozen backbone and unfreeze it\n",
    "model = load_model(EfficientNetV2SCustom(num_outputs=102), MODELS_PATH / 'HW05_effnetv2s_frozen_backbone.pt')\n",
    "model.unfreeze_backbone()\n",
    "\n",
    "# Configure Training for unfrozen model\n",
    "ckpt_file = MODELS_PATH / 'HW05_effnetv2s_unfrozen_backbone.pt'\n",
    "epochs = 10\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.001, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "# Train and save\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file,\n",
    "                        lr_schedule=scheduler,\n",
    "                        scheduler_step_per_batch=True,\n",
    "                        #early_stop_crit='max',\n",
    "                        #early_stop_metric='ACC',\n",
    "                        #patience=1,\n",
    "                        pretend_train=False)\n",
    "\n",
    "print('results from unfrozen')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dfc44ad24b488a82536acd1d21921a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from first frozen\n",
      "   epoch  total time  train loss  val loss  train ACC   val ACC\n",
      "0      0   11.541814    4.622006  4.198492   0.039216  0.201961\n",
      "1      1   22.926343    3.984223  3.757851   0.254902  0.384314\n",
      "2      2   34.616255    3.542271  3.425205   0.413725  0.450980\n",
      "3      3   46.276812    3.191760  3.124381   0.503922  0.493137\n",
      "4      4   57.802936    2.870678  2.910887   0.587255  0.500980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1355d2877146ca9f9be59271804a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from unfrozen\n",
      "   epoch  total time  train loss  val loss  train ACC   val ACC            lr\n",
      "0      0   13.637757    1.647008  1.153886   0.686275  0.716667  2.845967e-04\n",
      "1      1   28.386930    0.562287  0.934798   0.857843  0.755882  7.691054e-04\n",
      "2      2   45.844816    0.433745  0.998435   0.881373  0.741176  9.999508e-04\n",
      "3      3   62.852023    0.497462  0.889549   0.863725  0.768627  9.473978e-04\n",
      "4      4   79.829498    0.315314  0.693951   0.922549  0.815686  8.062326e-04\n",
      "5      5   96.138303    0.213922  0.509777   0.937255  0.870588  6.044147e-04\n",
      "6      6  113.896498    0.085581  0.375674   0.978431  0.909804  3.819165e-04\n",
      "7      7  130.724488    0.041973  0.289377   0.989216  0.927451  1.828066e-04\n",
      "8      8  146.978459    0.016492  0.279471   0.997059  0.928431  4.652118e-05\n",
      "9      9  163.910269    0.015655  0.279825   0.996078  0.927451  5.317392e-08\n"
     ]
    }
   ],
   "source": [
    "class EfficientNetV2MCustom(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom EfficientNetV2-S model with a modified final layer for a specified number of output classes.\n",
    "\n",
    "    Args:\n",
    "        num_outputs (int): The number of output classes for the modified final layer.\n",
    "        weights (EfficientNet_V2_S_Weights or None): Pretrained weights to load for EfficientNetV2-S. If None, the model is randomly initialized.\n",
    "\n",
    "    Methods:\n",
    "        freeze_backbone(): Freezes all layers of the backbone except the final classification head.\n",
    "        unfreeze_backbone(): Unfreezes all layers of the backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs: int, weights=None):\n",
    "        \"\"\"\n",
    "        Initializes the EfficientNetV2SCustom model.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): The number of output classes for the modified final layer.\n",
    "            weights (EfficientNet_V2_S_Weights or None): Pretrained weights for EfficientNetV2-S. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(EfficientNetV2MCustom, self).__init__()\n",
    "        # Load EfficientNetV2-S with specified weights (pretrained or None)\n",
    "        self.model = models.efficientnet_v2_m(weights=weights)\n",
    "        \n",
    "        # Replace the final fully connected layer to fit the desired output size\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "        self.model.classifier[1] = nn.Linear(in_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Freezes all layers of the backbone except the final classification head.\n",
    "        This is useful for transfer learning scenarios where only the head is fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ensure the final fully connected layer remains trainable\n",
    "        for param in self.model.classifier[1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Unfreezes all layers of the backbone, allowing the entire model to be fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
    "\n",
    "# Example usage\n",
    "model = EfficientNetV2MCustom(num_outputs=102, weights=EfficientNet_V2_M_Weights.DEFAULT)\n",
    "model.freeze_backbone()\n",
    "loss_func = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.AdamW(model.parameters())  # Adam optimizer\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "ckpt_file = MODELS_PATH / 'HW05_effnetv2m_frozen_backbone.pt'\n",
    "epochs = 5\n",
    "\n",
    "score_funcs = {'ACC':accuracy_score}\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file)\n",
    "print('results from first frozen')\n",
    "print(results)\n",
    "# load the model with the frozen backbone and unfreeze it\n",
    "model = load_model(EfficientNetV2MCustom(num_outputs=102), MODELS_PATH / 'HW05_effnetv2m_frozen_backbone.pt')\n",
    "model.unfreeze_backbone()\n",
    "\n",
    "# Configure Training for unfrozen model\n",
    "ckpt_file = MODELS_PATH / 'HW05_effnetv2m_unfrozen_backbone.pt'\n",
    "epochs = 10\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.001, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "# Train and save\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file,\n",
    "                        lr_schedule=scheduler,\n",
    "                        scheduler_step_per_batch=True,\n",
    "                        #early_stop_crit='max',\n",
    "                        #early_stop_metric='ACC',\n",
    "                        #patience=1,\n",
    "                        pretend_train=False)\n",
    "\n",
    "print('results from unfrozen')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_small-0c510722.pth\" to C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\\hub\\checkpoints\\convnext_small-0c510722.pth\n",
      "100%|██████████| 192M/192M [00:03<00:00, 57.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668527e5f9a548dfbfd1b49cc7546453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from first frozen\n",
      "   epoch  total time  train loss  val loss  train ACC   val ACC\n",
      "0      0   12.888771    4.394079  3.640191   0.087255  0.365686\n",
      "1      1   24.798908    3.363116  2.818370   0.476471  0.574510\n",
      "2      2   36.376364    2.680339  2.246206   0.634314  0.689216\n",
      "3      3   48.677295    2.154083  1.873590   0.738235  0.711765\n",
      "4      4   60.036880    1.772913  1.596199   0.792157  0.759804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d443193ac827466082fdf95f1e538a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from unfrozen\n",
      "   epoch  total time  train loss  val loss  train ACC   val ACC            lr\n",
      "0      0   16.460839    0.912522  0.547375   0.883333  0.918627  2.845967e-04\n",
      "1      1   35.332454    0.348125  0.413000   0.955882  0.904902  7.691054e-04\n",
      "2      2   54.415028    0.397739  0.703783   0.902941  0.799020  9.999508e-04\n",
      "3      3   71.640449    0.634795  0.724583   0.811765  0.804902  9.473978e-04\n",
      "4      4   88.223327    0.398121  0.500378   0.889216  0.852941  8.062326e-04\n",
      "5      5  104.519420    0.230908  0.497983   0.940196  0.878431  6.044147e-04\n",
      "6      6  120.740600    0.065830  0.314118   0.983333  0.919608  3.819165e-04\n",
      "7      7  137.474455    0.024095  0.256581   0.996078  0.928431  1.828066e-04\n",
      "8      8  153.966809    0.016123  0.241121   0.998039  0.933333  4.652118e-05\n",
      "9      9  170.211968    0.011608  0.238230   0.999020  0.935294  5.317392e-08\n"
     ]
    }
   ],
   "source": [
    "class ConvNeXtSmallCustom(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom ConvNeXt-Small model with a modified final layer for a specified number of output classes.\n",
    "\n",
    "    Args:\n",
    "        num_outputs (int): The number of output classes for the modified final layer.\n",
    "        weights (ConvNeXt_Small_Weights or None): Pretrained weights to load for ConvNeXt-Small. If None, the model is randomly initialized.\n",
    "\n",
    "    Methods:\n",
    "        freeze_backbone(): Freezes all layers of the backbone except the final classification head.\n",
    "        unfreeze_backbone(): Unfreezes all layers of the backbone.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, num_outputs: int, weights=None):\n",
    "        \"\"\"\n",
    "        Initializes the ConvNeXtSmallCustom model.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): The number of output classes for the modified final layer.\n",
    "            weights (ConvNeXt_Small_Weights or None): Pretrained weights for ConvNeXt-Small. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(ConvNeXtSmallCustom, self).__init__()\n",
    "        # Load ConvNeXt-Small with specified weights (pretrained or None)\n",
    "        self.model = convnext_small(weights=weights)\n",
    "        \n",
    "        # Replace the final fully connected layer to fit the desired output size\n",
    "        in_features = self.model.classifier[2].in_features\n",
    "        self.model.classifier[2] = nn.Linear(in_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Freezes all layers of the backbone except the final classification head.\n",
    "        This is useful for transfer learning scenarios where only the head is fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ensure the final fully connected layer remains trainable\n",
    "        for param in self.model.classifier[2].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Unfreezes all layers of the backbone, allowing the entire model to be fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "from torchvision.models import convnext_small, ConvNeXt_Small_Weights\n",
    "\n",
    "# Example usage\n",
    "model = ConvNeXtSmallCustom(num_outputs=102, weights='DEFAULT')\n",
    "model.freeze_backbone()\n",
    "loss_func = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.AdamW(model.parameters())  # Adam optimizer\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "ckpt_file = MODELS_PATH / 'HW05_convnext_frozen_backbone.pt'\n",
    "epochs = 5\n",
    "\n",
    "score_funcs = {'ACC':accuracy_score}\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file)\n",
    "print('results from first frozen')\n",
    "print(results)\n",
    "# load the model with the frozen backbone and unfreeze it\n",
    "model = load_model(ConvNeXtSmallCustom(num_outputs=102), MODELS_PATH / 'HW05_convnext_frozen_backbone.pt')\n",
    "model.unfreeze_backbone()\n",
    "\n",
    "# Configure Training for unfrozen model\n",
    "ckpt_file = MODELS_PATH / 'HW05_convnext_unfrozen_backbone.pt'\n",
    "epochs = 10\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.001, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "# Train and save\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file,\n",
    "                        lr_schedule=scheduler,\n",
    "                        scheduler_step_per_batch=True,\n",
    "                        #early_stop_crit='max',\n",
    "                        #early_stop_metric='ACC',\n",
    "                        #patience=1,\n",
    "                        pretend_train=False)\n",
    "\n",
    "print('results from unfrozen')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ace1f48d06d426c89bd3183773f6baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from unfrozen\n",
      "    epoch  total time  train loss  val loss  train ACC   val ACC            lr\n",
      "0       0   17.845672    4.579647  4.177164   0.032353  0.223529  1.538093e-04\n",
      "1       1   36.665318    3.324904  1.781848   0.413725  0.771569  4.412683e-04\n",
      "2       2   56.014508    1.141581  0.681193   0.835294  0.849020  7.660623e-04\n",
      "3       3   73.363088    0.571640  0.731323   0.883333  0.809804  9.741722e-04\n",
      "4       4   89.867398    0.567349  0.971712   0.869608  0.759804  9.936971e-04\n",
      "5       5  106.066579    0.547120  0.720939   0.862745  0.804902  9.484366e-04\n",
      "6       6  122.392435    0.375067  0.698968   0.900980  0.812745  8.633307e-04\n",
      "7       7  138.543738    0.170329  0.481625   0.962745  0.874510  7.459415e-04\n",
      "8       8  157.585560    0.079554  0.503962   0.979412  0.865686  6.066995e-04\n",
      "9       9  176.653614    0.067205  0.350411   0.990196  0.900980  4.579769e-04\n",
      "10     10  196.826587    0.038177  0.338550   0.991176  0.908824  3.129885e-04\n",
      "11     11  216.331042    0.012267  0.305802   0.999020  0.918627  1.846170e-04\n",
      "12     12  235.535174    0.010042  0.282694   0.999020  0.927451  8.426886e-05\n",
      "13     13  254.289671    0.007694  0.271936   0.999020  0.925490  2.086044e-05\n",
      "14     14  272.817173    0.010631  0.270792   0.997059  0.926471  2.585528e-08\n"
     ]
    }
   ],
   "source": [
    "class ConvNeXtSmallCustom(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom ConvNeXt-Small model with a modified final layer for a specified number of output classes.\n",
    "\n",
    "    Args:\n",
    "        num_outputs (int): The number of output classes for the modified final layer.\n",
    "        weights (ConvNeXt_Small_Weights or None): Pretrained weights to load for ConvNeXt-Small. If None, the model is randomly initialized.\n",
    "\n",
    "    Methods:\n",
    "        freeze_backbone(): Freezes all layers of the backbone except the final classification head.\n",
    "        unfreeze_backbone(): Unfreezes all layers of the backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs: int, weights=None):\n",
    "        \"\"\"\n",
    "        Initializes the ConvNeXtSmallCustom model.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): The number of output classes for the modified final layer.\n",
    "            weights (ConvNeXt_Small_Weights or None): Pretrained weights for ConvNeXt-Small. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(ConvNeXtSmallCustom, self).__init__()\n",
    "        # Load ConvNeXt-Small with specified weights (pretrained or None)\n",
    "        self.model = convnext_small(weights=weights)\n",
    "        \n",
    "        # Replace the final fully connected layer to fit the desired output size\n",
    "        in_features = self.model.classifier[2].in_features\n",
    "        self.model.classifier[2] = nn.Linear(in_features, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Freezes all layers of the backbone except the final classification head.\n",
    "        This is useful for transfer learning scenarios where only the head is fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ensure the final fully connected layer remains trainable\n",
    "        for param in self.model.classifier[2].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"\n",
    "        Unfreezes all layers of the backbone, allowing the entire model to be fine-tuned.\n",
    "        \"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "from torchvision.models import convnext_small, ConvNeXt_Small_Weights\n",
    "\n",
    "# Example usage\n",
    "model = ConvNeXtSmallCustom(num_outputs=102, weights='DEFAULT')\n",
    "loss_func = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.AdamW(model.parameters())  # Adam optimizer\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "ckpt_file = MODELS_PATH / 'HW05_convnext.pt'\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.001, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "# Train and save\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file,\n",
    "                        lr_schedule=scheduler,\n",
    "                        scheduler_step_per_batch=True,\n",
    "                        #early_stop_crit='max',\n",
    "                        #early_stop_metric='ACC',\n",
    "                        #patience=1,\n",
    "                        pretend_train=False)\n",
    "\n",
    "print('results from unfrozen')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a479c404ed404fa5a9a98793e75efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  total time  train loss  val loss  train ACC   val ACC        lr\n",
      "0       0   11.626570    5.220364  4.602282   0.030392  0.070588  0.000020\n",
      "1       1   25.202290    4.032850  3.722921   0.124510  0.180392  0.000020\n",
      "2       2   39.728007    3.046734  2.836828   0.341176  0.397059  0.000020\n",
      "3       3   54.210005    2.208314  2.126258   0.596078  0.575490  0.000021\n",
      "4       4   68.995062    1.479452  1.578535   0.762745  0.728431  0.000021\n",
      "5       5   83.802866    0.994949  1.165487   0.868627  0.815686  0.000021\n",
      "6       6   98.626444    0.637018  0.880288   0.948039  0.863725  0.000022\n",
      "7       7  113.155464    0.440195  0.731099   0.961765  0.891176  0.000023\n",
      "8       8  128.264617    0.294525  0.610322   0.982353  0.918627  0.000024\n",
      "9       9  144.346943    0.209792  0.539418   0.992157  0.922549  0.000025\n",
      "10     10  159.578101    0.162213  0.476298   0.992157  0.929412  0.000026\n",
      "11     11  173.948681    0.132028  0.443842   0.991176  0.930392  0.000027\n",
      "12     12  188.169427    0.104173  0.387837   0.995098  0.933333  0.000028\n",
      "13     13  203.312228    0.078876  0.375174   0.998039  0.942157  0.000030\n",
      "14     14  218.179233    0.069477  0.338497   0.997059  0.945098  0.000031\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained ViT-Small model\n",
    "model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
    "\n",
    "# Modify the classification head for 102 outputs\n",
    "model.head = nn.Linear(model.head.in_features, 102)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.AdamW(model.parameters())  # Adam optimizer\n",
    "\n",
    "epochs = 15\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.0005, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "ckpt_file = MODELS_PATH / 'HW05_vit.pt'\n",
    "\n",
    "\n",
    "score_funcs = {'ACC':accuracy_score}\n",
    "results = train_network(model,\n",
    "                        loss_func,\n",
    "                        train_loader,\n",
    "                        device=device,\n",
    "                        val_loader=valid_loader,\n",
    "                        epochs = epochs,\n",
    "                        optimizer = optimizer,\n",
    "                        lr_schedule=scheduler,\n",
    "                        score_funcs = score_funcs,\n",
    "                        checkpoint_file=ckpt_file)\n",
    "\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
