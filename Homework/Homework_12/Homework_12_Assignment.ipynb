{
 "cells": [
  {
   "cell_type": "code",
   "id": "nnrz0pndtv",
   "source": [
    "# DS776 Environment Setup & Package Update\n",
    "# Configures storage paths for proper cleanup/sync, then updates introdl if needed\n",
    "# If this cell fails, see Lessons/Course_Tools/AUTO_UPDATE_SYSTEM.md for help\n",
    "%run ../../Lessons/Course_Tools/auto_update_introdl.py"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91f35c78",
   "metadata": {},
   "source": "# Homework 12 - Text Summarization\n\n**Total Points: 40**\n\nWe're going to work with conversational data in this homework.  The `SAMsum` dataset consists of chat-like conversations and summaries like this:\n\nConversation-\n```\nOlivia: Who are you voting for in this election?\nOliver: Liberals as always.\nOlivia: Me too!!\nOliver: Great\n```\n\nSummary-\n```\nOlivia and Olivier are voting for liberals in this election.\n```\n\nApplications for this kind of summarization include generating chat and meeting summaries.\n\nThroughout this assignment you'll work with the first 100 conversations and summaries from the validation split of [\"knkarthick/samsum\"](https://huggingface.co/datasets/knkarthick/samsum) on Hugging Face."
  },
  {
   "cell_type": "markdown",
   "id": "9906df4a",
   "metadata": {},
   "source": "## Task 1 - Build a zero-shot LLM conversation summarizer (10 points)\n\nUse either an 8B local Llama model or an API-based model like `gemini-2.0-flash-lite` or better to build an `llm_summarizer` function that takes as input a list of conversations and returns a list of extracted summaries.  \n\n**Implementation Options:**\n- Use the `llm_generate()` function from the `introdl` package, OR\n- Write your own code/wrapper to access the OpenRouter API (as demonstrated in Lesson 11)\n\nYour function should be constructed similarly to `llm_classifier` or `llm_ner_extractor` in Lessons 8 and 10, respectively.  \n\nPut some effort into the prompt to make it good at generating succinct summaries of converations that identify both the topics and the people.\n\nYour list of returned summaries should be cleanly extracted summaries with no additional text such as parts of the input prompt.\n\nGive a qualitative evaluation of the first three generated summaries compared to the ground-truth summaries."
  },
  {
   "cell_type": "code",
   "id": "t7la4euedj",
   "source": "# YOUR CODE: Load the SAMsum dataset and get first 100 from validation split\n# Hint: from datasets import load_dataset\n# dataset = load_dataset(\"knkarthick/samsum\")\n# val_data = dataset[\"validation\"].select(range(100))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xifcgmx0ss",
   "source": "## Storage Guidance\n\n**Always use the path variables** (`MODELS_PATH`, `DATA_PATH`, `CACHE_PATH`) instead of hardcoded paths. The actual locations depend on your environment:\n\n| Variable | CoCalc Home Server | Compute Server |\n|----------|-------------------|----------------|\n| `MODELS_PATH` | `Homework_12_Models/` | `Homework_12_Models/` *(synced)* |\n| `DATA_PATH` | `~/home_workspace/data/` | `~/cs_workspace/data/` *(local)* |\n| `CACHE_PATH` | `~/home_workspace/downloads/` | `~/cs_workspace/downloads/` *(local)* |\n\n**Why this matters:**\n- On **Compute Servers**: Only `MODELS_PATH` syncs back to CoCalc (~10GB limit). Data and cache stay local (~50GB).\n- On **CoCalc Home**: Everything syncs and counts against the ~10GB limit.\n- **Storage_Cleanup.ipynb** (in this folder) helps free synced space when needed.\n\n**Tip:** Always write `MODELS_PATH / 'model.pt'` \u2014 never hardcode paths like `'Homework_12_Models/model.pt'`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "d1osq3sw394",
   "source": "# YOUR CODE: Build llm_summarizer function here\n# Requirements:\n# - Takes a list of conversations as input\n# - Returns a list of summaries\n# - Uses llm_generate() or your own API wrapper\n# - Prompt should encourage concise summaries that identify topics and people\n# - Clean the output to return just the summary (no prompt text)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "19v0dqb3cul",
   "source": "# YOUR CODE: Test llm_summarizer on first 3 conversations\n# Get first 3 conversations and their ground-truth summaries\n# Generate summaries using llm_summarizer\n# Display results for comparison",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lmtsb5k44zn",
   "source": "### Qualitative Evaluation of First Three Summaries\n\n\ud83d\udcdd **YOUR EVALUATION HERE:**\n\nFor each of the three examples, discuss:\n- How does the generated summary compare to the ground-truth?\n- Does it capture the key topics and identify the people involved?\n- Is it concise and coherent?\n- Any issues with the generated summary?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "1e0a9157",
   "metadata": {},
   "source": [
    "## Task 2 - Build a few-shot LLM conversation summarizer (6 points)\n",
    "\n",
    "Follow the same instructions as in Task 1, but add a few examples from the training data.  Don't simply pick the first examples, rather take some care to choose diverse conversations and/or conversations that are difficult to summarize."
   ]
  },
  {
   "cell_type": "code",
   "id": "dg23ol7p48m",
   "source": "# YOUR CODE: Select diverse few-shot examples from training data\n# Look at several training examples and pick 2-4 diverse ones\n# Consider: different conversation styles, lengths, topics",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4de3nps27fq",
   "source": "# YOUR CODE: Build few-shot llm_summarizer function\n# Include your selected examples in the prompt\n# Format: show conversation -> summary pairs before the test conversation",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uk51glbzlvh",
   "source": "# YOUR CODE: Test few-shot llm_summarizer on first 3 conversations\n# Use the same 3 conversations as Task 1 for comparison",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mzgwpc2fjx",
   "source": "### Qualitative Evaluation\n\n\ud83d\udcdd **YOUR EVALUATION HERE:**\n\nCompare the few-shot results to:\n- The ground-truth summaries\n- The zero-shot results from Task 1\n\nDoes few-shot prompting improve the quality? In what ways?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "3009ca1c",
   "metadata": {},
   "source": [
    "## Task 3 - Refine the llm_score function (10 points)\n",
    "\n",
    "For this task you can use a local Llama model or an API-based model.  (I personally find the API-based models much easier to use.)\n",
    "\n",
    "Start with the `llm_score` function from last week and refine the prompt to improve the scoring to better reflect similarities in semantic meaning between two texts.  Here are some guidelines that you should incorporate into your prompt:\n",
    "\n",
    "- A score of **100** means the texts have **identical meaning**.\n",
    "- A score of **80\u201399** means they are **strong paraphrases** or very similar in meaning.\n",
    "- A score of **50\u201379** means they are **somewhat related**, but not expressing the same idea.\n",
    "- A score of **1\u201349** means they are **barely or loosely related**.\n",
    "- A score of **0** means **no semantic similarity**.\n",
    "- Take into account word meaning, order, and structure.\n",
    "- Synonyms count as matches.\n",
    "- Do not reward scrambled words unless they convey the same meaning.\n",
    "- Make the prompt few-shot by including several text pairs and the corresponding similarity scores.\n",
    "\n",
    "Demonstrate your `llm_score` function by applying it to the 7 sentence pairs from the lesson.  Comment on the performance of the scoring.  Does it still get fooled by the sixth and seventh pairs like BERTScore did?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "mm327u0lrc",
   "source": "# The 7 sentence pairs from Lesson 12 for testing\ntest_pairs = [\n    {\n        \"reference\": \"The cat sat on the mat.\",\n        \"prediction\": \"The cat sat on the mat.\",\n        \"description\": \"Exact match\"\n    },\n    {\n        \"reference\": \"The cat sat on the mat.\",\n        \"prediction\": \"The feline rested on the rug.\",\n        \"description\": \"Synonym substitution\"\n    },\n    {\n        \"reference\": \"The cat sat on the mat in the afternoon.\",\n        \"prediction\": \"In the afternoon, the cat was sitting on the mat.\",\n        \"description\": \"Paraphrase with reordering\"\n    },\n    {\n        \"reference\": \"The government announced a stimulus package to support the economy during the recession.\",\n        \"prediction\": \"A stimulus package was announced.\",\n        \"description\": \"Shorter prediction\"\n    },\n    {\n        \"reference\": \"The plane crashed due to engine failure.\",\n        \"prediction\": \"The aircraft accident was caused by mechanical problems.\",\n        \"description\": \"Different vocabulary\"\n    },\n    {\n        \"reference\": \"The court ruled in favor of the defendant.\",\n        \"prediction\": \"The judge made a ruling in the case of the cat and the fiddle.\",\n        \"description\": \"Copying style not content (FAILURE CASE)\"\n    },\n    {\n        \"reference\": \"The stock market crashed due to unexpected inflation news.\",\n        \"prediction\": \"Inflation stock news market due crashed the unexpected.\",\n        \"description\": \"Word salad (FAILURE CASE)\"\n    }\n]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pjq5yt8rri",
   "source": "# YOUR CODE: Build llm_score function\n# \n# Requirements:\n# - Must handle both single strings and lists of strings\n#   - llm_score(ref, pred) -> single score\n#   - llm_score([ref1, ref2], [pred1, pred2]) -> list of scores\n# - Use llm_generate() with the guidelines from the task description\n# - Return scores as integers from 0-100\n# - Consider using temperature=0 for consistent scoring\n#\n# Note: Zero-shot prompting works surprisingly well for this task with modern LLMs.\n# You can experiment with few-shot examples if you'd like, but it may not improve performance.\n# \n# Hint: llm_generate() already handles lists of prompts, so you can leverage that!",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7y89q6iovlh",
   "source": "# YOUR CODE: Test llm_score on the 7 sentence pairs\n# For each pair, display:\n# - The description\n# - The reference and prediction texts\n# - The llm_score result\n# \n# You can test them one at a time or use the list functionality",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cz5coeqfhi",
   "source": "### Performance Analysis\n\n\ud83d\udcdd **YOUR ANALYSIS HERE:**\n\n- How well does llm_score perform on these examples?\n- Does it correctly handle the two FAILURE CASES (pairs 6 and 7)?\n- Compare to BERTScore performance from the lesson - does llm_score avoid the same pitfalls?\n- Did BERTScore get fooled by the sixth and seventh pairs? Does llm_score?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "2b103040",
   "metadata": {},
   "source": [
    "## Task 4 - Evaluate a Pre-trained Model and LLM_summarizer (10 points)\n",
    "\n",
    "For this task you're going to qualitatively and quantitatively compare the generated summaries from:\n",
    "1. The already fine-tuned Hugging Face model - ['philschmid/flan-t5-base-samsum'](https://huggingface.co/philschmid/flan-t5-base-samsum)\n",
    "2. The zero-shot or few shot LLM summarizer from above.\n",
    "\n",
    "If, for some reason, you can't get the specified Hugging Face model to work, then find a different Hugging Face summarization model that has already been fine-tuned on SAMsum.\n",
    "\n",
    "First, qualititavely compare the first three generated summaries from each approach to the ground-truth summaries.  Explain how the the two approaches seem to be working on the three examples.\n",
    "\n",
    "Second, compute ROUGE scores, BERTScore, and llm_score for the first 100 examples in the validation set. \n",
    "\n",
    "What do these scores suggest about the performance of the two approaches?  Is one approach clearly better than the other?  Is llm_score working well as a metric?  Does it agree with the other metrics?"
   ]
  },
  {
   "cell_type": "code",
   "id": "gxqv01lqli",
   "source": "# YOUR CODE: Load the fine-tuned Hugging Face model\n# Use 'philschmid/flan-t5-base-samsum' or another SAMsum-fine-tuned model",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9teqjatw5pm",
   "source": "# YOUR CODE: Generate summaries from both approaches for first 3 examples\n# 1. Generate using the Hugging Face model\n# 2. Generate using your llm_summarizer (zero-shot or few-shot)\n# 3. Display all results alongside ground-truth",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qa9k9qq2ebo",
   "source": "### Qualitative Comparison (First 3 Examples)\n\n\ud83d\udcdd **YOUR ANALYSIS HERE:**\n\nFor each approach, discuss:\n- How do the summaries compare to ground-truth?\n- Which approach produces better summaries?\n- How do the two approaches seem to be working?\n- Any notable differences in style or content?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cbuoqkxmp67",
   "source": "# YOUR CODE: Compute metrics for all 100 validation examples\n# \n# For both approaches:\n# 1. Generate summaries for all 100 examples\n# 2. Compute ROUGE scores (use evaluate.load(\"rouge\"))\n# 3. Compute BERTScore (use evaluate.load(\"bertscore\"))\n# 4. Compute llm_score for all 100 examples\n#    Hint: Your llm_score should handle lists of texts\n# \n# Display the results in a clear format for comparison",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "oim48b517cm",
   "source": "### Quantitative Analysis (100 Examples)\n\n\ud83d\udcdd **YOUR ANALYSIS HERE:**\n\n- What do the ROUGE, BERTScore, and llm_score results suggest about performance?\n- Is one approach clearly better than the other?\n- Do the three metrics agree with each other?\n- Is llm_score working well as a metric? Does it correlate with ROUGE and BERTScore?\n- Which metric do you think best reflects actual summary quality?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "85c6dd7f",
   "metadata": {},
   "source": [
    "## Task 5 - Comparison and Reflection (4 points)\n",
    "\n",
    "* Give a brief summary of what you learned in this assignment.\n",
    "\n",
    "* What did you find most difficult to understand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1xxzhexpb6j",
   "source": "## Task 6 - Using a Specialized Summarizer in Production (6 points)\n\nIn this task, you'll explore how to use the HuggingFace `pipeline` API to simplify deployment of the fine-tuned summarization model from Task 4.\n\n### Part A: Create a Summarization Pipeline (2 points)\n\nCreate a summarization pipeline using the `philschmid/flan-t5-base-samsum` model (or whichever fine-tuned model you used in Task 4). Demonstrate generating a summary for a single conversation from the validation set using the pipeline.\n\n**Hint**: The `pipeline()` function from the `transformers` library makes this very simple - you just need to specify the task type and model.\n\n### Part B: Batch Processing (2 points)\n\nUse the pipeline to generate summaries for a batch of 5 conversations at once. Compare the wall-clock time for batch processing versus processing the same 5 conversations individually in a loop.\n\n**Note**: You can use the `batch_size` parameter in the pipeline call to control how many examples are processed together.\n\n### Part C: Production Considerations (2 points)\n\nAnswer the following questions based on your experience with the pipeline API:\n\n1. What are the main advantages of using the `pipeline` API compared to manually handling tokenization, model inference, and decoding?\n\n2. For a production system that needs to summarize thousands of conversations per day, what factors would you need to consider when choosing between:\n   - A specialized fine-tuned model (like `philschmid/flan-t5-base-samsum`)\n   - A general-purpose LLM accessed via API (like your `llm_summarizer` from Tasks 1-2)\n   \n   Consider aspects like cost, latency, quality, and maintenance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "md4sfmcbkkd",
   "source": "# YOUR CODE: Part A - Create summarization pipeline\n# \n# Hint: from transformers import pipeline\n# summarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\")\n# \n# Test on a single conversation from validation set",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bspuoucln8u",
   "source": "# YOUR CODE: Part B - Batch processing comparison\n# \n# Compare timing for:\n# 1. Processing 5 conversations individually in a loop\n# 2. Processing 5 conversations together with batch_size parameter\n#\n# Timing pattern:\nimport time\n\n# Approach 1: Individual processing\nstart_time = time.time()\n# YOUR CODE: Process conversations one at a time\nindividual_time = time.time() - start_time\n\n# Approach 2: Batch processing\nstart_time = time.time()\n# YOUR CODE: Process all conversations together\nbatch_time = time.time() - start_time\n\nprint(f\"Individual processing: {individual_time:.2f} seconds\")\nprint(f\"Batch processing: {batch_time:.2f} seconds\")\nprint(f\"Speedup: {individual_time/batch_time:.2f}x\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1p8h9kg46do",
   "source": "### Part C: Production Considerations\n\n\ud83d\udcdd **YOUR ANSWERS HERE:**\n\n**Question 1:** What are the main advantages of using the `pipeline` API compared to manually handling tokenization, model inference, and decoding?\n\n**Question 2:** For a production system that needs to summarize thousands of conversations per day, what factors would you need to consider when choosing between:\n- A specialized fine-tuned model (like `philschmid/flan-t5-base-samsum`)\n- A general-purpose LLM accessed via API (like your `llm_summarizer`)\n\nConsider aspects like:\n- Cost (compute vs API fees)\n- Latency (response time)\n- Quality (accuracy and consistency)\n- Maintenance (updates, monitoring, debugging)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Notebook to HTML for Canvas Upload\n\nUncomment the two lines below and run the cell to export the current notebook to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from introdl import export_this_to_html\n# export_this_to_html()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}