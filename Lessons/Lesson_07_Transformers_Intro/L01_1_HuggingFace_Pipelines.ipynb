{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english, Size: 66,955,010 parameters\n",
      "[{'label': 'NEGATIVE', 'score': 0.999281108379364}]\n",
      "\n",
      "**Named Entity Recognition**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'ORG', 'score': 0.9927805, 'word': 'TechEmporium', 'start': 6, 'end': 18}, {'entity_group': 'LOC', 'score': 0.9996623, 'word': 'Canada', 'start': 103, 'end': 109}, {'entity_group': 'PER', 'score': 0.90358573, 'word': 'Alex', 'start': 451, 'end': 455}]\n",
      "\n",
      "**Question Answering**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450 parameters\n",
      "{'score': 0.70292729139328, 'start': 156, 'end': 193, 'answer': 'the left earcup doesn’t produce sound'}\n",
      "\n",
      "**Translation**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Helsinki-NLP/opus-mt-en-es, Size: 77,943,296 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hola TechEmporium, hace poco compré un conjunto de auriculares de cancelación de ruido en su tienda en línea en Canadá. Al recibir el artículo, noté un defecto: la oreja izquierda no produce sonido. Como alguien que confía en audio de calidad para mi trabajo, esto es un inconveniente importante. Me gustaría un reemplazo o un reembolso. Adjunto son los detalles del pedido y la prueba de compra. Por favor, hágame saber cómo podemos resolver este problema rápidamente.'}]\n",
      "\n",
      "**Summarization**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n",
      "[{'summary_text': \" TechEmporium's noise-canceling headphones fail to produce sound . The left earcup doesn't produce sound, and this is a major inconvenience for Alex .\"}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoModelForQuestionAnswering, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Sample Text\n",
    "text = (\n",
    "    \"Hello TechEmporium, I recently purchased a set of noise-canceling headphones \"\n",
    "    \"from your online store in Canada. Upon receiving the item, I noticed a defect: \"\n",
    "    \"the left earcup doesn’t produce sound. As someone who relies on quality audio \"\n",
    "    \"for my work, this is a major inconvenience. I would appreciate a replacement or \"\n",
    "    \"a refund. Attached are the order details and proof of purchase. Please let me \"\n",
    "    \"know how we can resolve this issue promptly. Best regards, Alex.\"\n",
    ")\n",
    "\n",
    "def print_model_info(pipe):\n",
    "    model = pipe.model\n",
    "    model_size = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {model.name_or_path}, Size: {model_size:,} parameters\")\n",
    "\n",
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "print_model_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\")\n",
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "print_model_info(ner_pipeline)\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n",
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\")\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "print_model_info(qa_pipeline)\n",
    "question = \"What is the defect?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n",
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "print_model_info(translation_pipeline)\n",
    "translation_result = translation_pipeline(text, max_length=200)\n",
    "print(translation_result)\n",
    "\n",
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\")\n",
    "print_model_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "The sentiment in the text is primarily negative. The customer expresses dissatisfaction because the product they received is defective, which is a major inconvenience for them. However, the tone is also polite and professional, as the customer is asking for a resolution (either a replacement or a refund) and providing necessary details for follow-up.\n",
      "\n",
      "**Named Entity Recognition**\n",
      "Here are the named entities in the text categorized:\n",
      "\n",
      "1. **Organization**: \n",
      "   - TechEmporium\n",
      "\n",
      "2. **Location**: \n",
      "   - Canada\n",
      "\n",
      "3. **Person**: \n",
      "   - Alex\n",
      "\n",
      "These entities are identified as significant names or terms within the text that refer to specific people, organizations, or locations.\n",
      "\n",
      "**Question Answering**\n",
      "The defect is that the left earcup of the noise-canceling headphones doesn’t produce sound.\n",
      "\n",
      "**Translation**\n",
      "Bonjour TechEmporium,\n",
      "\n",
      "J'ai récemment acheté une paire de casques à réduction de bruit sur votre boutique en ligne au Canada. À la réception de l'article, j'ai remarqué un défaut : l'écouteur gauche ne produit pas de son. Étant une personne qui dépend d'un son de qualité pour mon travail, cela représente un inconvénient majeur. Je vous serais reconnaissant(e) de procéder à un remplacement ou à un remboursement. Vous trouverez ci-joints les détails de la commande et la preuve d'achat. Veuillez me faire savoir comment nous pouvons résoudre ce problème rapidement.\n",
      "\n",
      "Cordialement,\n",
      "\n",
      "Alex.\n",
      "\n",
      "**Summarization**\n",
      "Alex contacted TechEmporium about a defective set of noise-canceling headphones, where the left earcup doesn't work. They requested a replacement or refund, providing order details and proof of purchase, and sought a prompt resolution.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load OpenAI API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def chatgpt_prompt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that performs various NLP tasks.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Sample Text\n",
    "text = (\n",
    "    \"Hello TechEmporium, I recently purchased a set of noise-canceling headphones \"\n",
    "    \"from your online store in Canada. Upon receiving the item, I noticed a defect: \"\n",
    "    \"the left earcup doesn’t produce sound. As someone who relies on quality audio \"\n",
    "    \"for my work, this is a major inconvenience. I would appreciate a replacement or \"\n",
    "    \"a refund. Attached are the order details and proof of purchase. Please let me \"\n",
    "    \"know how we can resolve this issue promptly. Best regards, Alex.\"\n",
    ")\n",
    "\n",
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_prompt = f\"Perform sentiment analysis on the following text: \\n{text}\"\n",
    "sentiment_result = chatgpt_prompt(sentiment_prompt)\n",
    "print(sentiment_result)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\")\n",
    "ner_prompt = f\"Identify and categorize the named entities in the following text: \\n{text}\"\n",
    "ner_result = chatgpt_prompt(ner_prompt)\n",
    "print(ner_result)\n",
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\")\n",
    "question = \"What is the defect?\"\n",
    "qa_prompt = f\"Based on the following text, answer the question: \\nText: {text}\\nQuestion: {question}\"\n",
    "qa_result = chatgpt_prompt(qa_prompt)\n",
    "print(qa_result)\n",
    "\n",
    "# Translation (English to French)\n",
    "print(\"\\n**Translation**\")\n",
    "translation_prompt = f\"Translate the following text to French: \\n{text}\"\n",
    "translation_result = chatgpt_prompt(translation_prompt)\n",
    "print(translation_result)\n",
    "\n",
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_prompt = f\"Summarize the following text in 25-50 words: \\n{text}\"\n",
    "summarization_result = chatgpt_prompt(summarization_prompt)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Cache for local models\n",
    "MODEL_CACHE = {}\n",
    "\n",
    "def llm_prompt(prompt, model_str, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate a response from an LLM based on the given model string.\n",
    "    \n",
    "    Parameters:\n",
    "        - prompt (str): The input text prompt.\n",
    "        - model_str (str): The model identifier (e.g., 'gpt-4o' or 'meta-llama/Llama-3.2-3B-Instruct').\n",
    "        - max_length (int, optional): Maximum response length. Default is 256.\n",
    "    \n",
    "    Returns:\n",
    "        - str: The model-generated response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Case 1: OpenAI API Model\n",
    "    if model_str in [\"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\"]:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Set OPENAI_API_KEY in the environment.\")\n",
    "        \n",
    "        openai.api_key = api_key\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_str,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_length,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except Exception as e:\n",
    "            return f\"OpenAI API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model\n",
    "    else:\n",
    "        if model_str not in MODEL_CACHE:\n",
    "            try:\n",
    "                print(f\"Loading model: {model_str} (this may take a while)...\")\n",
    "                MODEL_CACHE[model_str] = {\n",
    "                    \"model\": AutoModelForCausalLM.from_pretrained(model_str, torch_dtype=torch.float16, device_map=\"auto\"),\n",
    "                    \"tokenizer\": AutoTokenizer.from_pretrained(model_str)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return f\"Error loading model {model_str}: {str(e)}\"\n",
    "        \n",
    "        model = MODEL_CACHE[model_str][\"model\"]\n",
    "        tokenizer = MODEL_CACHE[model_str][\"tokenizer\"]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_length=max_length, temperature=0.7)\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **temperature** parameter in the `llm_prompt` function controls the **randomness** of the model's output. It determines how deterministic or creative the generated response will be.\n",
    "\n",
    "### **How Temperature Affects Output:**\n",
    "- **Low temperature (e.g., 0.1 - 0.3)** → **More deterministic**  \n",
    "  - The model will choose the most probable words, leading to more consistent and factual responses.\n",
    "  - Best for tasks requiring accuracy, such as summarization or answering factual questions.\n",
    "  \n",
    "- **Medium temperature (e.g., 0.5 - 0.7)** → **Balanced creativity and reliability**  \n",
    "  - Allows some variation while maintaining coherence.\n",
    "  - Good for general-purpose conversations.\n",
    "\n",
    "- **High temperature (e.g., 0.8 - 1.5)** → **More diverse and creative**  \n",
    "  - The model will take more risks, selecting less likely words more often.\n",
    "  - Can produce more original and imaginative responses but may become less coherent.\n",
    "  - Suitable for brainstorming, storytelling, or creative writing.\n",
    "\n",
    "### **Where It Applies in Your Function:**\n",
    "In the function:\n",
    "```python\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model_str,\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "              {\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=max_length,\n",
    "    temperature=0.7  # <-- Here\n",
    ")\n",
    "```\n",
    "- Setting `temperature=0.7` provides a good balance between creativity and coherence.\n",
    "- If you want deterministic responses, set `temperature=0.0` or `0.1`.\n",
    "- If you want more variety in responses, increase it to `0.9` or higher.\n",
    "\n",
    "For local Hugging Face models, the equivalent setting in `generate()`:\n",
    "```python\n",
    "output = model.generate(**inputs, max_length=max_length, temperature=0.7)\n",
    "```\n",
    "works the same way.\n",
    "\n",
    "### **Should You Make Temperature Adjustable?**\n",
    "If your use case requires different levels of randomness (e.g., factual QA vs. creative writing), you could add `temperature` as a parameter:\n",
    "```python\n",
    "def llm_prompt(prompt, model_str, max_length=256, temperature=0.7):\n",
    "```\n",
    "Then use it like this:\n",
    "```python\n",
    "llm_prompt(\"Tell me a story.\", \"gpt-4o\", temperature=1.0)  # More creative\n",
    "llm_prompt(\"What is the capital of France?\", \"gpt-4o\", temperature=0.1)  # More deterministic\n",
    "```\n",
    "Would you like me to update your function to include an adjustable `temperature` parameter? 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Cache for local models to avoid reloading\n",
    "MODEL_CACHE = {}\n",
    "\n",
    "def llm_prompt(prompt, model_str, max_length=256, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate a response from an LLM based on the given model string.\n",
    "    \n",
    "    Parameters:\n",
    "        - prompt (str): The input text prompt.\n",
    "        - model_str (str): The model identifier (e.g., 'gpt-4o' or 'meta-llama/Llama-3.2-3B-Instruct').\n",
    "        - max_length (int, optional): Maximum response length. Default is 256.\n",
    "        - temperature (float, optional): Controls randomness (0.0 = deterministic, 1.0+ = creative). Default is 0.7.\n",
    "    \n",
    "    Returns:\n",
    "        - str: The model-generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Case 1: OpenAI API Model\n",
    "    if model_str in [\"gpt-4o\", \"gpt-4o-mini\", \"o1\", \"o1-mini\"]:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Set OPENAI_API_KEY in the environment.\")\n",
    "\n",
    "        openai.api_key = api_key\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_str,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_length,\n",
    "                temperature=temperature  # Adjustable temperature\n",
    "            )\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except Exception as e:\n",
    "            return f\"OpenAI API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model (e.g., LLaMA)\n",
    "    else:\n",
    "        if model_str not in MODEL_CACHE:\n",
    "            try:\n",
    "                print(f\"Loading model: {model_str} (this may take a while)...\")\n",
    "                MODEL_CACHE[model_str] = {\n",
    "                    \"model\": AutoModelForCausalLM.from_pretrained(model_str, torch_dtype=torch.float16, device_map=\"auto\"),\n",
    "                    \"tokenizer\": AutoTokenizer.from_pretrained(model_str)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return f\"Error loading model {model_str}: {str(e)}\"\n",
    "        \n",
    "        model = MODEL_CACHE[model_str][\"model\"]\n",
    "        tokenizer = MODEL_CACHE[model_str][\"tokenizer\"]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate response with adjustable temperature\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_length=max_length, temperature=temperature)\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI GPT-4o with a lower temperature for more deterministic responses\n",
    "response = llm_prompt(\"Explain quantum entanglement.\", \"gpt-4o\", temperature=0.3)\n",
    "print(response)\n",
    "\n",
    "# Using LLaMA with a higher temperature for more creative output\n",
    "response = llm_prompt(\"Tell me a sci-fi story.\", \"meta-llama/Llama-3.2-3B-Instruct\", temperature=1.2)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've updated your `llm_prompt` function to include **search strategies** for local Hugging Face models, giving you control over how responses are generated. You can now specify **different decoding strategies** using a new parameter:  \n",
    "- **`search_strategy=\"greedy\"`** (default) – simple, deterministic response  \n",
    "- **`search_strategy=\"beam\"`** – beam search for better fluency  \n",
    "- **`search_strategy=\"top_k\"`** – random sampling from the top-k most likely tokens  \n",
    "- **`search_strategy=\"top_p\"`** – nucleus sampling for diverse responses  \n",
    "- **`search_strategy=\"contrastive\"`** – contrastive search for high-quality output  \n",
    "\n",
    "---\n",
    "\n",
    "### **Updated `llm_prompt` Function**\n",
    "```python\n",
    "import os\n",
    "import openai\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Cache for local models to avoid reloading\n",
    "MODEL_CACHE = {}\n",
    "\n",
    "def llm_prompt(prompt, model_str, max_length=256, temperature=0.7, search_strategy=\"greedy\", top_k=50, top_p=0.9, num_beams=5):\n",
    "    \"\"\"\n",
    "    Generate a response from an LLM based on the given model string.\n",
    "    \n",
    "    Parameters:\n",
    "        - prompt (str): The input text prompt.\n",
    "        - model_str (str): The model identifier (e.g., 'gpt-4o' or 'meta-llama/Llama-3.2-3B-Instruct').\n",
    "        - max_length (int, optional): Maximum response length. Default is 256.\n",
    "        - temperature (float, optional): Controls randomness (0.0 = deterministic, 1.0+ = creative). Default is 0.7.\n",
    "        - search_strategy (str, optional): Decoding strategy. Options: 'greedy', 'beam', 'top_k', 'top_p', 'contrastive'. Default is 'greedy'.\n",
    "        - top_k (int, optional): Used if `search_strategy=\"top_k\"`. Default is 50.\n",
    "        - top_p (float, optional): Used if `search_strategy=\"top_p\"`. Default is 0.9.\n",
    "        - num_beams (int, optional): Used if `search_strategy=\"beam\"`. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        - str: The model-generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Case 1: OpenAI API Model\n",
    "    if model_str in [\"gpt-4o\", \"gpt-4o-mini\", \"o1\", \"o1-mini\"]:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Set OPENAI_API_KEY in the environment.\")\n",
    "\n",
    "        openai.api_key = api_key\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_str,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_length,\n",
    "                temperature=temperature  # Adjustable temperature\n",
    "            )\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except Exception as e:\n",
    "            return f\"OpenAI API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model (e.g., LLaMA)\n",
    "    else:\n",
    "        if model_str not in MODEL_CACHE:\n",
    "            try:\n",
    "                print(f\"Loading model: {model_str} (this may take a while)...\")\n",
    "                MODEL_CACHE[model_str] = {\n",
    "                    \"model\": AutoModelForCausalLM.from_pretrained(model_str, torch_dtype=torch.float16, device_map=\"auto\"),\n",
    "                    \"tokenizer\": AutoTokenizer.from_pretrained(model_str)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return f\"Error loading model {model_str}: {str(e)}\"\n",
    "        \n",
    "        model = MODEL_CACHE[model_str][\"model\"]\n",
    "        tokenizer = MODEL_CACHE[model_str][\"tokenizer\"]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Apply search strategy\n",
    "        gen_kwargs = {\"max_length\": max_length, \"temperature\": temperature}\n",
    "\n",
    "        if search_strategy == \"greedy\":\n",
    "            gen_kwargs[\"do_sample\"] = False  # Greedy decoding\n",
    "        elif search_strategy == \"beam\":\n",
    "            gen_kwargs[\"num_beams\"] = num_beams  # Beam search\n",
    "        elif search_strategy == \"top_k\":\n",
    "            gen_kwargs.update({\"do_sample\": True, \"top_k\": top_k})  # Top-k sampling\n",
    "        elif search_strategy == \"top_p\":\n",
    "            gen_kwargs.update({\"do_sample\": True, \"top_p\": top_p})  # Nucleus sampling\n",
    "        elif search_strategy == \"contrastive\":\n",
    "            gen_kwargs.update({\"penalty_alpha\": 0.6, \"top_k\": 4})  # Contrastive search\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **New Features & Search Strategies**\n",
    "| Search Strategy  | How It Works | Best Used For |\n",
    "|------------------|-------------|--------------|\n",
    "| **greedy** (default) | Picks the most likely word at each step. | Fast, deterministic output. |\n",
    "| **beam** | Considers multiple possible sequences and picks the best. | Producing high-quality, fluent text. |\n",
    "| **top_k** | Samples from the top-k most likely words. | Adding variety while staying relevant. |\n",
    "| **top_p** | Samples dynamically from the most probable subset of words. | More diverse, natural text. |\n",
    "| **contrastive** | Uses **contrastive search** to balance diversity and quality. | High-quality open-ended generation. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Usage**\n",
    "#### **1️⃣ OpenAI GPT-4o (API)**\n",
    "```python\n",
    "response = llm_prompt(\"Explain quantum mechanics.\", \"gpt-4o\", temperature=0.3)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "#### **2️⃣ LLaMA Using Greedy Decoding**\n",
    "```python\n",
    "response = llm_prompt(\"Tell me a story.\", \"meta-llama/Llama-3.2-3B-Instruct\", search_strategy=\"greedy\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "#### **3️⃣ LLaMA Using Beam Search for Higher Fluency**\n",
    "```python\n",
    "response = llm_prompt(\"Summarize this article.\", \"meta-llama/Llama-3.2-3B-Instruct\", search_strategy=\"beam\", num_beams=5)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "#### **4️⃣ LLaMA Using Top-k Sampling for More Variety**\n",
    "```python\n",
    "response = llm_prompt(\"Write a poem about AI.\", \"meta-llama/Llama-3.2-3B-Instruct\", search_strategy=\"top_k\", top_k=40)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "#### **5️⃣ LLaMA Using Contrastive Search for High-Quality Responses**\n",
    "```python\n",
    "response = llm_prompt(\"Describe the future of AI.\", \"meta-llama/Llama-3.2-3B-Instruct\", search_strategy=\"contrastive\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Update is Useful**\n",
    "✅ **More Control** – Choose between fluency (beam), diversity (top-p), or determinism (greedy).  \n",
    "✅ **Better Text Quality** – Contrastive search improves coherence.  \n",
    "✅ **Preserves API Flexibility** – GPT models still work the same way.  \n",
    "✅ **Fast & Efficient** – **Caches** the model for reuse instead of reloading it each time.  \n",
    "\n",
    "Would you like any additional customization, such as **temperature-dependent sampling adjustments** or **log probabilities** for debugging? 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
